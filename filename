digraph {
	graph [size="2819.25,2819.25"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	140106745666896 [label="
 ()" fillcolor=darkolivegreen1]
	140106869108608 [label="AddBackward0
------------
alpha: 1"]
	140106869103280 -> 140106869108608
	140106869103280 [label="AddBackward0
------------
alpha: 1"]
	140106869107120 -> 140106869103280
	140106869107120 [label="AddBackward0
------------
alpha: 1"]
	140106869108800 -> 140106869107120
	140106869108800 [label="AddBackward0
------------
alpha: 1"]
	140106869103568 -> 140106869108800
	140106869103568 [label="AddBackward0
------------
alpha: 1"]
	140106869105104 -> 140106869103568
	140106869105104 [label="AddBackward0
------------
alpha: 1"]
	140106869107552 -> 140106869105104
	140106869107552 [label="AddBackward0
------------
alpha: 1"]
	140106869108416 -> 140106869107552
	140106869108416 [label="AddBackward0
------------
alpha: 1"]
	140106869102320 -> 140106869108416
	140106869102320 [label="AddBackward0
------------
alpha: 1"]
	140106869101648 -> 140106869102320
	140106869101648 [label="AddBackward0
------------
alpha: 1"]
	140106869109472 -> 140106869101648
	140106869109472 [label="AddBackward0
------------
alpha: 1"]
	140106869104432 -> 140106869109472
	140106869104432 [label="AddBackward0
------------
alpha: 1"]
	140106869102944 -> 140106869104432
	140106869102944 [label="AddBackward0
------------
alpha: 1"]
	140106869105968 -> 140106869102944
	140106869105968 [label="AddBackward0
------------
alpha: 1"]
	140106869105584 -> 140106869105968
	140106869105584 [label="MeanBackward0
------------------------
self_sym_numel:      700
self_sym_sizes: (7, 100)"]
	140106869106112 -> 140106869105584
	140106869106112 -> 140106774828736 [dir=none]
	140106774828736 [label="self
 (7, 100)" fillcolor=orange]
	140106869106112 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106869105392 -> 140106869106112
	140106869105392 [label="RsubBackward1
-------------
alpha: 1"]
	140106869106160 -> 140106869105392
	140106869106160 [label="ViewBackward0
---------------------------
self_sym_sizes: (7, 1, 100)"]
	140106869103712 -> 140106869106160
	140106869103712 -> 140106752164624 [dir=none]
	140106752164624 [label="input
 (7, 1024, 100)" fillcolor=orange]
	140106869103712 -> 140106757929168 [dir=none]
	140106757929168 [label="weight
 (1, 1024, 3)" fillcolor=orange]
	140106869103712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869104288 -> 140106869103712
	140106869104288 -> 140106752164304 [dir=none]
	140106752164304 [label="self
 (7, 1024, 100)" fillcolor=orange]
	140106869104288 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869106832 -> 140106869104288
	140106869106832 -> 140107110291776 [dir=none]
	140107110291776 [label="input
 (7, 1024, 100)" fillcolor=orange]
	140106869106832 -> 140106794052688 [dir=none]
	140106794052688 [label="weight
 (1024, 1024, 5)" fillcolor=orange]
	140106869106832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869106016 -> 140106869106832
	140106869106016 -> 140107110300816 [dir=none]
	140107110300816 [label="self
 (7, 1024, 100)" fillcolor=orange]
	140106869106016 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869106208 -> 140106869106016
	140106869106208 -> 140106774826976 [dir=none]
	140106774826976 [label="input
 (7, 1024, 400)" fillcolor=orange]
	140106869106208 -> 140106757930768 [dir=none]
	140106757930768 [label="weight
 (1024, 4, 41)" fillcolor=orange]
	140106869106208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :           (1,)
groups            :            256
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (20,)
stride            :           (4,)
transposed        :          False
weight            : [saved tensor]"]
	140106869107072 -> 140106869106208
	140106869107072 -> 140106774835296 [dir=none]
	140106774835296 [label="self
 (7, 1024, 400)" fillcolor=orange]
	140106869107072 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869105824 -> 140106869107072
	140106869105824 -> 140106774832416 [dir=none]
	140106774832416 [label="input
 (7, 256, 1600)" fillcolor=orange]
	140106869105824 -> 140106774837696 [dir=none]
	140106774837696 [label="weight
 (1024, 4, 41)" fillcolor=orange]
	140106869105824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :           (1,)
groups            :             64
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (20,)
stride            :           (4,)
transposed        :          False
weight            : [saved tensor]"]
	140106869107600 -> 140106869105824
	140106869107600 -> 140106757930928 [dir=none]
	140106757930928 [label="self
 (7, 256, 1600)" fillcolor=orange]
	140106869107600 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869103040 -> 140106869107600
	140106869103040 -> 140106757930528 [dir=none]
	140106757930528 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140106869103040 -> 140106757930288 [dir=none]
	140106757930288 [label="weight
 (256, 4, 41)" fillcolor=orange]
	140106869103040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :             16
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (20,)
stride            :           (4,)
transposed        :          False
weight            : [saved tensor]"]
	140106869102896 -> 140106869103040
	140106869102896 -> 140106757930048 [dir=none]
	140106757930048 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140106869102896 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869102608 -> 140106869102896
	140106869102608 -> 140106757929728 [dir=none]
	140106757929728 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869102608 -> 140106757929328 [dir=none]
	140106757929328 [label="weight
 (64, 4, 41)" fillcolor=orange]
	140106869102608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              4
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (20,)
stride            :           (4,)
transposed        :          False
weight            : [saved tensor]"]
	140106869106976 -> 140106869102608
	140106869106976 -> 140106757929648 [dir=none]
	140106757929648 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869106976 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869105008 -> 140106869106976
	140106869105008 -> 140106784278864 [dir=none]
	140106784278864 [label="input
 (7, 1, 25600)" fillcolor=orange]
	140106869105008 -> 140106761900480 [dir=none]
	140106761900480 [label="weight
 (16, 1, 15)" fillcolor=orange]
	140106869105008 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (7,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869108320 -> 140106869105008
	140106869108320 -> 140106769813952 [dir=none]
	140106769813952 [label="result
 (7, 1, 25600)" fillcolor=orange]
	140106869108320 [label="TanhBackward0
----------------------
result: [saved tensor]"]
	140106869104912 -> 140106869108320
	140106869104912 -> 140106788249472 [dir=none]
	140106788249472 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869104912 -> 140107082872480 [dir=none]
	140107082872480 [label="weight
 (1, 16, 7)" fillcolor=orange]
	140106869104912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (0,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869108272 -> 140106869104912
	140106869108272 -> 140107077238304 [dir=none]
	140107077238304 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869108272 [label="LeakyReluBackward0
------------------------------
negative_slope:           0.01
self          : [saved tensor]"]
	140106869103856 -> 140106869108272
	140106869103856 -> 140106761899520 [dir=none]
	140106761899520 [label="other
 ()" fillcolor=orange]
	140106869103856 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869108512 -> 140106869103856
	140106869108512 [label="AddBackward0
------------
alpha: 1"]
	140106869105296 -> 140106869108512
	140106869105296 [label="AddBackward0
------------
alpha: 1"]
	140106869109040 -> 140106869105296
	140106869109040 [label="AddBackward0
------------
alpha: 1"]
	140106869107984 -> 140106869109040
	140106869107984 -> 140106784278464 [dir=none]
	140106784278464 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869107984 -> 140106784278784 [dir=none]
	140106784278784 [label="weight
 (16, 16, 3)" fillcolor=orange]
	140106869107984 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869109376 -> 140106869107984
	140106869109376 -> 140106784278384 [dir=none]
	140106784278384 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869109376 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869109232 -> 140106869109376
	140106869109232 -> 140106784277904 [dir=none]
	140106784277904 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869109232 -> 140106784277504 [dir=none]
	140106784277504 [label="weight
 (16, 16, 3)" fillcolor=orange]
	140106869109232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869108848 -> 140106869109232
	140106869108848 -> 140106784277824 [dir=none]
	140106784277824 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869108848 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869096752 -> 140106869108848
	140106869096752 [label="AddBackward0
------------
alpha: 1"]
	140106869110048 -> 140106869096752
	140106869110048 -> 140106784277104 [dir=none]
	140106784277104 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869110048 -> 140106784277184 [dir=none]
	140106784277184 [label="weight
 (16, 16, 3)" fillcolor=orange]
	140106869110048 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869110240 -> 140106869110048
	140106869110240 -> 140106788257072 [dir=none]
	140106788257072 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869110240 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869110432 -> 140106869110240
	140106869110432 -> 140106788256992 [dir=none]
	140106788256992 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869110432 -> 140106788256592 [dir=none]
	140106788256592 [label="weight
 (16, 16, 3)" fillcolor=orange]
	140106869110432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869110576 -> 140106869110432
	140106869110576 -> 140106788256832 [dir=none]
	140106788256832 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869110576 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869109616 -> 140106869110576
	140106869109616 [label="AddBackward0
------------
alpha: 1"]
	140106869110816 -> 140106869109616
	140106869110816 -> 140106788256352 [dir=none]
	140106788256352 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869110816 -> 140106788256432 [dir=none]
	140106788256432 [label="weight
 (16, 16, 3)" fillcolor=orange]
	140106869110816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869111008 -> 140106869110816
	140106869111008 -> 140106788256272 [dir=none]
	140106788256272 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869111008 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869111200 -> 140106869111008
	140106869111200 -> 140106788256112 [dir=none]
	140106788256112 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869111200 -> 140106788256192 [dir=none]
	140106788256192 [label="weight
 (16, 16, 3)" fillcolor=orange]
	140106869111200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869111584 -> 140106869111200
	140106869111584 -> 140107125193312 [dir=none]
	140107125193312 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869111584 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869110672 -> 140106869111584
	140106869110672 -> 140106798116800 [dir=none]
	140106798116800 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140106869110672 -> 140106788255792 [dir=none]
	140106788255792 [label="weight
 (32, 16, 2)" fillcolor=orange]
	140106869110672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (2,)
transposed        :           True
weight            : [saved tensor]"]
	140106869111872 -> 140106869110672
	140106869111872 -> 140107058265552 [dir=none]
	140107058265552 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140106869111872 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869112064 -> 140106869111872
	140106869112064 -> 140106788256672 [dir=none]
	140106788256672 [label="other
 ()" fillcolor=orange]
	140106869112064 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869112208 -> 140106869112064
	140106869112208 [label="AddBackward0
------------
alpha: 1"]
	140106869112304 -> 140106869112208
	140106869112304 [label="AddBackward0
------------
alpha: 1"]
	140106869112448 -> 140106869112304
	140106869112448 [label="AddBackward0
------------
alpha: 1"]
	140106869112592 -> 140106869112448
	140106869112592 -> 140106788248912 [dir=none]
	140106788248912 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140106869112592 -> 140106788248992 [dir=none]
	140106788248992 [label="weight
 (32, 32, 3)" fillcolor=orange]
	140106869112592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869112784 -> 140106869112592
	140106869112784 -> 140106788248752 [dir=none]
	140106788248752 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140106869112784 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604672720 -> 140106869112784
	140107604672720 -> 140106788248272 [dir=none]
	140106788248272 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107604672720 -> 140106788247952 [dir=none]
	140106788247952 [label="weight
 (32, 32, 3)" fillcolor=orange]
	140107604672720 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604672864 -> 140107604672720
	140107604672864 -> 140106788248032 [dir=none]
	140106788248032 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107604672864 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869112544 -> 140107604672864
	140106869112544 [label="AddBackward0
------------
alpha: 1"]
	140107604673104 -> 140106869112544
	140107604673104 -> 140106788247712 [dir=none]
	140106788247712 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107604673104 -> 140106788247872 [dir=none]
	140106788247872 [label="weight
 (32, 32, 3)" fillcolor=orange]
	140107604673104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604673296 -> 140107604673104
	140107604673296 -> 140106788247392 [dir=none]
	140106788247392 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107604673296 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604673488 -> 140107604673296
	140107604673488 -> 140106788246432 [dir=none]
	140106788246432 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107604673488 -> 140106788245792 [dir=none]
	140106788245792 [label="weight
 (32, 32, 3)" fillcolor=orange]
	140107604673488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604673632 -> 140107604673488
	140107604673632 -> 140106788246112 [dir=none]
	140106788246112 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107604673632 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604672960 -> 140107604673632
	140107604672960 [label="AddBackward0
------------
alpha: 1"]
	140107604673872 -> 140107604672960
	140107604673872 -> 140106788245712 [dir=none]
	140106788245712 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107604673872 -> 140106788241552 [dir=none]
	140106788241552 [label="weight
 (32, 32, 3)" fillcolor=orange]
	140107604673872 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604674064 -> 140107604673872
	140107604674064 -> 140106788245632 [dir=none]
	140106788245632 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107604674064 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604674256 -> 140107604674064
	140107604674256 -> 140106788245472 [dir=none]
	140106788245472 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107604674256 -> 140106788245552 [dir=none]
	140106788245552 [label="weight
 (32, 32, 3)" fillcolor=orange]
	140107604674256 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604674400 -> 140107604674256
	140107604674400 -> 140106788244272 [dir=none]
	140106788244272 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107604674400 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604673728 -> 140107604674400
	140107604673728 -> 140107053617120 [dir=none]
	140107053617120 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604673728 -> 140106800232656 [dir=none]
	140106800232656 [label="weight
 (64, 32, 2)" fillcolor=orange]
	140107604673728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (2,)
transposed        :           True
weight            : [saved tensor]"]
	140107604674688 -> 140107604673728
	140107604674688 -> 140107099127728 [dir=none]
	140107099127728 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604674688 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604674880 -> 140107604674688
	140107604674880 -> 140107110300096 [dir=none]
	140107110300096 [label="other
 ()" fillcolor=orange]
	140107604674880 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107604675024 -> 140107604674880
	140107604675024 [label="AddBackward0
------------
alpha: 1"]
	140107604675120 -> 140107604675024
	140107604675120 [label="AddBackward0
------------
alpha: 1"]
	140107604675264 -> 140107604675120
	140107604675264 [label="AddBackward0
------------
alpha: 1"]
	140107604675408 -> 140107604675264
	140107604675408 -> 140106794052608 [dir=none]
	140106794052608 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604675408 -> 140107125190512 [dir=none]
	140107125190512 [label="weight
 (64, 64, 3)" fillcolor=orange]
	140107604675408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604675552 -> 140107604675408
	140107604675552 -> 140106794046368 [dir=none]
	140106794046368 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604675552 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604675744 -> 140107604675552
	140107604675744 -> 140106794050528 [dir=none]
	140106794050528 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604675744 -> 140106794050048 [dir=none]
	140106794050048 [label="weight
 (64, 64, 3)" fillcolor=orange]
	140107604675744 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604675888 -> 140107604675744
	140107604675888 -> 140106794049648 [dir=none]
	140106794049648 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604675888 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604675360 -> 140107604675888
	140107604675360 [label="AddBackward0
------------
alpha: 1"]
	140107604676128 -> 140107604675360
	140107604676128 -> 140106794052528 [dir=none]
	140106794052528 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604676128 -> 140106794056128 [dir=none]
	140106794056128 [label="weight
 (64, 64, 3)" fillcolor=orange]
	140107604676128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604676320 -> 140107604676128
	140107604676320 -> 140107110295536 [dir=none]
	140107110295536 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604676320 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604676512 -> 140107604676320
	140107604676512 -> 140106788243312 [dir=none]
	140106788243312 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604676512 -> 140107031833120 [dir=none]
	140107031833120 [label="weight
 (64, 64, 3)" fillcolor=orange]
	140107604676512 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604676656 -> 140107604676512
	140107604676656 -> 140106788243232 [dir=none]
	140106788243232 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604676656 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604675984 -> 140107604676656
	140107604675984 [label="AddBackward0
------------
alpha: 1"]
	140107604676896 -> 140107604675984
	140107604676896 -> 140106788242992 [dir=none]
	140106788242992 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604676896 -> 140106788243072 [dir=none]
	140106788243072 [label="weight
 (64, 64, 3)" fillcolor=orange]
	140107604676896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604677088 -> 140107604676896
	140107604677088 -> 140106788242912 [dir=none]
	140106788242912 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604677088 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604677328 -> 140107604677088
	140107604677328 -> 140106788242352 [dir=none]
	140106788242352 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604677328 -> 140106788242752 [dir=none]
	140106788242752 [label="weight
 (64, 64, 3)" fillcolor=orange]
	140107604677328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604677472 -> 140107604677328
	140107604677472 -> 140107125192992 [dir=none]
	140107125192992 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604677472 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604676752 -> 140107604677472
	140107604676752 -> 140106794054608 [dir=none]
	140106794054608 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604676752 -> 140106788242032 [dir=none]
	140106788242032 [label="weight
 (128, 64, 8)" fillcolor=orange]
	140107604676752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (4,)
transposed        :           True
weight            : [saved tensor]"]
	140107604677760 -> 140107604676752
	140107604677760 -> 140106798118640 [dir=none]
	140106798118640 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604677760 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604677952 -> 140107604677760
	140107604677952 -> 140106784286944 [dir=none]
	140106784286944 [label="other
 ()" fillcolor=orange]
	140107604677952 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107604678096 -> 140107604677952
	140107604678096 [label="AddBackward0
------------
alpha: 1"]
	140107604678192 -> 140107604678096
	140107604678192 [label="AddBackward0
------------
alpha: 1"]
	140107604678336 -> 140107604678192
	140107604678336 [label="AddBackward0
------------
alpha: 1"]
	140107604678480 -> 140107604678336
	140107604678480 -> 140106794047488 [dir=none]
	140106794047488 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604678480 -> 140106794047648 [dir=none]
	140106794047648 [label="weight
 (128, 128, 3)" fillcolor=orange]
	140107604678480 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604678672 -> 140107604678480
	140107604678672 -> 140106794047408 [dir=none]
	140106794047408 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604678672 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604678864 -> 140107604678672
	140107604678864 -> 140106794047328 [dir=none]
	140106794047328 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604678864 -> 140106794046688 [dir=none]
	140106794046688 [label="weight
 (128, 128, 3)" fillcolor=orange]
	140107604678864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604679008 -> 140107604678864
	140107604679008 -> 140106794046848 [dir=none]
	140106794046848 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604679008 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604678432 -> 140107604679008
	140107604678432 [label="AddBackward0
------------
alpha: 1"]
	140107604679248 -> 140107604678432
	140107604679248 -> 140106794046528 [dir=none]
	140106794046528 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604679248 -> 140106794046608 [dir=none]
	140106794046608 [label="weight
 (128, 128, 3)" fillcolor=orange]
	140107604679248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604679440 -> 140107604679248
	140107604679440 -> 140106794046448 [dir=none]
	140106794046448 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604679440 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604679632 -> 140107604679440
	140107604679632 -> 140106794045088 [dir=none]
	140106794045088 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604679632 -> 140106794045728 [dir=none]
	140106794045728 [label="weight
 (128, 128, 3)" fillcolor=orange]
	140107604679632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604679776 -> 140107604679632
	140107604679776 -> 140106794046128 [dir=none]
	140106794046128 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604679776 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604679104 -> 140107604679776
	140107604679104 [label="AddBackward0
------------
alpha: 1"]
	140107604680016 -> 140107604679104
	140107604680016 -> 140106794046288 [dir=none]
	140106794046288 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604680016 -> 140106794046048 [dir=none]
	140106794046048 [label="weight
 (128, 128, 3)" fillcolor=orange]
	140107604680016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604680208 -> 140107604680016
	140107604680208 -> 140106794046208 [dir=none]
	140106794046208 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604680208 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604680400 -> 140107604680208
	140107604680400 -> 140106794045968 [dir=none]
	140106794045968 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604680400 -> 140106794044768 [dir=none]
	140106794044768 [label="weight
 (128, 128, 3)" fillcolor=orange]
	140107604680400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604680544 -> 140107604680400
	140107604680544 -> 140107125192672 [dir=none]
	140107125192672 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604680544 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604679872 -> 140107604680544
	140107604679872 -> 140107031824000 [dir=none]
	140107031824000 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604679872 -> 140106794045248 [dir=none]
	140106794045248 [label="weight
 (256, 128, 8)" fillcolor=orange]
	140107604679872 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (4,)
transposed        :           True
weight            : [saved tensor]"]
	140107604680832 -> 140107604679872
	140107604680832 -> 140107031828000 [dir=none]
	140107031828000 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604680832 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604681024 -> 140107604680832
	140107604681024 -> 140107092174912 [dir=none]
	140107092174912 [label="other
 ()" fillcolor=orange]
	140107604681024 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107604681168 -> 140107604681024
	140107604681168 [label="AddBackward0
------------
alpha: 1"]
	140107604681264 -> 140107604681168
	140107604681264 [label="AddBackward0
------------
alpha: 1"]
	140107604681408 -> 140107604681264
	140107604681408 [label="AddBackward0
------------
alpha: 1"]
	140107604681552 -> 140107604681408
	140107604681552 -> 140106794048848 [dir=none]
	140106794048848 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604681552 -> 140106794043888 [dir=none]
	140106794043888 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140107604681552 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604681696 -> 140107604681552
	140107604681696 -> 140106794048448 [dir=none]
	140106794048448 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604681696 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604681936 -> 140107604681696
	140107604681936 -> 140106798116640 [dir=none]
	140106798116640 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604681936 -> 140106800821920 [dir=none]
	140106800821920 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140107604681936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604682080 -> 140107604681936
	140107604682080 -> 140106798115520 [dir=none]
	140106798115520 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604682080 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604681504 -> 140107604682080
	140107604681504 [label="AddBackward0
------------
alpha: 1"]
	140107604682320 -> 140107604681504
	140107604682320 -> 140106800231216 [dir=none]
	140106800231216 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604682320 -> 140107934884272 [dir=none]
	140107934884272 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140107604682320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604682512 -> 140107604682320
	140107604682512 -> 140107019760512 [dir=none]
	140107019760512 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604682512 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604682704 -> 140107604682512
	140107604682704 -> 140106798115440 [dir=none]
	140106798115440 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604682704 -> 140106798111360 [dir=none]
	140106798111360 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140107604682704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604682848 -> 140107604682704
	140107604682848 -> 140106798114800 [dir=none]
	140106798114800 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604682848 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604682176 -> 140107604682848
	140107604682176 [label="AddBackward0
------------
alpha: 1"]
	140107604683088 -> 140107604682176
	140107604683088 -> 140106798104800 [dir=none]
	140106798104800 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604683088 -> 140106798109280 [dir=none]
	140106798109280 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140107604683088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604683280 -> 140107604683088
	140107604683280 -> 140106800234336 [dir=none]
	140106800234336 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604683280 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604683472 -> 140107604683280
	140107604683472 -> 140106800229296 [dir=none]
	140106800229296 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604683472 -> 140107125185712 [dir=none]
	140107125185712 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140107604683472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604683616 -> 140107604683472
	140107604683616 -> 140107071076800 [dir=none]
	140107071076800 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604683616 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604682944 -> 140107604683616
	140107604682944 -> 140107031826960 [dir=none]
	140107031826960 [label="input
 (7, 512, 50)" fillcolor=orange]
	140107604682944 -> 140107058257232 [dir=none]
	140107058257232 [label="weight
 (512, 256, 16)" fillcolor=orange]
	140107604682944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (4,)
stride            :           (8,)
transposed        :           True
weight            : [saved tensor]"]
	140107604683952 -> 140107604682944
	140107604683952 -> 140106800809200 [dir=none]
	140106800809200 [label="self
 (7, 512, 50)" fillcolor=orange]
	140107604683952 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604684144 -> 140107604683952
	140107604684144 [label="AddBackward0
------------
alpha: 1"]
	140107604684240 -> 140107604684144
	140107604684240 -> 140106806767152 [dir=none]
	140106806767152 [label="input
 (7, 192, 50)" fillcolor=orange]
	140107604684240 -> 140107125192192 [dir=none]
	140107125192192 [label="weight
 (512, 192, 7)" fillcolor=orange]
	140107604684240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604684480 -> 140107604684240
	140107604684480 [label=CopySlices]
	140107604673824 -> 140107604684480
	140107604673824 [label=CopySlices]
	140107604684720 -> 140107604673824
	140107604684720 [label=CopySlices]
	140107604684864 -> 140107604684720
	140107604684864 [label=CopySlices]
	140107604685008 -> 140107604684864
	140107604685008 [label=CopySlices]
	140107604685152 -> 140107604685008
	140107604685152 [label=CopySlices]
	140107604685296 -> 140107604685152
	140107604685296 [label=CopySlices]
	140107604685440 -> 140107604685296
	140107604685440 [label="SliceBackward0
--------------------------
dim           :          1
end           :        244
self_sym_sizes: (192, 286)
start         :        194
step          :          1"]
	140107604685536 -> 140107604685440
	140107604685536 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604685680 -> 140107604685536
	140107604685680 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             0
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604685680
	140107604685728 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107604685728 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107604686016 -> 140107604685728
	140107604686016 [label="AddBackward0
------------
alpha: 1"]
	140107604686112 -> 140107604686016
	140107604686112 [label="SplitBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 384, 286)
split_size    :           192"]
	140107604686256 -> 140107604686112
	140107604686256 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107604686256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107604686400 -> 140107604686256
	140107604686400 -> 140107058263952 [dir=none]
	140107058263952 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107604686400 -> 140107040384528 [dir=none]
	140107040384528 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107604686400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604686544 -> 140107604686400
	140107604686544 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107604686544 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107604686736 -> 140107604686544
	140107604686736 [label="AddBackward0
------------
alpha: 1"]
	140107604686832 -> 140107604686736
	140107604686832 [label="AddBackward0
------------
alpha: 1"]
	140107604686976 -> 140107604686832
	140107604686976 [label="AddBackward0
------------
alpha: 1"]
	140107604687120 -> 140107604686976
	140107604687120 [label="AddBackward0
------------
alpha: 1"]
	140107604687264 -> 140107604687120
	140107604687264 [label="AddBackward0
------------
alpha: 1"]
	140107604687408 -> 140107604687264
	140107604687408 [label="AddBackward0
------------
alpha: 1"]
	140107604687552 -> 140107604687408
	140107604687552 [label="AddBackward0
------------
alpha: 1"]
	140107604687696 -> 140107604687552
	140107604687696 [label="AddBackward0
------------
alpha: 1"]
	140107604687840 -> 140107604687696
	140107604687840 [label="AddBackward0
------------
alpha: 1"]
	140107604687984 -> 140107604687840
	140107604687984 [label="AddBackward0
------------
alpha: 1"]
	140107604688128 -> 140107604687984
	140107604688128 [label="AddBackward0
------------
alpha: 1"]
	140107604688272 -> 140107604688128
	140107604688272 [label="AddBackward0
------------
alpha: 1"]
	140107604688416 -> 140107604688272
	140107604688416 [label="AddBackward0
------------
alpha: 1"]
	140107604688560 -> 140107604688416
	140107604688560 [label="AddBackward0
------------
alpha: 1"]
	140107604688704 -> 140107604688560
	140107604688704 [label="AddBackward0
------------
alpha: 1"]
	140107604688848 -> 140107604688704
	140107604688848 [label="AddBackward0
------------
alpha: 1"]
	140107522408608 -> 140107604688848
	140107522408608 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522408704 -> 140107522408608
	140107522408704 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522408800 -> 140107522408704
	140107522408800 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522408896 -> 140107522408800
	140107522408896 -> 140107071069600 [dir=none]
	140107071069600 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522408896 -> 140107110300496 [dir=none]
	140107110300496 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522408896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522409184 -> 140107522408896
	140107522409184 -> 140106761905440 [dir=none]
	140106761905440 [label="other
 (7, 192, 286)" fillcolor=orange]
	140107522409184 -> 140106769819232 [dir=none]
	140106769819232 [label="self
 (7, 192, 286)" fillcolor=orange]
	140107522409184 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140107522409376 -> 140107522409184
	140107522409376 -> 140106757915808 [dir=none]
	140106757915808 [label="result
 (7, 192, 286)" fillcolor=orange]
	140107522409376 [label="TanhBackward0
----------------------
result: [saved tensor]"]
	140107522409520 -> 140107522409376
	140107522409520 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522409616 -> 140107522409520
	140107522409616 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522409664 -> 140107522409616
	140107522409664 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522409808 -> 140107522409664
	140107522409808 [label="AddBackward0
------------
alpha: 1"]
	140107522409952 -> 140107522409808
	140107522409952 -> 140107077228304 [dir=none]
	140107077228304 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522409952 -> 140107099121728 [dir=none]
	140107099121728 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522409952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522410240 -> 140107522409952
	140107522410240 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522410240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522410432 -> 140107522410240
	140107522410432 -> 140107110300416 [dir=none]
	140107110300416 [label="input
 (7, 1025, 286)" fillcolor=orange]
	140107522410432 -> 140107046551776 [dir=none]
	140107046551776 [label="weight
 (192, 1025, 1)" fillcolor=orange]
	140107522410432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522410576 -> 140107522410432
	140107046551776 [label="enc_q.pre.weight
 (192, 1025, 1)" fillcolor=lightblue]
	140107046551776 -> 140107522410576
	140107522410576 [label=AccumulateGrad]
	140107522410480 -> 140107522410432
	140107046552736 [label="enc_q.pre.bias
 (192)" fillcolor=lightblue]
	140107046552736 -> 140107522410480
	140107522410480 [label=AccumulateGrad]
	140107522410144 -> 140107522409952
	140107522410144 -> 140107046553136 [dir=none]
	140107046553136 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522410144 -> 140107934855504 [dir=none]
	140107934855504 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522410144 -> 140107046554416 [dir=none]
	140107046554416 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522410144 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522410624 -> 140107522410144
	140107046554416 [label="enc_q.enc.in_layers.0.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046554416 -> 140107522410624
	140107522410624 [label=AccumulateGrad]
	140107522410336 -> 140107522410144
	140107046553136 [label="enc_q.enc.in_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046553136 -> 140107522410336
	140107522410336 [label=AccumulateGrad]
	140107522410096 -> 140107522409952
	140107046553856 [label="enc_q.enc.in_layers.0.bias
 (384)" fillcolor=lightblue]
	140107046553856 -> 140107522410096
	140107522410096 [label=AccumulateGrad]
	140107522409904 -> 140107522409808
	140107522409904 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522410672 -> 140107522409904
	140107522410672 [label="SliceBackward0
----------------------------
dim           :            1
end           :          384
self_sym_sizes: (7, 6144, 1)
start         :            0
step          :            1"]
	140107522410768 -> 140107522410672
	140107522410768 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522410768
	140107522410816 -> 140107092171792 [dir=none]
	140107092171792 [label="input
 (7, 512, 1)" fillcolor=orange]
	140107522410816 -> 140107077237344 [dir=none]
	140107077237344 [label="weight
 (6144, 512, 1)" fillcolor=orange]
	140107522410816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (6144,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522411152 -> 140107522410816
	140107522411152 -> 140107053627040 [dir=none]
	140107053627040 [label="g
 (6144, 1, 1)" fillcolor=orange]
	140107522411152 -> 140106784283664 [dir=none]
	140106784283664 [label="result1
 (6144, 1, 1)" fillcolor=orange]
	140107522411152 -> 140107046553776 [dir=none]
	140107046553776 [label="v
 (6144, 512, 1)" fillcolor=orange]
	140107522411152 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522411296 -> 140107522411152
	140107046553776 [label="enc_q.enc.cond_layer.weight_v
 (6144, 512, 1)" fillcolor=lightblue]
	140107046553776 -> 140107522411296
	140107522411296 [label=AccumulateGrad]
	140107522411248 -> 140107522411152
	140107053627040 [label="enc_q.enc.cond_layer.weight_g
 (6144, 1, 1)" fillcolor=lightblue]
	140107053627040 -> 140107522411248
	140107522411248 [label=AccumulateGrad]
	140107522410960 -> 140107522410816
	140107046553376 [label="enc_q.enc.cond_layer.bias
 (6144)" fillcolor=lightblue]
	140107046553376 -> 140107522410960
	140107522410960 [label=AccumulateGrad]
	140107522409328 -> 140107522409184
	140107522409328 -> 140106774838496 [dir=none]
	140106774838496 [label="result
 (7, 192, 286)" fillcolor=orange]
	140107522409328 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140107522409424 -> 140107522409328
	140107522409424 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522410048 -> 140107522409424
	140107522410048 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522409664 -> 140107522410048
	140107522409040 -> 140107522408896
	140107522409040 -> 140107046554096 [dir=none]
	140107046554096 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522409040 -> 140106774834016 [dir=none]
	140106774834016 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522409040 -> 140107046555296 [dir=none]
	140107046555296 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522409040 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522409760 -> 140107522409040
	140107046555296 [label="enc_q.enc.res_skip_layers.0.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046555296 -> 140107522409760
	140107522409760 [label=AccumulateGrad]
	140107522409568 -> 140107522409040
	140107046554096 [label="enc_q.enc.res_skip_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046554096 -> 140107522409568
	140107522409568 [label=AccumulateGrad]
	140107522408992 -> 140107522408896
	140107046554816 [label="enc_q.enc.res_skip_layers.0.bias
 (384)" fillcolor=lightblue]
	140107046554816 -> 140107522408992
	140107522408992 [label=AccumulateGrad]
	140107604688800 -> 140107604688704
	140107604688800 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522408752 -> 140107604688800
	140107522408752 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522409232 -> 140107522408752
	140107522409232 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522410384 -> 140107522409232
	140107522410384 -> 140107103451584 [dir=none]
	140107103451584 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522410384 -> 140107071070080 [dir=none]
	140107071070080 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522410384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522409472 -> 140107522410384
	140107522409472 [label=CppFunction]
	140107522411344 -> 140107522409472
	140107522411344 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522411488 -> 140107522411344
	140107522411488 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522411584 -> 140107522411488
	140107522411584 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522411680 -> 140107522411584
	140107522411680 [label="AddBackward0
------------
alpha: 1"]
	140107522411776 -> 140107522411680
	140107522411776 -> 140107071070160 [dir=none]
	140107071070160 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522411776 -> 140107099126288 [dir=none]
	140107099126288 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522411776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522411920 -> 140107522411776
	140107522411920 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522411920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522412112 -> 140107522411920
	140107522412112 [label="AddBackward0
------------
alpha: 1"]
	140107522410240 -> 140107522412112
	140107522412208 -> 140107522412112
	140107522412208 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522412304 -> 140107522412208
	140107522412304 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522412400 -> 140107522412304
	140107522412400 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522408896 -> 140107522412400
	140107522411872 -> 140107522411776
	140107522411872 -> 140107046554976 [dir=none]
	140107046554976 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522411872 -> 140106757919648 [dir=none]
	140106757919648 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522411872 -> 140107046555776 [dir=none]
	140107046555776 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522411872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522412256 -> 140107522411872
	140107046555776 [label="enc_q.enc.in_layers.1.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046555776 -> 140107522412256
	140107522412256 [label=AccumulateGrad]
	140107522412160 -> 140107522411872
	140107046554976 [label="enc_q.enc.in_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046554976 -> 140107522412160
	140107522412160 [label=AccumulateGrad]
	140107522411824 -> 140107522411776
	140107046555376 [label="enc_q.enc.in_layers.1.bias
 (384)" fillcolor=lightblue]
	140107046555376 -> 140107522411824
	140107522411824 [label=AccumulateGrad]
	140107522411728 -> 140107522411680
	140107522411728 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522412352 -> 140107522411728
	140107522412352 [label="SliceBackward0
----------------------------
dim           :            1
end           :          768
self_sym_sizes: (7, 6144, 1)
start         :          384
step          :            1"]
	140107522412448 -> 140107522412352
	140107522412448 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522412448
	140107522411392 -> 140107522409472
	140107522411392 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522411632 -> 140107522411392
	140107522411632 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522411584 -> 140107522411632
	140107522409280 -> 140107522410384
	140107522409280 -> 140107046555536 [dir=none]
	140107046555536 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522409280 -> 140106800234176 [dir=none]
	140106800234176 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522409280 -> 140107046558176 [dir=none]
	140107046558176 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522409280 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522408848 -> 140107522409280
	140107046558176 [label="enc_q.enc.res_skip_layers.1.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046558176 -> 140107522408848
	140107522408848 [label=AccumulateGrad]
	140107522411536 -> 140107522409280
	140107046555536 [label="enc_q.enc.res_skip_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046555536 -> 140107522411536
	140107522411536 [label=AccumulateGrad]
	140107522408560 -> 140107522410384
	140107046556496 [label="enc_q.enc.res_skip_layers.1.bias
 (384)" fillcolor=lightblue]
	140107046556496 -> 140107522408560
	140107522408560 [label=AccumulateGrad]
	140107604688656 -> 140107604688560
	140107604688656 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604688752 -> 140107604688656
	140107604688752 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522410288 -> 140107604688752
	140107522410288 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522411200 -> 140107522410288
	140107522411200 -> 140107065190544 [dir=none]
	140107065190544 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522411200 -> 140107077236224 [dir=none]
	140107077236224 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522411200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522412064 -> 140107522411200
	140107522412064 [label=CppFunction]
	140107522412592 -> 140107522412064
	140107522412592 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522412736 -> 140107522412592
	140107522412736 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522412832 -> 140107522412736
	140107522412832 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522412928 -> 140107522412832
	140107522412928 [label="AddBackward0
------------
alpha: 1"]
	140107522413024 -> 140107522412928
	140107522413024 -> 140107071068800 [dir=none]
	140107071068800 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522413024 -> 140107071070560 [dir=none]
	140107071070560 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522413024 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522413168 -> 140107522413024
	140107522413168 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522413168 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522413312 -> 140107522413168
	140107522413312 [label="AddBackward0
------------
alpha: 1"]
	140107522411920 -> 140107522413312
	140107522413504 -> 140107522413312
	140107522413504 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522413600 -> 140107522413504
	140107522413600 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522413696 -> 140107522413600
	140107522413696 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522410384 -> 140107522413696
	140107522413120 -> 140107522413024
	140107522413120 -> 140107046556736 [dir=none]
	140107046556736 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522413120 -> 140106752172944 [dir=none]
	140106752172944 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522413120 -> 140107046559136 [dir=none]
	140107046559136 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522413120 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522413552 -> 140107522413120
	140107046559136 [label="enc_q.enc.in_layers.2.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046559136 -> 140107522413552
	140107522413552 [label=AccumulateGrad]
	140107522413456 -> 140107522413120
	140107046556736 [label="enc_q.enc.in_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046556736 -> 140107522413456
	140107522413456 [label=AccumulateGrad]
	140107522413072 -> 140107522413024
	140107046558736 [label="enc_q.enc.in_layers.2.bias
 (384)" fillcolor=lightblue]
	140107046558736 -> 140107522413072
	140107522413072 [label=AccumulateGrad]
	140107522412976 -> 140107522412928
	140107522412976 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522413648 -> 140107522412976
	140107522413648 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1152
self_sym_sizes: (7, 6144, 1)
start         :          768
step          :            1"]
	140107522413744 -> 140107522413648
	140107522413744 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522413744
	140107522411968 -> 140107522412064
	140107522411968 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522412880 -> 140107522411968
	140107522412880 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522412832 -> 140107522412880
	140107522410912 -> 140107522411200
	140107522410912 -> 140107046558896 [dir=none]
	140107046558896 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522410912 -> 140106757921088 [dir=none]
	140106757921088 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522410912 -> 140107046559856 [dir=none]
	140107046559856 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522410912 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522413792 -> 140107522410912
	140107046559856 [label="enc_q.enc.res_skip_layers.2.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046559856 -> 140107522413792
	140107522413792 [label=AccumulateGrad]
	140107522412784 -> 140107522410912
	140107046558896 [label="enc_q.enc.res_skip_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046558896 -> 140107522412784
	140107522412784 [label=AccumulateGrad]
	140107522408656 -> 140107522411200
	140107046559216 [label="enc_q.enc.res_skip_layers.2.bias
 (384)" fillcolor=lightblue]
	140107046559216 -> 140107522408656
	140107522408656 [label=AccumulateGrad]
	140107604688512 -> 140107604688416
	140107604688512 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604688608 -> 140107604688512
	140107604688608 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522412544 -> 140107604688608
	140107522412544 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522412640 -> 140107522412544
	140107522412640 -> 140107110298816 [dir=none]
	140107110298816 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522412640 -> 140107065187824 [dir=none]
	140107065187824 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522412640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522413264 -> 140107522412640
	140107522413264 [label=CppFunction]
	140107522413888 -> 140107522413264
	140107522413888 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522414032 -> 140107522413888
	140107522414032 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522414128 -> 140107522414032
	140107522414128 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522414224 -> 140107522414128
	140107522414224 [label="AddBackward0
------------
alpha: 1"]
	140107522414320 -> 140107522414224
	140107522414320 -> 140107071072240 [dir=none]
	140107071072240 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522414320 -> 140107071073840 [dir=none]
	140107071073840 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522414320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522414464 -> 140107522414320
	140107522414464 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522414464 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522414656 -> 140107522414464
	140107522414656 [label="AddBackward0
------------
alpha: 1"]
	140107522413168 -> 140107522414656
	140107522414752 -> 140107522414656
	140107522414752 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522414848 -> 140107522414752
	140107522414848 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522414944 -> 140107522414848
	140107522414944 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522411200 -> 140107522414944
	140107522414416 -> 140107522414320
	140107522414416 -> 140107046559376 [dir=none]
	140107046559376 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522414416 -> 140107103443984 [dir=none]
	140107103443984 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522414416 -> 140107046560736 [dir=none]
	140107046560736 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522414416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522414800 -> 140107522414416
	140107046560736 [label="enc_q.enc.in_layers.3.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046560736 -> 140107522414800
	140107522414800 [label=AccumulateGrad]
	140107522414704 -> 140107522414416
	140107046559376 [label="enc_q.enc.in_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046559376 -> 140107522414704
	140107522414704 [label=AccumulateGrad]
	140107522414368 -> 140107522414320
	140107046560016 [label="enc_q.enc.in_layers.3.bias
 (384)" fillcolor=lightblue]
	140107046560016 -> 140107522414368
	140107522414368 [label=AccumulateGrad]
	140107522414272 -> 140107522414224
	140107522414272 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522414896 -> 140107522414272
	140107522414896 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1536
self_sym_sizes: (7, 6144, 1)
start         :         1152
step          :            1"]
	140107522414992 -> 140107522414896
	140107522414992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522414992
	140107522413216 -> 140107522413264
	140107522413216 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522413936 -> 140107522413216
	140107522413936 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522414128 -> 140107522413936
	140107522412016 -> 140107522412640
	140107522412016 -> 140107938286464 [dir=none]
	140107938286464 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522412016 -> 140106731492176 [dir=none]
	140106731492176 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522412016 -> 140107046561136 [dir=none]
	140107046561136 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522412016 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522414608 -> 140107522412016
	140107046561136 [label="enc_q.enc.res_skip_layers.3.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046561136 -> 140107522414608
	140107522414608 [label=AccumulateGrad]
	140107522414176 -> 140107522412016
	140107938286464 [label="enc_q.enc.res_skip_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107938286464 -> 140107522414176
	140107522414176 [label=AccumulateGrad]
	140107522408512 -> 140107522412640
	140107046561056 [label="enc_q.enc.res_skip_layers.3.bias
 (384)" fillcolor=lightblue]
	140107046561056 -> 140107522408512
	140107522408512 [label=AccumulateGrad]
	140107604688368 -> 140107604688272
	140107604688368 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604688464 -> 140107604688368
	140107604688464 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522412496 -> 140107604688464
	140107522412496 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522410720 -> 140107522412496
	140107522410720 -> 140107071069920 [dir=none]
	140107071069920 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522410720 -> 140107065190624 [dir=none]
	140107065190624 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522410720 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522415040 -> 140107522410720
	140107522415040 [label=CppFunction]
	140107522414512 -> 140107522415040
	140107522414512 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522415232 -> 140107522414512
	140107522415232 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522415328 -> 140107522415232
	140107522415328 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522415424 -> 140107522415328
	140107522415424 [label="AddBackward0
------------
alpha: 1"]
	140107522415520 -> 140107522415424
	140107522415520 -> 140107065189824 [dir=none]
	140107065189824 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522415520 -> 140107065188544 [dir=none]
	140107065188544 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522415520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522415664 -> 140107522415520
	140107522415664 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522415664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522415856 -> 140107522415664
	140107522415856 [label="AddBackward0
------------
alpha: 1"]
	140107522414464 -> 140107522415856
	140107522415952 -> 140107522415856
	140107522415952 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522416048 -> 140107522415952
	140107522416048 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522416144 -> 140107522416048
	140107522416144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522412640 -> 140107522416144
	140107522415616 -> 140107522415520
	140107522415616 -> 140107046560096 [dir=none]
	140107046560096 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522415616 -> 140107103444064 [dir=none]
	140107103444064 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522415616 -> 140107046561536 [dir=none]
	140107046561536 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522415616 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522416000 -> 140107522415616
	140107046561536 [label="enc_q.enc.in_layers.4.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046561536 -> 140107522416000
	140107522416000 [label=AccumulateGrad]
	140107522415904 -> 140107522415616
	140107046560096 [label="enc_q.enc.in_layers.4.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046560096 -> 140107522415904
	140107522415904 [label=AccumulateGrad]
	140107522415568 -> 140107522415520
	140107046561216 [label="enc_q.enc.in_layers.4.bias
 (384)" fillcolor=lightblue]
	140107046561216 -> 140107522415568
	140107522415568 [label=AccumulateGrad]
	140107522415472 -> 140107522415424
	140107522415472 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522416096 -> 140107522415472
	140107522416096 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1920
self_sym_sizes: (7, 6144, 1)
start         :         1536
step          :            1"]
	140107522416192 -> 140107522416096
	140107522416192 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522416192
	140107522414560 -> 140107522415040
	140107522414560 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522415376 -> 140107522414560
	140107522415376 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522415328 -> 140107522415376
	140107522414080 -> 140107522410720
	140107522414080 -> 140107046561296 [dir=none]
	140107046561296 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522414080 -> 140106731492416 [dir=none]
	140106731492416 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522414080 -> 140107046562256 [dir=none]
	140107046562256 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522414080 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522415136 -> 140107522414080
	140107046562256 [label="enc_q.enc.res_skip_layers.4.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046562256 -> 140107522415136
	140107522415136 [label=AccumulateGrad]
	140107522415280 -> 140107522414080
	140107046561296 [label="enc_q.enc.res_skip_layers.4.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046561296 -> 140107522415280
	140107522415280 [label=AccumulateGrad]
	140107522413840 -> 140107522410720
	140107046561856 [label="enc_q.enc.res_skip_layers.4.bias
 (384)" fillcolor=lightblue]
	140107046561856 -> 140107522413840
	140107522413840 [label=AccumulateGrad]
	140107604688224 -> 140107604688128
	140107604688224 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604688320 -> 140107604688224
	140107604688320 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522413408 -> 140107604688320
	140107522413408 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522415184 -> 140107522413408
	140107522415184 -> 140107071073200 [dir=none]
	140107071073200 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522415184 -> 140107065195664 [dir=none]
	140107065195664 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522415184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522416288 -> 140107522415184
	140107522416288 [label=CppFunction]
	140107522416384 -> 140107522416288
	140107522416384 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522416528 -> 140107522416384
	140107522416528 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522416624 -> 140107522416528
	140107522416624 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522416720 -> 140107522416624
	140107522416720 [label="AddBackward0
------------
alpha: 1"]
	140107522416816 -> 140107522416720
	140107522416816 -> 140107065189104 [dir=none]
	140107065189104 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522416816 -> 140107065190384 [dir=none]
	140107065190384 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522416816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522416960 -> 140107522416816
	140107522416960 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522416960 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522417152 -> 140107522416960
	140107522417152 [label="AddBackward0
------------
alpha: 1"]
	140107522415664 -> 140107522417152
	140107522417248 -> 140107522417152
	140107522417248 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522417344 -> 140107522417248
	140107522417344 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522417440 -> 140107522417344
	140107522417440 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522410720 -> 140107522417440
	140107522416912 -> 140107522416816
	140107522416912 -> 140107046561936 [dir=none]
	140107046561936 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522416912 -> 140106731492976 [dir=none]
	140106731492976 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522416912 -> 140107046562656 [dir=none]
	140107046562656 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522416912 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522417296 -> 140107522416912
	140107046562656 [label="enc_q.enc.in_layers.5.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046562656 -> 140107522417296
	140107522417296 [label=AccumulateGrad]
	140107522417200 -> 140107522416912
	140107046561936 [label="enc_q.enc.in_layers.5.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046561936 -> 140107522417200
	140107522417200 [label=AccumulateGrad]
	140107522416864 -> 140107522416816
	140107046562336 [label="enc_q.enc.in_layers.5.bias
 (384)" fillcolor=lightblue]
	140107046562336 -> 140107522416864
	140107522416864 [label=AccumulateGrad]
	140107522416768 -> 140107522416720
	140107522416768 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522417392 -> 140107522416768
	140107522417392 [label="SliceBackward0
----------------------------
dim           :            1
end           :         2304
self_sym_sizes: (7, 6144, 1)
start         :         1920
step          :            1"]
	140107522417104 -> 140107522417392
	140107522417104 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522417104
	140107522416336 -> 140107522416288
	140107522416336 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522416672 -> 140107522416336
	140107522416672 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522416624 -> 140107522416672
	140107522415088 -> 140107522415184
	140107522415088 -> 140107046562416 [dir=none]
	140107046562416 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522415088 -> 140106731492336 [dir=none]
	140106731492336 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522415088 -> 140107046562976 [dir=none]
	140107046562976 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522415088 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522416240 -> 140107522415088
	140107046562976 [label="enc_q.enc.res_skip_layers.5.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046562976 -> 140107522416240
	140107522416240 [label=AccumulateGrad]
	140107522416576 -> 140107522415088
	140107046562416 [label="enc_q.enc.res_skip_layers.5.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046562416 -> 140107522416576
	140107522416576 [label=AccumulateGrad]
	140107522411440 -> 140107522415184
	140107046562736 [label="enc_q.enc.res_skip_layers.5.bias
 (384)" fillcolor=lightblue]
	140107046562736 -> 140107522411440
	140107522411440 [label=AccumulateGrad]
	140107604688080 -> 140107604687984
	140107604688080 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604688176 -> 140107604688080
	140107604688176 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522415760 -> 140107604688176
	140107522415760 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522416432 -> 140107522415760
	140107522416432 -> 140107065188864 [dir=none]
	140107065188864 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522416432 -> 140107065196144 [dir=none]
	140107065196144 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522416432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522417008 -> 140107522416432
	140107522417008 [label=CppFunction]
	140107522417584 -> 140107522417008
	140107522417584 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522417728 -> 140107522417584
	140107522417728 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522417824 -> 140107522417728
	140107522417824 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522417920 -> 140107522417824
	140107522417920 [label="AddBackward0
------------
alpha: 1"]
	140107522418016 -> 140107522417920
	140107522418016 -> 140107065190704 [dir=none]
	140107065190704 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522418016 -> 140107065195184 [dir=none]
	140107065195184 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522418016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522418160 -> 140107522418016
	140107522418160 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522418160 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522418352 -> 140107522418160
	140107522418352 [label="AddBackward0
------------
alpha: 1"]
	140107522416960 -> 140107522418352
	140107522418448 -> 140107522418352
	140107522418448 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522418544 -> 140107522418448
	140107522418544 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522418640 -> 140107522418544
	140107522418640 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522415184 -> 140107522418640
	140107522418112 -> 140107522418016
	140107522418112 -> 140107046562816 [dir=none]
	140107046562816 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522418112 -> 140106778581392 [dir=none]
	140106778581392 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522418112 -> 140107046563376 [dir=none]
	140107046563376 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522418112 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522418496 -> 140107522418112
	140107046563376 [label="enc_q.enc.in_layers.6.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046563376 -> 140107522418496
	140107522418496 [label=AccumulateGrad]
	140107522418400 -> 140107522418112
	140107046562816 [label="enc_q.enc.in_layers.6.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046562816 -> 140107522418400
	140107522418400 [label=AccumulateGrad]
	140107522418064 -> 140107522418016
	140107046563056 [label="enc_q.enc.in_layers.6.bias
 (384)" fillcolor=lightblue]
	140107046563056 -> 140107522418064
	140107522418064 [label=AccumulateGrad]
	140107522417968 -> 140107522417920
	140107522417968 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522418592 -> 140107522417968
	140107522418592 [label="SliceBackward0
----------------------------
dim           :            1
end           :         2688
self_sym_sizes: (7, 6144, 1)
start         :         2304
step          :            1"]
	140107522418688 -> 140107522418592
	140107522418688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522418688
	140107522417536 -> 140107522417008
	140107522417536 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522417872 -> 140107522417536
	140107522417872 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522417824 -> 140107522417872
	140107522415712 -> 140107522416432
	140107522415712 -> 140107046563216 [dir=none]
	140107046563216 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522415712 -> 140106731492496 [dir=none]
	140106731492496 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522415712 -> 140107046563936 [dir=none]
	140107046563936 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522415712 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522418736 -> 140107522415712
	140107046563936 [label="enc_q.enc.res_skip_layers.6.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046563936 -> 140107522418736
	140107522418736 [label=AccumulateGrad]
	140107522417776 -> 140107522415712
	140107046563216 [label="enc_q.enc.res_skip_layers.6.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046563216 -> 140107522417776
	140107522417776 [label=AccumulateGrad]
	140107522413360 -> 140107522416432
	140107046563536 [label="enc_q.enc.res_skip_layers.6.bias
 (384)" fillcolor=lightblue]
	140107046563536 -> 140107522413360
	140107522413360 [label=AccumulateGrad]
	140107604687936 -> 140107604687840
	140107604687936 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604688032 -> 140107604687936
	140107604688032 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522417056 -> 140107604688032
	140107522417056 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522417632 -> 140107522417056
	140107522417632 -> 140107065190304 [dir=none]
	140107065190304 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522417632 -> 140107058250112 [dir=none]
	140107058250112 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522417632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522418304 -> 140107522417632
	140107522418304 [label=CppFunction]
	140107522418928 -> 140107522418304
	140107522418928 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522419072 -> 140107522418928
	140107522419072 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522419168 -> 140107522419072
	140107522419168 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522419264 -> 140107522419168
	140107522419264 [label="AddBackward0
------------
alpha: 1"]
	140107522419360 -> 140107522419264
	140107522419360 -> 140107065195904 [dir=none]
	140107065195904 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522419360 -> 140107065196064 [dir=none]
	140107065196064 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522419360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522419504 -> 140107522419360
	140107522419504 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522419504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522419696 -> 140107522419504
	140107522419696 [label="AddBackward0
------------
alpha: 1"]
	140107522418160 -> 140107522419696
	140107522419792 -> 140107522419696
	140107522419792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522419888 -> 140107522419792
	140107522419888 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522419984 -> 140107522419888
	140107522419984 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522416432 -> 140107522419984
	140107522419456 -> 140107522419360
	140107522419456 -> 140107046563696 [dir=none]
	140107046563696 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522419456 -> 140106731492256 [dir=none]
	140106731492256 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522419456 -> 140107046564256 [dir=none]
	140107046564256 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522419456 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522419840 -> 140107522419456
	140107046564256 [label="enc_q.enc.in_layers.7.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046564256 -> 140107522419840
	140107522419840 [label=AccumulateGrad]
	140107522419744 -> 140107522419456
	140107046563696 [label="enc_q.enc.in_layers.7.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046563696 -> 140107522419744
	140107522419744 [label=AccumulateGrad]
	140107522419408 -> 140107522419360
	140107046564016 [label="enc_q.enc.in_layers.7.bias
 (384)" fillcolor=lightblue]
	140107046564016 -> 140107522419408
	140107522419408 [label=AccumulateGrad]
	140107522419312 -> 140107522419264
	140107522419312 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522419936 -> 140107522419312
	140107522419936 [label="SliceBackward0
----------------------------
dim           :            1
end           :         3072
self_sym_sizes: (7, 6144, 1)
start         :         2688
step          :            1"]
	140107522420032 -> 140107522419936
	140107522420032 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522420032
	140107522418208 -> 140107522418304
	140107522418208 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522419216 -> 140107522418208
	140107522419216 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522419168 -> 140107522419216
	140107522417488 -> 140107522417632
	140107522417488 -> 140107046564096 [dir=none]
	140107046564096 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522417488 -> 140106731493856 [dir=none]
	140106731493856 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522417488 -> 140107046564896 [dir=none]
	140107046564896 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522417488 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522420080 -> 140107522417488
	140107046564896 [label="enc_q.enc.res_skip_layers.7.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046564896 -> 140107522420080
	140107522420080 [label=AccumulateGrad]
	140107522419120 -> 140107522417488
	140107046564096 [label="enc_q.enc.res_skip_layers.7.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046564096 -> 140107522419120
	140107522419120 [label=AccumulateGrad]
	140107522412688 -> 140107522417632
	140107046564416 [label="enc_q.enc.res_skip_layers.7.bias
 (384)" fillcolor=lightblue]
	140107046564416 -> 140107522412688
	140107522412688 [label=AccumulateGrad]
	140107604687792 -> 140107604687696
	140107604687792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604687888 -> 140107604687792
	140107604687888 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522418832 -> 140107604687888
	140107522418832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522418976 -> 140107522418832
	140107522418976 -> 140107065191504 [dir=none]
	140107065191504 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522418976 -> 140107058253152 [dir=none]
	140107058253152 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522418976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522419648 -> 140107522418976
	140107522419648 [label=CppFunction]
	140107522420176 -> 140107522419648
	140107522420176 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522420320 -> 140107522420176
	140107522420320 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522420416 -> 140107522420320
	140107522420416 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522420512 -> 140107522420416
	140107522420512 [label="AddBackward0
------------
alpha: 1"]
	140107522420608 -> 140107522420512
	140107522420608 -> 140107065196304 [dir=none]
	140107065196304 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522420608 -> 140107065196464 [dir=none]
	140107065196464 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522420608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522420752 -> 140107522420608
	140107522420752 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522420752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522420944 -> 140107522420752
	140107522420944 [label="AddBackward0
------------
alpha: 1"]
	140107522419504 -> 140107522420944
	140107522421040 -> 140107522420944
	140107522421040 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522421136 -> 140107522421040
	140107522421136 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522421232 -> 140107522421136
	140107522421232 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522417632 -> 140107522421232
	140107522420704 -> 140107522420608
	140107522420704 -> 140107046564576 [dir=none]
	140107046564576 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522420704 -> 140106731494256 [dir=none]
	140106731494256 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522420704 -> 140107046565376 [dir=none]
	140107046565376 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522420704 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522421088 -> 140107522420704
	140107046565376 [label="enc_q.enc.in_layers.8.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046565376 -> 140107522421088
	140107522421088 [label=AccumulateGrad]
	140107522420992 -> 140107522420704
	140107046564576 [label="enc_q.enc.in_layers.8.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046564576 -> 140107522420992
	140107522420992 [label=AccumulateGrad]
	140107522420656 -> 140107522420608
	140107046565056 [label="enc_q.enc.in_layers.8.bias
 (384)" fillcolor=lightblue]
	140107046565056 -> 140107522420656
	140107522420656 [label=AccumulateGrad]
	140107522420560 -> 140107522420512
	140107522420560 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522421184 -> 140107522420560
	140107522421184 [label="SliceBackward0
----------------------------
dim           :            1
end           :         3456
self_sym_sizes: (7, 6144, 1)
start         :         3072
step          :            1"]
	140107522421280 -> 140107522421184
	140107522421280 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522421280
	140107522419552 -> 140107522419648
	140107522419552 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522420464 -> 140107522419552
	140107522420464 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522420416 -> 140107522420464
	140107522418256 -> 140107522418976
	140107522418256 -> 140107046565136 [dir=none]
	140107046565136 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522418256 -> 140106731494656 [dir=none]
	140106731494656 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522418256 -> 140107046566096 [dir=none]
	140107046566096 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522418256 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522421328 -> 140107522418256
	140107046566096 [label="enc_q.enc.res_skip_layers.8.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046566096 -> 140107522421328
	140107522421328 [label=AccumulateGrad]
	140107522420368 -> 140107522418256
	140107046565136 [label="enc_q.enc.res_skip_layers.8.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046565136 -> 140107522420368
	140107522420368 [label=AccumulateGrad]
	140107522415808 -> 140107522418976
	140107046565616 [label="enc_q.enc.res_skip_layers.8.bias
 (384)" fillcolor=lightblue]
	140107046565616 -> 140107522415808
	140107522415808 [label=AccumulateGrad]
	140107604687648 -> 140107604687552
	140107604687648 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604687744 -> 140107604687648
	140107604687744 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522420128 -> 140107604687744
	140107522420128 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522420800 -> 140107522420128
	140107522420800 -> 140107065195984 [dir=none]
	140107065195984 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522420800 -> 140107058256192 [dir=none]
	140107058256192 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522420800 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522421472 -> 140107522420800
	140107522421472 [label=CppFunction]
	140107522421664 -> 140107522421472
	140107522421664 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522421808 -> 140107522421664
	140107522421808 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522421904 -> 140107522421808
	140107522421904 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522422000 -> 140107522421904
	140107522422000 [label="AddBackward0
------------
alpha: 1"]
	140107522422096 -> 140107522422000
	140107522422096 -> 140107058250912 [dir=none]
	140107058250912 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522422096 -> 140107058250432 [dir=none]
	140107058250432 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522422096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522422240 -> 140107522422096
	140107522422240 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522422240 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522422432 -> 140107522422240
	140107522422432 [label="AddBackward0
------------
alpha: 1"]
	140107522420752 -> 140107522422432
	140107522422528 -> 140107522422432
	140107522422528 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522422624 -> 140107522422528
	140107522422624 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522422720 -> 140107522422624
	140107522422720 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522418976 -> 140107522422720
	140107522422192 -> 140107522422096
	140107522422192 -> 140107046565696 [dir=none]
	140107046565696 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522422192 -> 140106731493696 [dir=none]
	140106731493696 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522422192 -> 140107046566496 [dir=none]
	140107046566496 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522422192 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522422576 -> 140107522422192
	140107046566496 [label="enc_q.enc.in_layers.9.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046566496 -> 140107522422576
	140107522422576 [label=AccumulateGrad]
	140107522422480 -> 140107522422192
	140107046565696 [label="enc_q.enc.in_layers.9.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046565696 -> 140107522422480
	140107522422480 [label=AccumulateGrad]
	140107522422144 -> 140107522422096
	140107046566176 [label="enc_q.enc.in_layers.9.bias
 (384)" fillcolor=lightblue]
	140107046566176 -> 140107522422144
	140107522422144 [label=AccumulateGrad]
	140107522422048 -> 140107522422000
	140107522422048 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522422672 -> 140107522422048
	140107522422672 [label="SliceBackward0
----------------------------
dim           :            1
end           :         3840
self_sym_sizes: (7, 6144, 1)
start         :         3456
step          :            1"]
	140107522422768 -> 140107522422672
	140107522422768 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522422768
	140107522421616 -> 140107522421472
	140107522421616 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522421952 -> 140107522421616
	140107522421952 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522421904 -> 140107522421952
	140107522421424 -> 140107522420800
	140107522421424 -> 140107046566336 [dir=none]
	140107046566336 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522421424 -> 140106731494496 [dir=none]
	140106731494496 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522421424 -> 140107046567136 [dir=none]
	140107046567136 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522421424 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522422816 -> 140107522421424
	140107046567136 [label="enc_q.enc.res_skip_layers.9.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107046567136 -> 140107522422816
	140107522422816 [label=AccumulateGrad]
	140107522421856 -> 140107522421424
	140107046566336 [label="enc_q.enc.res_skip_layers.9.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046566336 -> 140107522421856
	140107522421856 [label=AccumulateGrad]
	140107522416480 -> 140107522420800
	140107046566896 [label="enc_q.enc.res_skip_layers.9.bias
 (384)" fillcolor=lightblue]
	140107046566896 -> 140107522416480
	140107522416480 [label=AccumulateGrad]
	140107604687504 -> 140107604687408
	140107604687504 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604687600 -> 140107604687504
	140107604687600 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522421520 -> 140107604687600
	140107522421520 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522421712 -> 140107522421520
	140107522421712 -> 140107058249952 [dir=none]
	140107058249952 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522421712 -> 140107058259792 [dir=none]
	140107058259792 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522421712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522421376 -> 140107522421712
	140107522421376 [label=CppFunction]
	140107522422912 -> 140107522421376
	140107522422912 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522423056 -> 140107522422912
	140107522423056 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522423152 -> 140107522423056
	140107522423152 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522423248 -> 140107522423152
	140107522423248 [label="AddBackward0
------------
alpha: 1"]
	140107522423344 -> 140107522423248
	140107522423344 -> 140107058253872 [dir=none]
	140107058253872 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522423344 -> 140107058256032 [dir=none]
	140107058256032 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522423344 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522423488 -> 140107522423344
	140107522423488 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107522423488 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107522423680 -> 140107522423488
	140107522423680 [label="AddBackward0
------------
alpha: 1"]
	140107522422240 -> 140107522423680
	140107522423776 -> 140107522423680
	140107522423776 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522423872 -> 140107522423776
	140107522423872 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107522423968 -> 140107522423872
	140107522423968 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522420800 -> 140107522423968
	140107522423440 -> 140107522423344
	140107522423440 -> 140107046566976 [dir=none]
	140107046566976 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522423440 -> 140106731494416 [dir=none]
	140106731494416 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522423440 -> 140107046567456 [dir=none]
	140107046567456 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107522423440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522423824 -> 140107522423440
	140107046567456 [label="enc_q.enc.in_layers.10.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107046567456 -> 140107522423824
	140107522423824 [label=AccumulateGrad]
	140107522423728 -> 140107522423440
	140107046566976 [label="enc_q.enc.in_layers.10.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046566976 -> 140107522423728
	140107522423728 [label=AccumulateGrad]
	140107522423392 -> 140107522423344
	140107046567216 [label="enc_q.enc.in_layers.10.bias
 (384)" fillcolor=lightblue]
	140107046567216 -> 140107522423392
	140107522423392 [label=AccumulateGrad]
	140107522422336 -> 140107522423248
	140107522422336 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107522423920 -> 140107522422336
	140107522423920 [label="SliceBackward0
----------------------------
dim           :            1
end           :         4224
self_sym_sizes: (7, 6144, 1)
start         :         3840
step          :            1"]
	140107522424016 -> 140107522423920
	140107522424016 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107522424016
	140107522422288 -> 140107522421376
	140107522422288 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522423584 -> 140107522422288
	140107522423584 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522423152 -> 140107522423584
	140107522421568 -> 140107522421712
	140107522421568 -> 140107046567296 [dir=none]
	140107046567296 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522421568 -> 140106731494736 [dir=none]
	140106731494736 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522421568 -> 140107040375088 [dir=none]
	140107040375088 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522421568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522424160 -> 140107522421568
	140107040375088 [label="enc_q.enc.res_skip_layers.10.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040375088 -> 140107522424160
	140107522424160 [label=AccumulateGrad]
	140107522424112 -> 140107522421568
	140107046567296 [label="enc_q.enc.res_skip_layers.10.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046567296 -> 140107522424112
	140107522424112 [label=AccumulateGrad]
	140107522417680 -> 140107522421712
	140107934852464 [label="enc_q.enc.res_skip_layers.10.bias
 (384)" fillcolor=lightblue]
	140107934852464 -> 140107522417680
	140107522417680 [label=AccumulateGrad]
	140107604687360 -> 140107604687264
	140107604687360 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604687456 -> 140107604687360
	140107604687456 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522422864 -> 140107604687456
	140107522422864 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522423536 -> 140107522422864
	140107522423536 -> 140107058250352 [dir=none]
	140107058250352 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522423536 -> 140107058261072 [dir=none]
	140107058261072 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522423536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522424208 -> 140107522423536
	140107522424208 [label=CppFunction]
	140107522424400 -> 140107522424208
	140107522424400 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522424544 -> 140107522424400
	140107522424544 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107522424640 -> 140107522424544
	140107522424640 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522424736 -> 140107522424640
	140107522424736 [label="AddBackward0
------------
alpha: 1"]
	140107522424784 -> 140107522424736
	140107522424784 -> 140107058256352 [dir=none]
	140107058256352 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522424784 -> 140107058259712 [dir=none]
	140107058259712 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107522424784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508465872 -> 140107522424784
	140107508465872 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107508465872 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508466064 -> 140107508465872
	140107508466064 [label="AddBackward0
------------
alpha: 1"]
	140107522423488 -> 140107508466064
	140107508466160 -> 140107508466064
	140107508466160 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107508466256 -> 140107508466160
	140107508466256 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107508466352 -> 140107508466256
	140107508466352 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522421712 -> 140107508466352
	140107508465824 -> 140107522424784
	140107508465824 -> 140107046567856 [dir=none]
	140107046567856 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508465824 -> 140106731492016 [dir=none]
	140106731492016 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508465824 -> 140107040375488 [dir=none]
	140107040375488 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107508465824 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508466208 -> 140107508465824
	140107040375488 [label="enc_q.enc.in_layers.11.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040375488 -> 140107508466208
	140107508466208 [label=AccumulateGrad]
	140107508466112 -> 140107508465824
	140107046567856 [label="enc_q.enc.in_layers.11.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107046567856 -> 140107508466112
	140107508466112 [label=AccumulateGrad]
	140107508465776 -> 140107522424784
	140107040375168 [label="enc_q.enc.in_layers.11.bias
 (384)" fillcolor=lightblue]
	140107040375168 -> 140107508465776
	140107508465776 [label=AccumulateGrad]
	140107522424448 -> 140107522424736
	140107522424448 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107508466304 -> 140107522424448
	140107508466304 [label="SliceBackward0
----------------------------
dim           :            1
end           :         4608
self_sym_sizes: (7, 6144, 1)
start         :         4224
step          :            1"]
	140107508466400 -> 140107508466304
	140107508466400 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107508466400
	140107522424352 -> 140107522424208
	140107522424352 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107522424688 -> 140107522424352
	140107522424688 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107522424640 -> 140107522424688
	140107522422384 -> 140107522423536
	140107522422384 -> 140107040375248 [dir=none]
	140107040375248 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107522422384 -> 140106731494896 [dir=none]
	140106731494896 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107522422384 -> 140107040376128 [dir=none]
	140107040376128 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107522422384 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522423632 -> 140107522422384
	140107040376128 [label="enc_q.enc.res_skip_layers.11.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040376128 -> 140107522423632
	140107522423632 [label=AccumulateGrad]
	140107522424592 -> 140107522422384
	140107040375248 [label="enc_q.enc.res_skip_layers.11.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040375248 -> 140107522424592
	140107522424592 [label=AccumulateGrad]
	140107522419024 -> 140107522423536
	140107040375728 [label="enc_q.enc.res_skip_layers.11.bias
 (384)" fillcolor=lightblue]
	140107040375728 -> 140107522419024
	140107522419024 [label=AccumulateGrad]
	140107604687216 -> 140107604687120
	140107604687216 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604687312 -> 140107604687216
	140107604687312 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522424256 -> 140107604687312
	140107522424256 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522424304 -> 140107522424256
	140107522424304 -> 140107058255312 [dir=none]
	140107058255312 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107522424304 -> 140107058262272 [dir=none]
	140107058262272 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107522424304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522420848 -> 140107522424304
	140107522420848 [label=CppFunction]
	140107508465920 -> 140107522420848
	140107508465920 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508466688 -> 140107508465920
	140107508466688 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107508466784 -> 140107508466688
	140107508466784 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508466880 -> 140107508466784
	140107508466880 [label="AddBackward0
------------
alpha: 1"]
	140107508466976 -> 140107508466880
	140107508466976 -> 140107058260032 [dir=none]
	140107058260032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107508466976 -> 140107058260512 [dir=none]
	140107058260512 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107508466976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508467120 -> 140107508466976
	140107508467120 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107508467120 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508467312 -> 140107508467120
	140107508467312 [label="AddBackward0
------------
alpha: 1"]
	140107508465872 -> 140107508467312
	140107508467408 -> 140107508467312
	140107508467408 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107508467504 -> 140107508467408
	140107508467504 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107508467600 -> 140107508467504
	140107508467600 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522423536 -> 140107508467600
	140107508467072 -> 140107508466976
	140107508467072 -> 140107040375808 [dir=none]
	140107040375808 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508467072 -> 140106757919008 [dir=none]
	140106757919008 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508467072 -> 140107040376928 [dir=none]
	140107040376928 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107508467072 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508467456 -> 140107508467072
	140107040376928 [label="enc_q.enc.in_layers.12.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040376928 -> 140107508467456
	140107508467456 [label=AccumulateGrad]
	140107508467360 -> 140107508467072
	140107040375808 [label="enc_q.enc.in_layers.12.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040375808 -> 140107508467360
	140107508467360 [label=AccumulateGrad]
	140107508467024 -> 140107508466976
	140107040376368 [label="enc_q.enc.in_layers.12.bias
 (384)" fillcolor=lightblue]
	140107040376368 -> 140107508467024
	140107508467024 [label=AccumulateGrad]
	140107508466928 -> 140107508466880
	140107508466928 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107508467552 -> 140107508466928
	140107508467552 [label="SliceBackward0
----------------------------
dim           :            1
end           :         4992
self_sym_sizes: (7, 6144, 1)
start         :         4608
step          :            1"]
	140107508467648 -> 140107508467552
	140107508467648 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107508467648
	140107508465968 -> 140107522420848
	140107508465968 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508466832 -> 140107508465968
	140107508466832 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508466784 -> 140107508466832
	140107508466448 -> 140107522424304
	140107508466448 -> 140107040376528 [dir=none]
	140107040376528 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508466448 -> 140106731495616 [dir=none]
	140106731495616 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508466448 -> 140107040377488 [dir=none]
	140107040377488 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107508466448 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508467696 -> 140107508466448
	140107040377488 [label="enc_q.enc.res_skip_layers.12.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040377488 -> 140107508467696
	140107508467696 [label=AccumulateGrad]
	140107508466736 -> 140107508466448
	140107040376528 [label="enc_q.enc.res_skip_layers.12.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040376528 -> 140107508466736
	140107508466736 [label=AccumulateGrad]
	140107508465728 -> 140107522424304
	140107040377008 [label="enc_q.enc.res_skip_layers.12.bias
 (384)" fillcolor=lightblue]
	140107040377008 -> 140107508465728
	140107508465728 [label=AccumulateGrad]
	140107604687072 -> 140107604686976
	140107604687072 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107604687168 -> 140107604687072
	140107604687168 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107522424496 -> 140107604687168
	140107522424496 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107508466544 -> 140107522424496
	140107508466544 -> 140107058256592 [dir=none]
	140107058256592 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107508466544 -> 140107058262592 [dir=none]
	140107058262592 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107508466544 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508467264 -> 140107508466544
	140107508467264 [label=CppFunction]
	140107508467792 -> 140107508467264
	140107508467792 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508467936 -> 140107508467792
	140107508467936 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107508468032 -> 140107508467936
	140107508468032 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508468128 -> 140107508468032
	140107508468128 [label="AddBackward0
------------
alpha: 1"]
	140107508468224 -> 140107508468128
	140107508468224 -> 140107058262032 [dir=none]
	140107058262032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107508468224 -> 140107058262192 [dir=none]
	140107058262192 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107508468224 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508468368 -> 140107508468224
	140107508468368 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107508468368 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508468560 -> 140107508468368
	140107508468560 [label="AddBackward0
------------
alpha: 1"]
	140107508467120 -> 140107508468560
	140107508468656 -> 140107508468560
	140107508468656 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107508468752 -> 140107508468656
	140107508468752 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107508468848 -> 140107508468752
	140107508468848 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107522424304 -> 140107508468848
	140107508468320 -> 140107508468224
	140107508468320 -> 140107040377088 [dir=none]
	140107040377088 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508468320 -> 140106731495456 [dir=none]
	140106731495456 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508468320 -> 140107040379008 [dir=none]
	140107040379008 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107508468320 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508468704 -> 140107508468320
	140107040379008 [label="enc_q.enc.in_layers.13.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040379008 -> 140107508468704
	140107508468704 [label=AccumulateGrad]
	140107508468608 -> 140107508468320
	140107040377088 [label="enc_q.enc.in_layers.13.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040377088 -> 140107508468608
	140107508468608 [label=AccumulateGrad]
	140107508468272 -> 140107508468224
	140107040377808 [label="enc_q.enc.in_layers.13.bias
 (384)" fillcolor=lightblue]
	140107040377808 -> 140107508468272
	140107508468272 [label=AccumulateGrad]
	140107508468176 -> 140107508468128
	140107508468176 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107508468800 -> 140107508468176
	140107508468800 [label="SliceBackward0
----------------------------
dim           :            1
end           :         5376
self_sym_sizes: (7, 6144, 1)
start         :         4992
step          :            1"]
	140107508468896 -> 140107508468800
	140107508468896 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107508468896
	140107508467168 -> 140107508467264
	140107508467168 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508468080 -> 140107508467168
	140107508468080 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508468032 -> 140107508468080
	140107508466496 -> 140107508466544
	140107508466496 -> 140107040378368 [dir=none]
	140107040378368 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508466496 -> 140106731495136 [dir=none]
	140106731495136 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508466496 -> 140107040379568 [dir=none]
	140107040379568 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107508466496 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508468944 -> 140107508466496
	140107040379568 [label="enc_q.enc.res_skip_layers.13.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040379568 -> 140107508468944
	140107508468944 [label=AccumulateGrad]
	140107508467984 -> 140107508466496
	140107040378368 [label="enc_q.enc.res_skip_layers.13.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040378368 -> 140107508467984
	140107508467984 [label=AccumulateGrad]
	140107508466016 -> 140107508466544
	140107040379088 [label="enc_q.enc.res_skip_layers.13.bias
 (384)" fillcolor=lightblue]
	140107040379088 -> 140107508466016
	140107508466016 [label=AccumulateGrad]
	140107604686928 -> 140107604686832
	140107604686928 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107522421760 -> 140107604686928
	140107522421760 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140107604687024 -> 140107522421760
	140107604687024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107508467840 -> 140107604687024
	140107508467840 -> 140107058259952 [dir=none]
	140107058259952 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107508467840 -> 140107058263552 [dir=none]
	140107058263552 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140107508467840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508468512 -> 140107508467840
	140107508468512 [label=CppFunction]
	140107508469040 -> 140107508468512
	140107508469040 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508469184 -> 140107508469040
	140107508469184 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107508469280 -> 140107508469184
	140107508469280 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508469424 -> 140107508469280
	140107508469424 [label="AddBackward0
------------
alpha: 1"]
	140107508469520 -> 140107508469424
	140107508469520 -> 140107058262352 [dir=none]
	140107058262352 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107508469520 -> 140107058262512 [dir=none]
	140107058262512 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107508469520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508469664 -> 140107508469520
	140107508469664 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107508469664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508469856 -> 140107508469664
	140107508469856 [label="AddBackward0
------------
alpha: 1"]
	140107508468368 -> 140107508469856
	140107508469952 -> 140107508469856
	140107508469952 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107508470048 -> 140107508469952
	140107508470048 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107508470144 -> 140107508470048
	140107508470144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107508466544 -> 140107508470144
	140107508469616 -> 140107508469520
	140107508469616 -> 140107040379168 [dir=none]
	140107040379168 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508469616 -> 140106731495376 [dir=none]
	140106731495376 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508469616 -> 140107040381088 [dir=none]
	140107040381088 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107508469616 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508469904 -> 140107508469616
	140107040381088 [label="enc_q.enc.in_layers.14.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040381088 -> 140107508469904
	140107508469904 [label=AccumulateGrad]
	140107508469808 -> 140107508469616
	140107040379168 [label="enc_q.enc.in_layers.14.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040379168 -> 140107508469808
	140107508469808 [label=AccumulateGrad]
	140107508469568 -> 140107508469520
	140107040379648 [label="enc_q.enc.in_layers.14.bias
 (384)" fillcolor=lightblue]
	140107040379648 -> 140107508469568
	140107508469568 [label=AccumulateGrad]
	140107508469472 -> 140107508469424
	140107508469472 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107508470240 -> 140107508469472
	140107508470240 [label="SliceBackward0
----------------------------
dim           :            1
end           :         5760
self_sym_sizes: (7, 6144, 1)
start         :         5376
step          :            1"]
	140107508469760 -> 140107508470240
	140107508469760 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107508469760
	140107508468416 -> 140107508468512
	140107508468416 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508469376 -> 140107508468416
	140107508469376 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508469280 -> 140107508469376
	140107508467216 -> 140107508467840
	140107508467216 -> 140107040379728 [dir=none]
	140107040379728 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508467216 -> 140106731495696 [dir=none]
	140106731495696 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508467216 -> 140107040381808 [dir=none]
	140107040381808 [label="v
 (384, 192, 1)" fillcolor=orange]
	140107508467216 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508470192 -> 140107508467216
	140107040381808 [label="enc_q.enc.res_skip_layers.14.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040381808 -> 140107508470192
	140107508470192 [label=AccumulateGrad]
	140107508469232 -> 140107508467216
	140107040379728 [label="enc_q.enc.res_skip_layers.14.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040379728 -> 140107508469232
	140107508469232 [label=AccumulateGrad]
	140107508466592 -> 140107508467840
	140107040381168 [label="enc_q.enc.res_skip_layers.14.bias
 (384)" fillcolor=lightblue]
	140107040381168 -> 140107508466592
	140107508466592 [label=AccumulateGrad]
	140107604686784 -> 140107604686736
	140107604686784 -> 140107058262112 [dir=none]
	140107058262112 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107604686784 -> 140107058264112 [dir=none]
	140107058264112 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140107604686784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107522423008 -> 140107604686784
	140107522423008 [label=CppFunction]
	140107508469328 -> 140107522423008
	140107508469328 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508470336 -> 140107508469328
	140107508470336 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140107508469712 -> 140107508470336
	140107508469712 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508470432 -> 140107508469712
	140107508470432 [label="AddBackward0
------------
alpha: 1"]
	140107508470528 -> 140107508470432
	140107508470528 -> 140107058262752 [dir=none]
	140107058262752 [label="input
 (7, 192, 286)" fillcolor=orange]
	140107508470528 -> 140107058263472 [dir=none]
	140107058263472 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140107508470528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508470672 -> 140107508470528
	140107508470672 -> 140107077229424 [dir=none]
	140107077229424 [label="other
 (7, 1, 286)" fillcolor=orange]
	140107508470672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508470864 -> 140107508470672
	140107508470864 [label="AddBackward0
------------
alpha: 1"]
	140107508469664 -> 140107508470864
	140107508470960 -> 140107508470864
	140107508470960 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140107508471056 -> 140107508470960
	140107508471056 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140107508471152 -> 140107508471056
	140107508471152 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140107508467840 -> 140107508471152
	140107508470624 -> 140107508470528
	140107508470624 -> 140107040381408 [dir=none]
	140107040381408 [label="g
 (384, 1, 1)" fillcolor=orange]
	140107508470624 -> 140106731496256 [dir=none]
	140106731496256 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140107508470624 -> 140107040383008 [dir=none]
	140107040383008 [label="v
 (384, 192, 5)" fillcolor=orange]
	140107508470624 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508471008 -> 140107508470624
	140107040383008 [label="enc_q.enc.in_layers.15.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040383008 -> 140107508471008
	140107508471008 [label=AccumulateGrad]
	140107508470912 -> 140107508470624
	140107040381408 [label="enc_q.enc.in_layers.15.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040381408 -> 140107508470912
	140107508470912 [label=AccumulateGrad]
	140107508470576 -> 140107508470528
	140107040382288 [label="enc_q.enc.in_layers.15.bias
 (384)" fillcolor=lightblue]
	140107040382288 -> 140107508470576
	140107508470576 [label=AccumulateGrad]
	140107508470480 -> 140107508470432
	140107508470480 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140107508471104 -> 140107508470480
	140107508471104 [label="SliceBackward0
----------------------------
dim           :            1
end           :         6144
self_sym_sizes: (7, 6144, 1)
start         :         5760
step          :            1"]
	140107508470000 -> 140107508471104
	140107508470000 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140107522410816 -> 140107508470000
	140107508469136 -> 140107522423008
	140107508469136 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140107508470384 -> 140107508469136
	140107508470384 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140107508469712 -> 140107508470384
	140107604686880 -> 140107604686784
	140107604686880 -> 140107040382368 [dir=none]
	140107040382368 [label="g
 (192, 1, 1)" fillcolor=orange]
	140107604686880 -> 140106731497376 [dir=none]
	140106731497376 [label="result1
 (192, 1, 1)" fillcolor=orange]
	140107604686880 -> 140107040384288 [dir=none]
	140107040384288 [label="v
 (192, 192, 1)" fillcolor=orange]
	140107604686880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508471248 -> 140107604686880
	140107040384288 [label="enc_q.enc.res_skip_layers.15.weight_v
 (192, 192, 1)" fillcolor=lightblue]
	140107040384288 -> 140107508471248
	140107508471248 [label=AccumulateGrad]
	140107508470288 -> 140107604686880
	140107040382368 [label="enc_q.enc.res_skip_layers.15.weight_g
 (192, 1, 1)" fillcolor=lightblue]
	140107040382368 -> 140107508470288
	140107508470288 [label=AccumulateGrad]
	140107508467744 -> 140107604686784
	140107040383808 [label="enc_q.enc.res_skip_layers.15.bias
 (192)" fillcolor=lightblue]
	140107040383808 -> 140107508467744
	140107508467744 [label=AccumulateGrad]
	140107604686448 -> 140107604686400
	140107040384528 [label="enc_q.proj.weight
 (384, 192, 1)" fillcolor=lightblue]
	140107040384528 -> 140107604686448
	140107604686448 [label=AccumulateGrad]
	140107604686160 -> 140107604686400
	140107040384448 [label="enc_q.proj.bias
 (384)" fillcolor=lightblue]
	140107040384448 -> 140107604686160
	140107604686160 [label=AccumulateGrad]
	140107604686064 -> 140107604686016
	140107604686064 -> 140107058263712 [dir=none]
	140107058263712 [label="self
 (7, 192, 286)" fillcolor=orange]
	140107604686064 [label="MulBackward0
---------------------
other:           None
self : [saved tensor]"]
	140107604686592 -> 140107604686064
	140107604686592 -> 140106731496176 [dir=none]
	140106731496176 [label="result
 (7, 192, 286)" fillcolor=orange]
	140107604686592 [label="ExpBackward0
----------------------
result: [saved tensor]"]
	140107604686112 -> 140107604686592
	140107604685248 -> 140107604685152
	140107604685248 [label="SliceBackward0
--------------------------
dim           :          1
end           :        142
self_sym_sizes: (192, 286)
start         :         92
step          :          1"]
	140107604685632 -> 140107604685248
	140107604685632 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604685872 -> 140107604685632
	140107604685872 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             1
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604685872
	140107604685104 -> 140107604685008
	140107604685104 [label="SliceBackward0
--------------------------
dim           :          1
end           :        227
self_sym_sizes: (192, 286)
start         :        177
step          :          1"]
	140107604685344 -> 140107604685104
	140107604685344 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604685824 -> 140107604685344
	140107604685824 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             2
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604685824
	140107604684960 -> 140107604684864
	140107604684960 [label="SliceBackward0
--------------------------
dim           :          1
end           :        167
self_sym_sizes: (192, 286)
start         :        117
step          :          1"]
	140107604686304 -> 140107604684960
	140107604686304 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604685392 -> 140107604686304
	140107604685392 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             3
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604685392
	140107604684816 -> 140107604684720
	140107604684816 [label="SliceBackward0
--------------------------
dim           :          1
end           :         79
self_sym_sizes: (192, 286)
start         :         29
step          :          1"]
	140107604686640 -> 140107604684816
	140107604686640 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604685200 -> 140107604686640
	140107604685200 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             4
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604685200
	140107604684672 -> 140107604673824
	140107604684672 [label="SliceBackward0
--------------------------
dim           :          1
end           :        128
self_sym_sizes: (192, 286)
start         :         78
step          :          1"]
	140107604686688 -> 140107604684672
	140107604686688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604685056 -> 140107604686688
	140107604685056 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             5
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604685056
	140107604681840 -> 140107604684480
	140107604681840 [label="SliceBackward0
--------------------------
dim           :          1
end           :        132
self_sym_sizes: (192, 286)
start         :         82
step          :          1"]
	140107604686208 -> 140107604681840
	140107604686208 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:          (192, 286)
start         :                   0
step          :                   1"]
	140107604684768 -> 140107604686208
	140107604684768 [label="SelectBackward0
-----------------------------
dim           :             0
index         :             6
self_sym_sizes: (7, 192, 286)"]
	140107604685728 -> 140107604684768
	140107604684336 -> 140107604684240
	140107125192192 [label="dec.conv_pre.weight
 (512, 192, 7)" fillcolor=lightblue]
	140107125192192 -> 140107604684336
	140107604684336 [label=AccumulateGrad]
	140107604684288 -> 140107604684240
	140107125192272 [label="dec.conv_pre.bias
 (512)" fillcolor=lightblue]
	140107125192272 -> 140107604684288
	140107604684288 [label=AccumulateGrad]
	140107604684192 -> 140107604684144
	140107604684192 -> 140107092170592 [dir=none]
	140107092170592 [label="input
 (7, 512, 1)" fillcolor=orange]
	140107604684192 -> 140107082872720 [dir=none]
	140107082872720 [label="weight
 (512, 512, 1)" fillcolor=orange]
	140107604684192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604685488 -> 140107604684192
	140107604685488 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551615"]
	140107604684624 -> 140107604685488
	140107604684624 -> 140107092167552 [dir=none]
	140107092167552 [label="other
 (7, 1)" fillcolor=orange]
	140107604684624 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508468992 -> 140107604684624
	140107508468992 [label="SumBackward1
-----------------------------
dim           :          (1,)
keepdim       :         False
self_sym_sizes: (7, 286, 512)"]
	140107508466640 -> 140107508468992
	140107508466640 -> 140107092170512 [dir=none]
	140107092170512 [label="mask
 (7, 286, 1)" fillcolor=orange]
	140107508466640 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140107508470768 -> 140107508466640
	140107508470768 [label="ViewBackward0
---------------------------
self_sym_sizes: (2002, 512)"]
	140107508471344 -> 140107508470768
	140107508471344 -> 140106731497536 [dir=none]
	140106731497536 [label="mat1
 (2002, 128)" fillcolor=orange]
	140107508471344 -> 140106778589232 [dir=none]
	140106778589232 [label="mat2
 (128, 512)" fillcolor=orange]
	140107508471344 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (2002, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 512)
mat2_sym_strides:       (1, 128)"]
	140107508471440 -> 140107508471344
	140107019753792 [label="ref_enc.fc.fc.bias
 (512)" fillcolor=lightblue]
	140107019753792 -> 140107508471440
	140107508471440 [label=AccumulateGrad]
	140107508471392 -> 140107508471344
	140107508471392 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508471536 -> 140107508471392
	140107508471536 [label="AddBackward0
------------
alpha: 1"]
	140107508471728 -> 140107508471536
	140107508471728 -> 140106731492576 [dir=none]
	140106731492576 [label="result1
 (7, 286, 128)" fillcolor=orange]
	140107508471728 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140107508471872 -> 140107508471728
	140107508471872 [label="ViewBackward0
---------------------------
self_sym_sizes: (2002, 128)"]
	140107508471968 -> 140107508471872
	140107508471968 -> 140106731496656 [dir=none]
	140106731496656 [label="mat1
 (2002, 128)" fillcolor=orange]
	140107508471968 -> 140106731497216 [dir=none]
	140106731497216 [label="mat2
 (128, 128)" fillcolor=orange]
	140107508471968 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (2002, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 128)
mat2_sym_strides:       (1, 128)"]
	140107508472064 -> 140107508471968
	140107019753232 [label="ref_enc.slf_attn.fc.bias
 (128)" fillcolor=lightblue]
	140107019753232 -> 140107508472064
	140107508472064 [label=AccumulateGrad]
	140107508472016 -> 140107508471968
	140107508472016 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508472160 -> 140107508472016
	140107508472160 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 286, 2, 64)"]
	140107508472352 -> 140107508472160
	140107508472352 [label=CloneBackward0]
	140107508472448 -> 140107508472352
	140107508472448 [label="PermuteBackward0
------------------
dims: (1, 2, 0, 3)"]
	140107508472544 -> 140107508472448
	140107508472544 [label="ViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 64)"]
	140107508472640 -> 140107508472544
	140107508472640 -> 140107099120048 [dir=none]
	140107099120048 [label="mat2
 (14, 286, 64)" fillcolor=orange]
	140107508472640 -> 140107103442544 [dir=none]
	140107103442544 [label="self
 (14, 286, 286)" fillcolor=orange]
	140107508472640 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140107508472784 -> 140107508472640
	140107508472784 -> 140106731496816 [dir=none]
	140106731496816 [label="result1
 (14, 286, 286)" fillcolor=orange]
	140107508472784 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140107508472928 -> 140107508472784
	140107508472928 -> 140106731495296 [dir=none]
	140106731495296 [label="result
 (14, 286, 286)" fillcolor=orange]
	140107508472928 [label="SoftmaxBackward0
----------------------
dim   :              2
result: [saved tensor]"]
	140107508473072 -> 140107508472928
	140107508473072 -> 140107099123968 [dir=none]
	140107099123968 [label="mask
 (14, 286, 286)" fillcolor=orange]
	140107508473072 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140107508473216 -> 140107508473072
	140107508473216 -> 140106731497936 [dir=none]
	140106731497936 [label="other
 ()" fillcolor=orange]
	140107508473216 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140107508473360 -> 140107508473216
	140107508473360 -> 140107092164752 [dir=none]
	140107092164752 [label="mat2
 (14, 64, 286)" fillcolor=orange]
	140107508473360 -> 140107099126928 [dir=none]
	140107099126928 [label="self
 (14, 286, 64)" fillcolor=orange]
	140107508473360 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140107508473456 -> 140107508473360
	140107508473456 [label="ViewBackward0
-------------------------------
self_sym_sizes: (2, 7, 286, 64)"]
	140107508473600 -> 140107508473456
	140107508473600 [label=CloneBackward0]
	140107508473696 -> 140107508473600
	140107508473696 [label="PermuteBackward0
------------------
dims: (2, 0, 1, 3)"]
	140107508473744 -> 140107508473696
	140107508473744 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508473888 -> 140107508473744
	140107508473888 [label="ViewBackward0
---------------------------
self_sym_sizes: (2002, 128)"]
	140107508474032 -> 140107508473888
	140107508474032 -> 140106731497696 [dir=none]
	140106731497696 [label="mat1
 (2002, 128)" fillcolor=orange]
	140107508474032 -> 140106731496016 [dir=none]
	140106731496016 [label="mat2
 (128, 128)" fillcolor=orange]
	140107508474032 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (2002, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 128)
mat2_sym_strides:       (1, 128)"]
	140107508474320 -> 140107508474032
	140107019751392 [label="ref_enc.slf_attn.w_qs.bias
 (128)" fillcolor=lightblue]
	140107019751392 -> 140107508474320
	140107508474320 [label=AccumulateGrad]
	140107508474176 -> 140107508474032
	140107508474176 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508471680 -> 140107508474176
	140107508471680 -> 140107099126208 [dir=none]
	140107099126208 [label="mask
 (7, 286, 1)" fillcolor=orange]
	140107508471680 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140107508474608 -> 140107508471680
	140107508474608 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140107508474704 -> 140107508474608
	140107508474704 [label="AddBackward0
------------
alpha: 1"]
	140107508474752 -> 140107508474704
	140107508474752 [label="AddBackward0
------------
alpha: 1"]
	140107508474992 -> 140107508474752
	140107508474992 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140107508475136 -> 140107508474992
	140107508475136 -> 140106761908160 [dir=none]
	140106761908160 [label="result1
 (7, 286, 128)" fillcolor=orange]
	140107508475136 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140107508475328 -> 140107508475136
	140107508475328 -> 140107099120128 [dir=none]
	140107099120128 [label="other
 (7, 286, 128)" fillcolor=orange]
	140107508475328 -> 140107099116368 [dir=none]
	140107099116368 [label="self
 (7, 286, 128)" fillcolor=orange]
	140107508475328 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140107508475472 -> 140107508475328
	140107508475472 [label="ViewBackward0
---------------------------
self_sym_sizes: (2002, 128)"]
	140107508475616 -> 140107508475472
	140107508475616 -> 140107071067200 [dir=none]
	140107071067200 [label="mat1
 (2002, 128)" fillcolor=orange]
	140107508475616 -> 140106731497776 [dir=none]
	140106731497776 [label="mat2
 (128, 128)" fillcolor=orange]
	140107508475616 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (2002, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 128)
mat2_sym_strides:       (1, 128)"]
	140107508475760 -> 140107508475616
	140107019750352 [label="ref_enc.spectral.3.fc.bias
 (128)" fillcolor=lightblue]
	140107019750352 -> 140107508475760
	140107508475760 [label=AccumulateGrad]
	140107508475664 -> 140107508475616
	140107508475664 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508475856 -> 140107508475664
	140107508475856 -> 140106731498096 [dir=none]
	140106731498096 [label="result1
 (7, 286, 128)" fillcolor=orange]
	140107508475856 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140107508476096 -> 140107508475856
	140107508476096 -> 140107099118128 [dir=none]
	140107099118128 [label="other
 (7, 286, 128)" fillcolor=orange]
	140107508476096 -> 140107099117488 [dir=none]
	140107099117488 [label="self
 (7, 286, 128)" fillcolor=orange]
	140107508476096 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140107508476240 -> 140107508476096
	140107508476240 [label="AddBackward0
------------
alpha: 1"]
	140107508476384 -> 140107508476240
	140107508476384 [label="UnsafeViewBackward0
---------------------------
self_sym_sizes: (2002, 128)"]
	140107508476528 -> 140107508476384
	140107508476528 -> 140107082873040 [dir=none]
	140107082873040 [label="self
 (2002, 1025)" fillcolor=orange]
	140107508476528 [label="MmBackward0
--------------------------------
mat2            :           None
mat2_sym_sizes  :    (1025, 128)
mat2_sym_strides:      (1, 1025)
self            : [saved tensor]
self_sym_sizes  :   (2002, 1025)
self_sym_strides:             ()"]
	140107508476672 -> 140107508476528
	140107508476672 [label=TBackward0]
	140107508476768 -> 140107508476672
	140107019749152 [label="ref_enc.spectral.0.fc.weight
 (128, 1025)" fillcolor=lightblue]
	140107019749152 -> 140107508476768
	140107508476768 [label=AccumulateGrad]
	140107508476336 -> 140107508476240
	140107019749792 [label="ref_enc.spectral.0.fc.bias
 (128)" fillcolor=lightblue]
	140107019749792 -> 140107508476336
	140107508476336 [label=AccumulateGrad]
	140107508476144 -> 140107508476096
	140107508476144 -> 140106731498256 [dir=none]
	140106731498256 [label="result
 (7, 286, 128)" fillcolor=orange]
	140107508476144 [label="TanhBackward0
----------------------
result: [saved tensor]"]
	140107508476576 -> 140107508476144
	140107508476576 -> 140107099117488 [dir=none]
	140107099117488 [label="self
 (7, 286, 128)" fillcolor=orange]
	140107508476576 [label="SoftplusBackward0
-------------------------
beta     :              1
self     : [saved tensor]
threshold:             20"]
	140107508476240 -> 140107508476576
	140107508475520 -> 140107508475616
	140107508475520 [label=TBackward0]
	140107508475952 -> 140107508475520
	140107019749872 [label="ref_enc.spectral.3.fc.weight
 (128, 128)" fillcolor=lightblue]
	140107019749872 -> 140107508475952
	140107508475952 [label=AccumulateGrad]
	140107508475376 -> 140107508475328
	140107508475376 -> 140106731498176 [dir=none]
	140106731498176 [label="result
 (7, 286, 128)" fillcolor=orange]
	140107508475376 [label="TanhBackward0
----------------------
result: [saved tensor]"]
	140107508476000 -> 140107508475376
	140107508476000 -> 140107099116368 [dir=none]
	140107099116368 [label="self
 (7, 286, 128)" fillcolor=orange]
	140107508476000 [label="SoftplusBackward0
-------------------------
beta     :              1
self     : [saved tensor]
threshold:             20"]
	140107508475472 -> 140107508476000
	140107508474944 -> 140107508474752
	140107508474944 -> 140106731498416 [dir=none]
	140106731498416 [label="result1
 (7, 128, 286)" fillcolor=orange]
	140107508474944 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140107508475040 -> 140107508474944
	140107508475040 -> 140107099123408 [dir=none]
	140107099123408 [label="other
 (7, 128, 286)" fillcolor=orange]
	140107508475040 -> 140107099122928 [dir=none]
	140107099122928 [label="self
 (7, 128, 286)" fillcolor=orange]
	140107508475040 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140107508476720 -> 140107508475040
	140107508476720 [label="SplitBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 256, 286)
split_size    :           128"]
	140107508475904 -> 140107508476720
	140107508475904 -> 140107099111888 [dir=none]
	140107099111888 [label="input
 (7, 128, 286)" fillcolor=orange]
	140107508475904 -> 140107040384848 [dir=none]
	140107040384848 [label="weight
 (256, 128, 5)" fillcolor=orange]
	140107508475904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508474992 -> 140107508475904
	140107508476288 -> 140107508475904
	140107040384848 [label="ref_enc.temporal.0.conv1.conv.weight
 (256, 128, 5)" fillcolor=lightblue]
	140107040384848 -> 140107508476288
	140107508476288 [label=AccumulateGrad]
	140107508476480 -> 140107508475904
	140107019750912 [label="ref_enc.temporal.0.conv1.conv.bias
 (256)" fillcolor=lightblue]
	140107019750912 -> 140107508476480
	140107508476480 [label=AccumulateGrad]
	140107508475808 -> 140107508475040
	140107508475808 -> 140106731498656 [dir=none]
	140106731498656 [label="result
 (7, 128, 286)" fillcolor=orange]
	140107508475808 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140107508476720 -> 140107508475808
	140107508474464 -> 140107508474704
	140107508474464 -> 140106731499136 [dir=none]
	140106731499136 [label="result1
 (7, 128, 286)" fillcolor=orange]
	140107508474464 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140107508475088 -> 140107508474464
	140107508475088 -> 140107099126048 [dir=none]
	140107099126048 [label="other
 (7, 128, 286)" fillcolor=orange]
	140107508475088 -> 140107099122368 [dir=none]
	140107099122368 [label="self
 (7, 128, 286)" fillcolor=orange]
	140107508475088 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140107508476912 -> 140107508475088
	140107508476912 [label="SplitBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 256, 286)
split_size    :           128"]
	140107508476960 -> 140107508476912
	140107508476960 -> 140107099124928 [dir=none]
	140107099124928 [label="input
 (7, 128, 286)" fillcolor=orange]
	140107508476960 -> 140107019750992 [dir=none]
	140107019750992 [label="weight
 (256, 128, 5)" fillcolor=orange]
	140107508476960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508474752 -> 140107508476960
	140107508477056 -> 140107508476960
	140107019750992 [label="ref_enc.temporal.1.conv1.conv.weight
 (256, 128, 5)" fillcolor=lightblue]
	140107019750992 -> 140107508477056
	140107508477056 [label=AccumulateGrad]
	140107508477008 -> 140107508476960
	140107019751232 [label="ref_enc.temporal.1.conv1.conv.bias
 (256)" fillcolor=lightblue]
	140107019751232 -> 140107508477008
	140107508477008 [label=AccumulateGrad]
	140107508476864 -> 140107508475088
	140107508476864 -> 140106731499216 [dir=none]
	140106731499216 [label="result
 (7, 128, 286)" fillcolor=orange]
	140107508476864 [label="SigmoidBackward0
----------------------
result: [saved tensor]"]
	140107508476912 -> 140107508476864
	140107508474128 -> 140107508474032
	140107508474128 [label=TBackward0]
	140107508474656 -> 140107508474128
	140107934851264 [label="ref_enc.slf_attn.w_qs.weight
 (128, 128)" fillcolor=lightblue]
	140107934851264 -> 140107508474656
	140107508474656 [label=AccumulateGrad]
	140107508473408 -> 140107508473360
	140107508473408 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140107508473504 -> 140107508473408
	140107508473504 [label="ViewBackward0
-------------------------------
self_sym_sizes: (2, 7, 286, 64)"]
	140107508473984 -> 140107508473504
	140107508473984 [label=CloneBackward0]
	140107508474512 -> 140107508473984
	140107508474512 [label="PermuteBackward0
------------------
dims: (2, 0, 1, 3)"]
	140107508474848 -> 140107508474512
	140107508474848 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508474896 -> 140107508474848
	140107508474896 [label="ViewBackward0
---------------------------
self_sym_sizes: (2002, 128)"]
	140107508475568 -> 140107508474896
	140107508475568 -> 140106731498576 [dir=none]
	140106731498576 [label="mat1
 (2002, 128)" fillcolor=orange]
	140107508475568 -> 140106731498496 [dir=none]
	140106731498496 [label="mat2
 (128, 128)" fillcolor=orange]
	140107508475568 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (2002, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 128)
mat2_sym_strides:       (1, 128)"]
	140107508477152 -> 140107508475568
	140107019752352 [label="ref_enc.slf_attn.w_ks.bias
 (128)" fillcolor=lightblue]
	140107019752352 -> 140107508477152
	140107508477152 [label=AccumulateGrad]
	140107508476432 -> 140107508475568
	140107508476432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508471680 -> 140107508476432
	140107508473552 -> 140107508475568
	140107508473552 [label=TBackward0]
	140107508477344 -> 140107508473552
	140107019752272 [label="ref_enc.slf_attn.w_ks.weight
 (128, 128)" fillcolor=lightblue]
	140107019752272 -> 140107508477344
	140107508477344 [label=AccumulateGrad]
	140107508472688 -> 140107508472640
	140107508472688 [label="ViewBackward0
-------------------------------
self_sym_sizes: (2, 7, 286, 64)"]
	140107508473120 -> 140107508472688
	140107508473120 [label=CloneBackward0]
	140107508472832 -> 140107508473120
	140107508472832 [label="PermuteBackward0
------------------
dims: (2, 0, 1, 3)"]
	140107508473840 -> 140107508472832
	140107508473840 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508475184 -> 140107508473840
	140107508475184 [label="ViewBackward0
---------------------------
self_sym_sizes: (2002, 128)"]
	140107508477104 -> 140107508475184
	140107508477104 -> 140106731499696 [dir=none]
	140106731499696 [label="mat1
 (2002, 128)" fillcolor=orange]
	140107508477104 -> 140106731499376 [dir=none]
	140106731499376 [label="mat2
 (128, 128)" fillcolor=orange]
	140107508477104 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :    (2002, 128)
mat1_sym_strides:       (128, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (128, 128)
mat2_sym_strides:       (1, 128)"]
	140107508477296 -> 140107508477104
	140107019752752 [label="ref_enc.slf_attn.w_vs.bias
 (128)" fillcolor=lightblue]
	140107019752752 -> 140107508477296
	140107508477296 [label=AccumulateGrad]
	140107508477200 -> 140107508477104
	140107508477200 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 128)"]
	140107508471680 -> 140107508477200
	140107508472880 -> 140107508477104
	140107508472880 [label=TBackward0]
	140107508477488 -> 140107508472880
	140107019752432 [label="ref_enc.slf_attn.w_vs.weight
 (128, 128)" fillcolor=lightblue]
	140107019752432 -> 140107508477488
	140107508477488 [label=AccumulateGrad]
	140107508471776 -> 140107508471968
	140107508471776 [label=TBackward0]
	140107508472400 -> 140107508471776
	140107019753072 [label="ref_enc.slf_attn.fc.weight
 (128, 128)" fillcolor=lightblue]
	140107019753072 -> 140107508472400
	140107508472400 [label=AccumulateGrad]
	140107508471680 -> 140107508471536
	140107508467888 -> 140107508471344
	140107508467888 [label=TBackward0]
	140107508471824 -> 140107508467888
	140107019753312 [label="ref_enc.fc.fc.weight
 (512, 128)" fillcolor=lightblue]
	140107019753312 -> 140107508471824
	140107508471824 [label=AccumulateGrad]
	140107604684576 -> 140107604684192
	140107082872720 [label="dec.cond.weight
 (512, 512, 1)" fillcolor=lightblue]
	140107082872720 -> 140107604684576
	140107604684576 [label=AccumulateGrad]
	140107604684528 -> 140107604684192
	140107082872880 [label="dec.cond.bias
 (512)" fillcolor=lightblue]
	140107082872880 -> 140107604684528
	140107604684528 [label=AccumulateGrad]
	140107604683712 -> 140107604682944
	140107604683712 -> 140107934857104 [dir=none]
	140107934857104 [label="g
 (512, 1, 1)" fillcolor=orange]
	140107604683712 -> 140106731499776 [dir=none]
	140106731499776 [label="result1
 (512, 1, 1)" fillcolor=orange]
	140107604683712 -> 140107125192432 [dir=none]
	140107125192432 [label="v
 (512, 256, 16)" fillcolor=orange]
	140107604683712 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604684912 -> 140107604683712
	140107125192432 [label="dec.ups.0.weight_v
 (512, 256, 16)" fillcolor=lightblue]
	140107125192432 -> 140107604684912
	140107604684912 [label=AccumulateGrad]
	140107604684048 -> 140107604683712
	140107934857104 [label="dec.ups.0.weight_g
 (512, 1, 1)" fillcolor=lightblue]
	140107934857104 -> 140107604684048
	140107604684048 [label=AccumulateGrad]
	140107604683760 -> 140107604682944
	140107934879632 [label="dec.ups.0.bias
 (256)" fillcolor=lightblue]
	140107934879632 -> 140107604683760
	140107604683760 [label=AccumulateGrad]
	140107604683520 -> 140107604683472
	140107604683520 -> 140107934884512 [dir=none]
	140107934884512 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604683520 -> 140106731499936 [dir=none]
	140106731499936 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604683520 -> 140107119181888 [dir=none]
	140107119181888 [label="v
 (256, 256, 3)" fillcolor=orange]
	140107604683520 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604684096 -> 140107604683520
	140107119181888 [label="dec.resblocks.0.convs1.0.parametrizations.weight.original1
 (256, 256, 3)" fillcolor=lightblue]
	140107119181888 -> 140107604684096
	140107604684096 [label=AccumulateGrad]
	140107604684000 -> 140107604683520
	140107934884512 [label="dec.resblocks.0.convs1.0.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107934884512 -> 140107604684000
	140107604684000 [label=AccumulateGrad]
	140107604683376 -> 140107604683472
	140107125194272 [label="dec.resblocks.0.convs1.0.bias
 (256)" fillcolor=lightblue]
	140107125194272 -> 140107604683376
	140107604683376 [label=AccumulateGrad]
	140107604683184 -> 140107604683088
	140107604683184 -> 140107119186048 [dir=none]
	140107119186048 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604683184 -> 140106731500336 [dir=none]
	140106731500336 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604683184 -> 140107119186128 [dir=none]
	140107119186128 [label="v
 (256, 256, 3)" fillcolor=orange]
	140107604683184 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604683808 -> 140107604683184
	140107119186128 [label="dec.resblocks.0.convs2.0.parametrizations.weight.original1
 (256, 256, 3)" fillcolor=lightblue]
	140107119186128 -> 140107604683808
	140107604683808 [label=AccumulateGrad]
	140107604683424 -> 140107604683184
	140107119186048 [label="dec.resblocks.0.convs2.0.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119186048 -> 140107604683424
	140107604683424 [label=AccumulateGrad]
	140107604683136 -> 140107604683088
	140107119185888 [label="dec.resblocks.0.convs2.0.bias
 (256)" fillcolor=lightblue]
	140107119185888 -> 140107604683136
	140107604683136 [label=AccumulateGrad]
	140107604682944 -> 140107604682176
	140107604682752 -> 140107604682704
	140107604682752 -> 140107119183248 [dir=none]
	140107119183248 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604682752 -> 140106731498736 [dir=none]
	140106731498736 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604682752 -> 140107119183408 [dir=none]
	140107119183408 [label="v
 (256, 256, 3)" fillcolor=orange]
	140107604682752 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604683328 -> 140107604682752
	140107119183408 [label="dec.resblocks.0.convs1.1.parametrizations.weight.original1
 (256, 256, 3)" fillcolor=lightblue]
	140107119183408 -> 140107604683328
	140107604683328 [label=AccumulateGrad]
	140107604682992 -> 140107604682752
	140107119183248 [label="dec.resblocks.0.convs1.1.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119183248 -> 140107604682992
	140107604682992 [label=AccumulateGrad]
	140107604682608 -> 140107604682704
	140107119182368 [label="dec.resblocks.0.convs1.1.bias
 (256)" fillcolor=lightblue]
	140107119182368 -> 140107604682608
	140107604682608 [label=AccumulateGrad]
	140107604682416 -> 140107604682320
	140107604682416 -> 140107119186848 [dir=none]
	140107119186848 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604682416 -> 140106731500896 [dir=none]
	140106731500896 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604682416 -> 140107119186928 [dir=none]
	140107119186928 [label="v
 (256, 256, 3)" fillcolor=orange]
	140107604682416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604683040 -> 140107604682416
	140107119186928 [label="dec.resblocks.0.convs2.1.parametrizations.weight.original1
 (256, 256, 3)" fillcolor=lightblue]
	140107119186928 -> 140107604683040
	140107604683040 [label=AccumulateGrad]
	140107604682896 -> 140107604682416
	140107119186848 [label="dec.resblocks.0.convs2.1.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119186848 -> 140107604682896
	140107604682896 [label=AccumulateGrad]
	140107604682368 -> 140107604682320
	140107119186608 [label="dec.resblocks.0.convs2.1.bias
 (256)" fillcolor=lightblue]
	140107119186608 -> 140107604682368
	140107604682368 [label=AccumulateGrad]
	140107604682176 -> 140107604681504
	140107604681984 -> 140107604681936
	140107604681984 -> 140107119185168 [dir=none]
	140107119185168 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604681984 -> 140106731500976 [dir=none]
	140106731500976 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604681984 -> 140107119185248 [dir=none]
	140107119185248 [label="v
 (256, 256, 3)" fillcolor=orange]
	140107604681984 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604682560 -> 140107604681984
	140107119185248 [label="dec.resblocks.0.convs1.2.parametrizations.weight.original1
 (256, 256, 3)" fillcolor=lightblue]
	140107119185248 -> 140107604682560
	140107604682560 [label=AccumulateGrad]
	140107604682224 -> 140107604681984
	140107119185168 [label="dec.resblocks.0.convs1.2.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119185168 -> 140107604682224
	140107604682224 [label=AccumulateGrad]
	140107604681792 -> 140107604681936
	140107119185008 [label="dec.resblocks.0.convs1.2.bias
 (256)" fillcolor=lightblue]
	140107119185008 -> 140107604681792
	140107604681792 [label=AccumulateGrad]
	140107604681648 -> 140107604681552
	140107604681648 -> 140107119188528 [dir=none]
	140107119188528 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604681648 -> 140106731500416 [dir=none]
	140106731500416 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604681648 -> 140107119188608 [dir=none]
	140107119188608 [label="v
 (256, 256, 3)" fillcolor=orange]
	140107604681648 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604682656 -> 140107604681648
	140107119188608 [label="dec.resblocks.0.convs2.2.parametrizations.weight.original1
 (256, 256, 3)" fillcolor=lightblue]
	140107119188608 -> 140107604682656
	140107604682656 [label=AccumulateGrad]
	140107604682128 -> 140107604681648
	140107119188528 [label="dec.resblocks.0.convs2.2.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119188528 -> 140107604682128
	140107604682128 [label=AccumulateGrad]
	140107604681600 -> 140107604681552
	140107119188368 [label="dec.resblocks.0.convs2.2.bias
 (256)" fillcolor=lightblue]
	140107119188368 -> 140107604681600
	140107604681600 [label=AccumulateGrad]
	140107604681504 -> 140107604681408
	140107604681360 -> 140107604681264
	140107604681360 [label="AddBackward0
------------
alpha: 1"]
	140107604682272 -> 140107604681360
	140107604682272 -> 140106794053648 [dir=none]
	140106794053648 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604682272 -> 140106794053968 [dir=none]
	140106794053968 [label="weight
 (256, 256, 7)" fillcolor=orange]
	140107604682272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604681888 -> 140107604682272
	140107604681888 -> 140106794053488 [dir=none]
	140106794053488 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107604681888 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508471488 -> 140107604681888
	140107508471488 -> 140106794052368 [dir=none]
	140106794052368 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508471488 -> 140106794051648 [dir=none]
	140106794051648 [label="weight
 (256, 256, 7)" fillcolor=orange]
	140107508471488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508472112 -> 140107508471488
	140107508472112 -> 140106794051808 [dir=none]
	140106794051808 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508472112 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604681744 -> 140107508472112
	140107604681744 [label="AddBackward0
------------
alpha: 1"]
	140107508472496 -> 140107604681744
	140107508472496 -> 140106794051408 [dir=none]
	140106794051408 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508472496 -> 140106794051568 [dir=none]
	140106794051568 [label="weight
 (256, 256, 7)" fillcolor=orange]
	140107508472496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508472976 -> 140107508472496
	140107508472976 -> 140106794049808 [dir=none]
	140106794049808 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508472976 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508474416 -> 140107508472976
	140107508474416 -> 140106794051088 [dir=none]
	140106794051088 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508474416 -> 140107103439744 [dir=none]
	140107103439744 [label="weight
 (256, 256, 7)" fillcolor=orange]
	140107508474416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (9,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508477248 -> 140107508474416
	140107508477248 -> 140106794050688 [dir=none]
	140106794050688 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508477248 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508471584 -> 140107508477248
	140107508471584 [label="AddBackward0
------------
alpha: 1"]
	140107508477680 -> 140107508471584
	140107508477680 -> 140106794046768 [dir=none]
	140106794046768 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508477680 -> 140106794043408 [dir=none]
	140106794043408 [label="weight
 (256, 256, 7)" fillcolor=orange]
	140107508477680 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508477776 -> 140107508477680
	140107508477776 -> 140106794049488 [dir=none]
	140106794049488 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508477776 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508477968 -> 140107508477776
	140107508477968 -> 140106798120640 [dir=none]
	140106798120640 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508477968 -> 140107934844464 [dir=none]
	140107934844464 [label="weight
 (256, 256, 7)" fillcolor=orange]
	140107508477968 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508478064 -> 140107508477968
	140107508478064 -> 140107071076800 [dir=none]
	140107071076800 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508478064 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604682944 -> 140107508478064
	140107508478016 -> 140107508477968
	140107508478016 -> 140107119189968 [dir=none]
	140107119189968 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508478016 -> 140106731499616 [dir=none]
	140106731499616 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508478016 -> 140107119190048 [dir=none]
	140107119190048 [label="v
 (256, 256, 7)" fillcolor=orange]
	140107508478016 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508478160 -> 140107508478016
	140107119190048 [label="dec.resblocks.1.convs1.0.parametrizations.weight.original1
 (256, 256, 7)" fillcolor=lightblue]
	140107119190048 -> 140107508478160
	140107508478160 [label=AccumulateGrad]
	140107508478208 -> 140107508478016
	140107119189968 [label="dec.resblocks.1.convs1.0.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119189968 -> 140107508478208
	140107508478208 [label=AccumulateGrad]
	140107508477872 -> 140107508477968
	140107119189568 [label="dec.resblocks.1.convs1.0.bias
 (256)" fillcolor=lightblue]
	140107119189568 -> 140107508477872
	140107508477872 [label=AccumulateGrad]
	140107508477728 -> 140107508477680
	140107508477728 -> 140107119193808 [dir=none]
	140107119193808 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508477728 -> 140106731501056 [dir=none]
	140106731501056 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508477728 -> 140107119193968 [dir=none]
	140107119193968 [label="v
 (256, 256, 7)" fillcolor=orange]
	140107508477728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508478352 -> 140107508477728
	140107119193968 [label="dec.resblocks.1.convs2.0.parametrizations.weight.original1
 (256, 256, 7)" fillcolor=lightblue]
	140107119193968 -> 140107508478352
	140107508478352 [label=AccumulateGrad]
	140107508478112 -> 140107508477728
	140107119193808 [label="dec.resblocks.1.convs2.0.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119193808 -> 140107508478112
	140107508478112 [label=AccumulateGrad]
	140107508477392 -> 140107508477680
	140107119193648 [label="dec.resblocks.1.convs2.0.bias
 (256)" fillcolor=lightblue]
	140107119193648 -> 140107508477392
	140107508477392 [label=AccumulateGrad]
	140107604682944 -> 140107508471584
	140107508477440 -> 140107508474416
	140107508477440 -> 140107119191168 [dir=none]
	140107119191168 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508477440 -> 140106731498336 [dir=none]
	140106731498336 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508477440 -> 140107119191408 [dir=none]
	140107119191408 [label="v
 (256, 256, 7)" fillcolor=orange]
	140107508477440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508477824 -> 140107508477440
	140107119191408 [label="dec.resblocks.1.convs1.1.parametrizations.weight.original1
 (256, 256, 7)" fillcolor=lightblue]
	140107119191408 -> 140107508477824
	140107508477824 [label=AccumulateGrad]
	140107508477536 -> 140107508477440
	140107119191168 [label="dec.resblocks.1.convs1.1.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119191168 -> 140107508477536
	140107508477536 [label=AccumulateGrad]
	140107508473648 -> 140107508474416
	140107119190848 [label="dec.resblocks.1.convs1.1.bias
 (256)" fillcolor=lightblue]
	140107119190848 -> 140107508473648
	140107508473648 [label=AccumulateGrad]
	140107508472256 -> 140107508472496
	140107508472256 -> 140107119195408 [dir=none]
	140107119195408 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508472256 -> 140106731501296 [dir=none]
	140106731501296 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508472256 -> 140107119195488 [dir=none]
	140107119195488 [label="v
 (256, 256, 7)" fillcolor=orange]
	140107508472256 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508478304 -> 140107508472256
	140107119195488 [label="dec.resblocks.1.convs2.1.parametrizations.weight.original1
 (256, 256, 7)" fillcolor=lightblue]
	140107119195488 -> 140107508478304
	140107508478304 [label=AccumulateGrad]
	140107508477584 -> 140107508472256
	140107119195408 [label="dec.resblocks.1.convs2.1.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119195408 -> 140107508477584
	140107508477584 [label=AccumulateGrad]
	140107508472208 -> 140107508472496
	140107119195168 [label="dec.resblocks.1.convs2.1.bias
 (256)" fillcolor=lightblue]
	140107119195168 -> 140107508472208
	140107508472208 [label=AccumulateGrad]
	140107508471584 -> 140107604681744
	140107508471632 -> 140107508471488
	140107508471632 -> 140107119192768 [dir=none]
	140107119192768 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508471632 -> 140106731501376 [dir=none]
	140106731501376 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508471632 -> 140107119193088 [dir=none]
	140107119193088 [label="v
 (256, 256, 7)" fillcolor=orange]
	140107508471632 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508473264 -> 140107508471632
	140107119193088 [label="dec.resblocks.1.convs1.2.parametrizations.weight.original1
 (256, 256, 7)" fillcolor=lightblue]
	140107119193088 -> 140107508473264
	140107508473264 [label=AccumulateGrad]
	140107508472304 -> 140107508471632
	140107119192768 [label="dec.resblocks.1.convs1.2.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119192768 -> 140107508472304
	140107508472304 [label=AccumulateGrad]
	140107508470720 -> 140107508471488
	140107119192448 [label="dec.resblocks.1.convs1.2.bias
 (256)" fillcolor=lightblue]
	140107119192448 -> 140107508470720
	140107508470720 [label=AccumulateGrad]
	140107508470816 -> 140107604682272
	140107508470816 -> 140107119196368 [dir=none]
	140107119196368 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508470816 -> 140106731500816 [dir=none]
	140106731500816 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508470816 -> 140107119196448 [dir=none]
	140107119196448 [label="v
 (256, 256, 7)" fillcolor=orange]
	140107508470816 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604683664 -> 140107508470816
	140107119196448 [label="dec.resblocks.1.convs2.2.parametrizations.weight.original1
 (256, 256, 7)" fillcolor=lightblue]
	140107119196448 -> 140107604683664
	140107604683664 [label=AccumulateGrad]
	140107508478256 -> 140107508470816
	140107119196368 [label="dec.resblocks.1.convs2.2.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107119196368 -> 140107508478256
	140107508478256 [label=AccumulateGrad]
	140107508471200 -> 140107604682272
	140107119196128 [label="dec.resblocks.1.convs2.2.bias
 (256)" fillcolor=lightblue]
	140107119196128 -> 140107508471200
	140107508471200 [label=AccumulateGrad]
	140107604681744 -> 140107604681360
	140107604681216 -> 140107604681168
	140107604681216 [label="AddBackward0
------------
alpha: 1"]
	140107604681456 -> 140107604681216
	140107604681456 -> 140106794053168 [dir=none]
	140106794053168 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107604681456 -> 140106794056368 [dir=none]
	140106794056368 [label="weight
 (256, 256, 11)" fillcolor=orange]
	140107604681456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508471296 -> 140107604681456
	140107508471296 -> 140106794056768 [dir=none]
	140106794056768 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508471296 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508478448 -> 140107508471296
	140107508478448 -> 140106794055648 [dir=none]
	140106794055648 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508478448 -> 140106794055408 [dir=none]
	140106794055408 [label="weight
 (256, 256, 11)" fillcolor=orange]
	140107508478448 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (25,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508478544 -> 140107508478448
	140107508478544 -> 140106794055488 [dir=none]
	140106794055488 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508478544 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604681312 -> 140107508478544
	140107604681312 [label="AddBackward0
------------
alpha: 1"]
	140107508478784 -> 140107604681312
	140107508478784 -> 140106794055168 [dir=none]
	140106794055168 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508478784 -> 140106794055248 [dir=none]
	140106794055248 [label="weight
 (256, 256, 11)" fillcolor=orange]
	140107508478784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508478928 -> 140107508478784
	140107508478928 -> 140107053624880 [dir=none]
	140107053624880 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508478928 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508479120 -> 140107508478928
	140107508479120 -> 140106794055088 [dir=none]
	140106794055088 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508479120 -> 140106794054928 [dir=none]
	140106794054928 [label="weight
 (256, 256, 11)" fillcolor=orange]
	140107508479120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508479216 -> 140107508479120
	140107508479216 -> 140106794055008 [dir=none]
	140106794055008 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508479216 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508478640 -> 140107508479216
	140107508478640 [label="AddBackward0
------------
alpha: 1"]
	140107508479456 -> 140107508478640
	140107508479456 -> 140106794054848 [dir=none]
	140106794054848 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508479456 -> 140107065187344 [dir=none]
	140107065187344 [label="weight
 (256, 256, 11)" fillcolor=orange]
	140107508479456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508479552 -> 140107508479456
	140107508479552 -> 140106794054288 [dir=none]
	140106794054288 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508479552 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508479744 -> 140107508479552
	140107508479744 -> 140106794054128 [dir=none]
	140106794054128 [label="input
 (7, 256, 400)" fillcolor=orange]
	140107508479744 -> 140106794054048 [dir=none]
	140106794054048 [label="weight
 (256, 256, 11)" fillcolor=orange]
	140107508479744 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508479840 -> 140107508479744
	140107508479840 -> 140107071076800 [dir=none]
	140107071076800 [label="self
 (7, 256, 400)" fillcolor=orange]
	140107508479840 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604682944 -> 140107508479840
	140107508479792 -> 140107508479744
	140107508479792 -> 140107115004608 [dir=none]
	140107115004608 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508479792 -> 140106731502016 [dir=none]
	140106731502016 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508479792 -> 140107115004688 [dir=none]
	140107115004688 [label="v
 (256, 256, 11)" fillcolor=orange]
	140107508479792 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508479936 -> 140107508479792
	140107115004688 [label="dec.resblocks.2.convs1.0.parametrizations.weight.original1
 (256, 256, 11)" fillcolor=lightblue]
	140107115004688 -> 140107508479936
	140107508479936 [label=AccumulateGrad]
	140107508479984 -> 140107508479792
	140107115004608 [label="dec.resblocks.2.convs1.0.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107115004608 -> 140107508479984
	140107508479984 [label=AccumulateGrad]
	140107508479648 -> 140107508479744
	140107119197648 [label="dec.resblocks.2.convs1.0.bias
 (256)" fillcolor=lightblue]
	140107119197648 -> 140107508479648
	140107508479648 [label=AccumulateGrad]
	140107508479504 -> 140107508479456
	140107508479504 -> 140107115011008 [dir=none]
	140107115011008 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508479504 -> 140106731500736 [dir=none]
	140106731500736 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508479504 -> 140107115011088 [dir=none]
	140107115011088 [label="v
 (256, 256, 11)" fillcolor=orange]
	140107508479504 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508480080 -> 140107508479504
	140107115011088 [label="dec.resblocks.2.convs2.0.parametrizations.weight.original1
 (256, 256, 11)" fillcolor=lightblue]
	140107115011088 -> 140107508480080
	140107508480080 [label=AccumulateGrad]
	140107508480128 -> 140107508479504
	140107115011008 [label="dec.resblocks.2.convs2.0.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107115011008 -> 140107508480128
	140107508480128 [label=AccumulateGrad]
	140107508479360 -> 140107508479456
	140107115009488 [label="dec.resblocks.2.convs2.0.bias
 (256)" fillcolor=lightblue]
	140107115009488 -> 140107508479360
	140107508479360 [label=AccumulateGrad]
	140107604682944 -> 140107508478640
	140107508479168 -> 140107508479120
	140107508479168 -> 140107115006288 [dir=none]
	140107115006288 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508479168 -> 140106731501776 [dir=none]
	140106731501776 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508479168 -> 140107934788288 [dir=none]
	140107934788288 [label="v
 (256, 256, 11)" fillcolor=orange]
	140107508479168 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508479600 -> 140107508479168
	140107934788288 [label="dec.resblocks.2.convs1.1.parametrizations.weight.original1
 (256, 256, 11)" fillcolor=lightblue]
	140107934788288 -> 140107508479600
	140107508479600 [label=AccumulateGrad]
	140107508479312 -> 140107508479168
	140107115006288 [label="dec.resblocks.2.convs1.1.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107115006288 -> 140107508479312
	140107508479312 [label=AccumulateGrad]
	140107508479024 -> 140107508479120
	140107115006048 [label="dec.resblocks.2.convs1.1.bias
 (256)" fillcolor=lightblue]
	140107115006048 -> 140107508479024
	140107508479024 [label=AccumulateGrad]
	140107508478880 -> 140107508478784
	140107508478880 -> 140107115012448 [dir=none]
	140107115012448 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508478880 -> 140106731502496 [dir=none]
	140106731502496 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508478880 -> 140107115012528 [dir=none]
	140107115012528 [label="v
 (256, 256, 11)" fillcolor=orange]
	140107508478880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508480032 -> 140107508478880
	140107115012528 [label="dec.resblocks.2.convs2.1.parametrizations.weight.original1
 (256, 256, 11)" fillcolor=lightblue]
	140107115012528 -> 140107508480032
	140107508480032 [label=AccumulateGrad]
	140107508479264 -> 140107508478880
	140107115012448 [label="dec.resblocks.2.convs2.1.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107115012448 -> 140107508479264
	140107508479264 [label=AccumulateGrad]
	140107508478832 -> 140107508478784
	140107115012208 [label="dec.resblocks.2.convs2.1.bias
 (256)" fillcolor=lightblue]
	140107115012208 -> 140107508478832
	140107508478832 [label=AccumulateGrad]
	140107508478640 -> 140107604681312
	140107508478496 -> 140107508478448
	140107508478496 -> 140107115007888 [dir=none]
	140107115007888 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508478496 -> 140106731502736 [dir=none]
	140106731502736 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508478496 -> 140107115007968 [dir=none]
	140107115007968 [label="v
 (256, 256, 11)" fillcolor=orange]
	140107508478496 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508478976 -> 140107508478496
	140107115007968 [label="dec.resblocks.2.convs1.2.parametrizations.weight.original1
 (256, 256, 11)" fillcolor=lightblue]
	140107115007968 -> 140107508478976
	140107508478976 [label=AccumulateGrad]
	140107508478688 -> 140107508478496
	140107115007888 [label="dec.resblocks.2.convs1.2.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107115007888 -> 140107508478688
	140107508478688 [label=AccumulateGrad]
	140107508477920 -> 140107508478448
	140107115007728 [label="dec.resblocks.2.convs1.2.bias
 (256)" fillcolor=lightblue]
	140107115007728 -> 140107508477920
	140107508477920 [label=AccumulateGrad]
	140107508477632 -> 140107604681456
	140107508477632 -> 140107115013968 [dir=none]
	140107115013968 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107508477632 -> 140106731501856 [dir=none]
	140106731501856 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107508477632 -> 140107115014048 [dir=none]
	140107115014048 [label="v
 (256, 256, 11)" fillcolor=orange]
	140107508477632 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508480176 -> 140107508477632
	140107115014048 [label="dec.resblocks.2.convs2.2.parametrizations.weight.original1
 (256, 256, 11)" fillcolor=lightblue]
	140107115014048 -> 140107508480176
	140107508480176 [label=AccumulateGrad]
	140107508478592 -> 140107508477632
	140107115013968 [label="dec.resblocks.2.convs2.2.parametrizations.weight.original0
 (256, 1, 1)" fillcolor=lightblue]
	140107115013968 -> 140107508478592
	140107508478592 [label=AccumulateGrad]
	140107508472592 -> 140107604681456
	140107115013488 [label="dec.resblocks.2.convs2.2.bias
 (256)" fillcolor=lightblue]
	140107115013488 -> 140107508472592
	140107508472592 [label=AccumulateGrad]
	140107604681312 -> 140107604681216
	140107604680640 -> 140107604679872
	140107604680640 -> 140107934883072 [dir=none]
	140107934883072 [label="g
 (256, 1, 1)" fillcolor=orange]
	140107604680640 -> 140106731502096 [dir=none]
	140106731502096 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140107604680640 -> 140107125192752 [dir=none]
	140107125192752 [label="v
 (256, 128, 8)" fillcolor=orange]
	140107604680640 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604680928 -> 140107604680640
	140107125192752 [label="dec.ups.1.weight_v
 (256, 128, 8)" fillcolor=lightblue]
	140107125192752 -> 140107604680928
	140107604680928 [label=AccumulateGrad]
	140107604681072 -> 140107604680640
	140107934883072 [label="dec.ups.1.weight_g
 (256, 1, 1)" fillcolor=lightblue]
	140107934883072 -> 140107604681072
	140107604681072 [label=AccumulateGrad]
	140107604680688 -> 140107604679872
	140107125192512 [label="dec.ups.1.bias
 (128)" fillcolor=lightblue]
	140107125192512 -> 140107604680688
	140107604680688 [label=AccumulateGrad]
	140107604680448 -> 140107604680400
	140107604680448 -> 140107115014688 [dir=none]
	140107115014688 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604680448 -> 140106731503536 [dir=none]
	140106731503536 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604680448 -> 140107115014768 [dir=none]
	140107115014768 [label="v
 (128, 128, 3)" fillcolor=orange]
	140107604680448 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604680976 -> 140107604680448
	140107115014768 [label="dec.resblocks.3.convs1.0.parametrizations.weight.original1
 (128, 128, 3)" fillcolor=lightblue]
	140107115014768 -> 140107604680976
	140107604680976 [label=AccumulateGrad]
	140107604680880 -> 140107604680448
	140107115014688 [label="dec.resblocks.3.convs1.0.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107115014688 -> 140107604680880
	140107604680880 [label=AccumulateGrad]
	140107604680304 -> 140107604680400
	140107115014288 [label="dec.resblocks.3.convs1.0.bias
 (128)" fillcolor=lightblue]
	140107115014288 -> 140107604680304
	140107604680304 [label=AccumulateGrad]
	140107604680112 -> 140107604680016
	140107604680112 -> 140107115018528 [dir=none]
	140107115018528 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604680112 -> 140106731501696 [dir=none]
	140106731501696 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604680112 -> 140107115018608 [dir=none]
	140107115018608 [label="v
 (128, 128, 3)" fillcolor=orange]
	140107604680112 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604680736 -> 140107604680112
	140107115018608 [label="dec.resblocks.3.convs2.0.parametrizations.weight.original1
 (128, 128, 3)" fillcolor=lightblue]
	140107115018608 -> 140107604680736
	140107604680736 [label=AccumulateGrad]
	140107604680592 -> 140107604680112
	140107115018528 [label="dec.resblocks.3.convs2.0.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107115018528 -> 140107604680592
	140107604680592 [label=AccumulateGrad]
	140107604680064 -> 140107604680016
	140107115018288 [label="dec.resblocks.3.convs2.0.bias
 (128)" fillcolor=lightblue]
	140107115018288 -> 140107604680064
	140107604680064 [label=AccumulateGrad]
	140107604679872 -> 140107604679104
	140107604679680 -> 140107604679632
	140107604679680 -> 140107115015968 [dir=none]
	140107115015968 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604679680 -> 140106731498016 [dir=none]
	140106731498016 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604679680 -> 140107115016048 [dir=none]
	140107115016048 [label="v
 (128, 128, 3)" fillcolor=orange]
	140107604679680 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604680256 -> 140107604679680
	140107115016048 [label="dec.resblocks.3.convs1.1.parametrizations.weight.original1
 (128, 128, 3)" fillcolor=lightblue]
	140107115016048 -> 140107604680256
	140107604680256 [label=AccumulateGrad]
	140107604679920 -> 140107604679680
	140107115015968 [label="dec.resblocks.3.convs1.1.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107115015968 -> 140107604679920
	140107604679920 [label=AccumulateGrad]
	140107604679536 -> 140107604679632
	140107115015728 [label="dec.resblocks.3.convs1.1.bias
 (128)" fillcolor=lightblue]
	140107115015728 -> 140107604679536
	140107604679536 [label=AccumulateGrad]
	140107604679344 -> 140107604679248
	140107604679344 -> 140107115019888 [dir=none]
	140107115019888 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604679344 -> 140106731501536 [dir=none]
	140106731501536 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604679344 -> 140107115019968 [dir=none]
	140107115019968 [label="v
 (128, 128, 3)" fillcolor=orange]
	140107604679344 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604680352 -> 140107604679344
	140107115019968 [label="dec.resblocks.3.convs2.1.parametrizations.weight.original1
 (128, 128, 3)" fillcolor=lightblue]
	140107115019968 -> 140107604680352
	140107604680352 [label=AccumulateGrad]
	140107604679824 -> 140107604679344
	140107115019888 [label="dec.resblocks.3.convs2.1.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107115019888 -> 140107604679824
	140107604679824 [label=AccumulateGrad]
	140107604679296 -> 140107604679248
	140107115019728 [label="dec.resblocks.3.convs2.1.bias
 (128)" fillcolor=lightblue]
	140107115019728 -> 140107604679296
	140107604679296 [label=AccumulateGrad]
	140107604679104 -> 140107604678432
	140107604678912 -> 140107604678864
	140107604678912 -> 140107115017008 [dir=none]
	140107115017008 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604678912 -> 140106731503216 [dir=none]
	140106731503216 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604678912 -> 140107115017168 [dir=none]
	140107115017168 [label="v
 (128, 128, 3)" fillcolor=orange]
	140107604678912 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604679488 -> 140107604678912
	140107115017168 [label="dec.resblocks.3.convs1.2.parametrizations.weight.original1
 (128, 128, 3)" fillcolor=lightblue]
	140107115017168 -> 140107604679488
	140107604679488 [label=AccumulateGrad]
	140107604679152 -> 140107604678912
	140107115017008 [label="dec.resblocks.3.convs1.2.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107115017008 -> 140107604679152
	140107604679152 [label=AccumulateGrad]
	140107604678768 -> 140107604678864
	140107115016848 [label="dec.resblocks.3.convs1.2.bias
 (128)" fillcolor=lightblue]
	140107115016848 -> 140107604678768
	140107604678768 [label=AccumulateGrad]
	140107604678576 -> 140107604678480
	140107604678576 -> 140107110286016 [dir=none]
	140107110286016 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604678576 -> 140106731503136 [dir=none]
	140106731503136 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604678576 -> 140107110286096 [dir=none]
	140107110286096 [label="v
 (128, 128, 3)" fillcolor=orange]
	140107604678576 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604679584 -> 140107604678576
	140107110286096 [label="dec.resblocks.3.convs2.2.parametrizations.weight.original1
 (128, 128, 3)" fillcolor=lightblue]
	140107110286096 -> 140107604679584
	140107604679584 [label=AccumulateGrad]
	140107604679056 -> 140107604678576
	140107110286016 [label="dec.resblocks.3.convs2.2.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110286016 -> 140107604679056
	140107604679056 [label=AccumulateGrad]
	140107604678528 -> 140107604678480
	140107110285776 [label="dec.resblocks.3.convs2.2.bias
 (128)" fillcolor=lightblue]
	140107110285776 -> 140107604678528
	140107604678528 [label=AccumulateGrad]
	140107604678432 -> 140107604678336
	140107604678288 -> 140107604678192
	140107604678288 [label="AddBackward0
------------
alpha: 1"]
	140107604679200 -> 140107604678288
	140107604679200 -> 140106794053808 [dir=none]
	140106794053808 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604679200 -> 140106794051488 [dir=none]
	140106794051488 [label="weight
 (128, 128, 7)" fillcolor=orange]
	140107604679200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604678816 -> 140107604679200
	140107604678816 -> 140106794054208 [dir=none]
	140106794054208 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107604678816 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508479408 -> 140107604678816
	140107508479408 -> 140106794053888 [dir=none]
	140106794053888 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107508479408 -> 140106794053328 [dir=none]
	140106794053328 [label="weight
 (128, 128, 7)" fillcolor=orange]
	140107508479408 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508479072 -> 140107508479408
	140107508479072 -> 140106794053248 [dir=none]
	140106794053248 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508479072 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604678720 -> 140107508479072
	140107604678720 [label="AddBackward0
------------
alpha: 1"]
	140107508480368 -> 140107604678720
	140107508480368 -> 140106794050848 [dir=none]
	140106794050848 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107508480368 -> 140106794051888 [dir=none]
	140106794051888 [label="weight
 (128, 128, 7)" fillcolor=orange]
	140107508480368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508480512 -> 140107508480368
	140107508480512 -> 140106794051248 [dir=none]
	140106794051248 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508480512 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508480704 -> 140107508480512
	140107508480704 -> 140106794049168 [dir=none]
	140106794049168 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107508480704 -> 140107103452544 [dir=none]
	140107103452544 [label="weight
 (128, 128, 7)" fillcolor=orange]
	140107508480704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (9,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508480800 -> 140107508480704
	140107508480800 -> 140106794049008 [dir=none]
	140106794049008 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508480800 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508480224 -> 140107508480800
	140107508480224 [label="AddBackward0
------------
alpha: 1"]
	140107508481040 -> 140107508480224
	140107508481040 -> 140106794048688 [dir=none]
	140106794048688 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107508481040 -> 140106794048768 [dir=none]
	140106794048768 [label="weight
 (128, 128, 7)" fillcolor=orange]
	140107508481040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508481136 -> 140107508481040
	140107508481136 -> 140106794048608 [dir=none]
	140106794048608 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508481136 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508481328 -> 140107508481136
	140107508481328 -> 140106794057008 [dir=none]
	140106794057008 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107508481328 -> 140106794048128 [dir=none]
	140106794048128 [label="weight
 (128, 128, 7)" fillcolor=orange]
	140107508481328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508481424 -> 140107508481328
	140107508481424 -> 140107125192672 [dir=none]
	140107125192672 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508481424 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604679872 -> 140107508481424
	140107508481376 -> 140107508481328
	140107508481376 -> 140107110287856 [dir=none]
	140107110287856 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508481376 -> 140106731487696 [dir=none]
	140106731487696 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508481376 -> 140107110287936 [dir=none]
	140107110287936 [label="v
 (128, 128, 7)" fillcolor=orange]
	140107508481376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508481520 -> 140107508481376
	140107110287936 [label="dec.resblocks.4.convs1.0.parametrizations.weight.original1
 (128, 128, 7)" fillcolor=lightblue]
	140107110287936 -> 140107508481520
	140107508481520 [label=AccumulateGrad]
	140107508481568 -> 140107508481376
	140107110287856 [label="dec.resblocks.4.convs1.0.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110287856 -> 140107508481568
	140107508481568 [label=AccumulateGrad]
	140107508481232 -> 140107508481328
	140107110287536 [label="dec.resblocks.4.convs1.0.bias
 (128)" fillcolor=lightblue]
	140107110287536 -> 140107508481232
	140107508481232 [label=AccumulateGrad]
	140107508481088 -> 140107508481040
	140107508481088 -> 140107110293216 [dir=none]
	140107110293216 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508481088 -> 140106725081152 [dir=none]
	140106725081152 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508481088 -> 140107110293296 [dir=none]
	140107110293296 [label="v
 (128, 128, 7)" fillcolor=orange]
	140107508481088 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508481712 -> 140107508481088
	140107110293296 [label="dec.resblocks.4.convs2.0.parametrizations.weight.original1
 (128, 128, 7)" fillcolor=lightblue]
	140107110293296 -> 140107508481712
	140107508481712 [label=AccumulateGrad]
	140107508481472 -> 140107508481088
	140107110293216 [label="dec.resblocks.4.convs2.0.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110293216 -> 140107508481472
	140107508481472 [label=AccumulateGrad]
	140107508480944 -> 140107508481040
	140107110293056 [label="dec.resblocks.4.convs2.0.bias
 (128)" fillcolor=lightblue]
	140107110293056 -> 140107508480944
	140107508480944 [label=AccumulateGrad]
	140107604679872 -> 140107508480224
	140107508480752 -> 140107508480704
	140107508480752 -> 140107110289696 [dir=none]
	140107110289696 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508480752 -> 140106725081872 [dir=none]
	140106725081872 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508480752 -> 140107110289936 [dir=none]
	140107110289936 [label="v
 (128, 128, 7)" fillcolor=orange]
	140107508480752 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508481184 -> 140107508480752
	140107110289936 [label="dec.resblocks.4.convs1.1.parametrizations.weight.original1
 (128, 128, 7)" fillcolor=lightblue]
	140107110289936 -> 140107508481184
	140107508481184 [label=AccumulateGrad]
	140107508480896 -> 140107508480752
	140107110289696 [label="dec.resblocks.4.convs1.1.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110289696 -> 140107508480896
	140107508480896 [label=AccumulateGrad]
	140107508480608 -> 140107508480704
	140107110289056 [label="dec.resblocks.4.convs1.1.bias
 (128)" fillcolor=lightblue]
	140107110289056 -> 140107508480608
	140107508480608 [label=AccumulateGrad]
	140107508480464 -> 140107508480368
	140107508480464 -> 140107110293936 [dir=none]
	140107110293936 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508480464 -> 140106725081472 [dir=none]
	140106725081472 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508480464 -> 140107110294016 [dir=none]
	140107110294016 [label="v
 (128, 128, 7)" fillcolor=orange]
	140107508480464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508481664 -> 140107508480464
	140107110294016 [label="dec.resblocks.4.convs2.1.parametrizations.weight.original1
 (128, 128, 7)" fillcolor=lightblue]
	140107110294016 -> 140107508481664
	140107508481664 [label=AccumulateGrad]
	140107508480848 -> 140107508480464
	140107110293936 [label="dec.resblocks.4.convs2.1.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110293936 -> 140107508480848
	140107508480848 [label=AccumulateGrad]
	140107508480416 -> 140107508480368
	140107110293776 [label="dec.resblocks.4.convs2.1.bias
 (128)" fillcolor=lightblue]
	140107110293776 -> 140107508480416
	140107508480416 [label=AccumulateGrad]
	140107508480224 -> 140107604678720
	140107508478400 -> 140107508479408
	140107508478400 -> 140107110291376 [dir=none]
	140107110291376 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508478400 -> 140106725081792 [dir=none]
	140106725081792 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508478400 -> 140107110291696 [dir=none]
	140107110291696 [label="v
 (128, 128, 7)" fillcolor=orange]
	140107508478400 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508480560 -> 140107508478400
	140107110291696 [label="dec.resblocks.4.convs1.2.parametrizations.weight.original1
 (128, 128, 7)" fillcolor=lightblue]
	140107110291696 -> 140107508480560
	140107508480560 [label=AccumulateGrad]
	140107508480272 -> 140107508478400
	140107110291376 [label="dec.resblocks.4.convs1.2.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110291376 -> 140107508480272
	140107508480272 [label=AccumulateGrad]
	140107508474368 -> 140107508479408
	140107110291136 [label="dec.resblocks.4.convs1.2.bias
 (128)" fillcolor=lightblue]
	140107110291136 -> 140107508474368
	140107508474368 [label=AccumulateGrad]
	140107604679968 -> 140107604679200
	140107604679968 -> 140107110294816 [dir=none]
	140107110294816 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604679968 -> 140106725081232 [dir=none]
	140106725081232 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604679968 -> 140107110294896 [dir=none]
	140107110294896 [label="v
 (128, 128, 7)" fillcolor=orange]
	140107604679968 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508481616 -> 140107604679968
	140107110294896 [label="dec.resblocks.4.convs2.2.parametrizations.weight.original1
 (128, 128, 7)" fillcolor=lightblue]
	140107110294896 -> 140107508481616
	140107508481616 [label=AccumulateGrad]
	140107508479696 -> 140107604679968
	140107110294816 [label="dec.resblocks.4.convs2.2.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110294816 -> 140107508479696
	140107508479696 [label=AccumulateGrad]
	140107508468464 -> 140107604679200
	140107110294656 [label="dec.resblocks.4.convs2.2.bias
 (128)" fillcolor=lightblue]
	140107110294656 -> 140107508468464
	140107508468464 [label=AccumulateGrad]
	140107604678720 -> 140107604678288
	140107604678144 -> 140107604678096
	140107604678144 [label="AddBackward0
------------
alpha: 1"]
	140107604678384 -> 140107604678144
	140107604678384 -> 140106794057568 [dir=none]
	140106794057568 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107604678384 -> 140106794057648 [dir=none]
	140106794057648 [label="weight
 (128, 128, 11)" fillcolor=orange]
	140107604678384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508478736 -> 140107604678384
	140107508478736 -> 140106794057488 [dir=none]
	140106794057488 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508478736 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107508481808 -> 140107508478736
	140107508481808 -> 140106794057328 [dir=none]
	140106794057328 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107508481808 -> 140106794057168 [dir=none]
	140106794057168 [label="weight
 (128, 128, 11)" fillcolor=orange]
	140107508481808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (25,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107508481904 -> 140107508481808
	140107508481904 -> 140106794057248 [dir=none]
	140106794057248 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107508481904 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604678240 -> 140107508481904
	140107604678240 [label="AddBackward0
------------
alpha: 1"]
	140107182850208 -> 140107604678240
	140107182850208 -> 140106794056928 [dir=none]
	140106794056928 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107182850208 -> 140106794057088 [dir=none]
	140106794057088 [label="weight
 (128, 128, 11)" fillcolor=orange]
	140107182850208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182850352 -> 140107182850208
	140107182850352 -> 140107103440704 [dir=none]
	140107103440704 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107182850352 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182850544 -> 140107182850352
	140107182850544 -> 140106794056688 [dir=none]
	140106794056688 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107182850544 -> 140106794055968 [dir=none]
	140106794055968 [label="weight
 (128, 128, 11)" fillcolor=orange]
	140107182850544 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182850640 -> 140107182850544
	140107182850640 -> 140106794056048 [dir=none]
	140106794056048 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107182850640 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182850112 -> 140107182850640
	140107182850112 [label="AddBackward0
------------
alpha: 1"]
	140107182850880 -> 140107182850112
	140107182850880 -> 140106794054768 [dir=none]
	140106794054768 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107182850880 -> 140106800815840 [dir=none]
	140106800815840 [label="weight
 (128, 128, 11)" fillcolor=orange]
	140107182850880 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182850976 -> 140107182850880
	140107182850976 -> 140106794054688 [dir=none]
	140106794054688 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107182850976 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182851168 -> 140107182850976
	140107182851168 -> 140106794054528 [dir=none]
	140106794054528 [label="input
 (7, 128, 1600)" fillcolor=orange]
	140107182851168 -> 140106794047808 [dir=none]
	140106794047808 [label="weight
 (128, 128, 11)" fillcolor=orange]
	140107182851168 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182851264 -> 140107182851168
	140107182851264 -> 140107125192672 [dir=none]
	140107125192672 [label="self
 (7, 128, 1600)" fillcolor=orange]
	140107182851264 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604679872 -> 140107182851264
	140107182851216 -> 140107182851168
	140107182851216 -> 140107110296336 [dir=none]
	140107110296336 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107182851216 -> 140106725081952 [dir=none]
	140106725081952 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107182851216 -> 140107110296416 [dir=none]
	140107110296416 [label="v
 (128, 128, 11)" fillcolor=orange]
	140107182851216 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182851360 -> 140107182851216
	140107110296416 [label="dec.resblocks.5.convs1.0.parametrizations.weight.original1
 (128, 128, 11)" fillcolor=lightblue]
	140107110296416 -> 140107182851360
	140107182851360 [label=AccumulateGrad]
	140107182851408 -> 140107182851216
	140107110296336 [label="dec.resblocks.5.convs1.0.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110296336 -> 140107182851408
	140107182851408 [label=AccumulateGrad]
	140107182851072 -> 140107182851168
	140107110294496 [label="dec.resblocks.5.convs1.0.bias
 (128)" fillcolor=lightblue]
	140107110294496 -> 140107182851072
	140107182851072 [label=AccumulateGrad]
	140107182850928 -> 140107182850880
	140107182850928 -> 140107110300256 [dir=none]
	140107110300256 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107182850928 -> 140106725082192 [dir=none]
	140106725082192 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107182850928 -> 140107110300336 [dir=none]
	140107110300336 [label="v
 (128, 128, 11)" fillcolor=orange]
	140107182850928 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182851552 -> 140107182850928
	140107110300336 [label="dec.resblocks.5.convs2.0.parametrizations.weight.original1
 (128, 128, 11)" fillcolor=lightblue]
	140107110300336 -> 140107182851552
	140107182851552 [label=AccumulateGrad]
	140107182851312 -> 140107182850928
	140107110300256 [label="dec.resblocks.5.convs2.0.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110300256 -> 140107182851312
	140107182851312 [label=AccumulateGrad]
	140107182850784 -> 140107182850880
	140107110299776 [label="dec.resblocks.5.convs2.0.bias
 (128)" fillcolor=lightblue]
	140107110299776 -> 140107182850784
	140107182850784 [label=AccumulateGrad]
	140107604679872 -> 140107182850112
	140107182850592 -> 140107182850544
	140107182850592 -> 140107110297456 [dir=none]
	140107110297456 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107182850592 -> 140106725082112 [dir=none]
	140106725082112 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107182850592 -> 140107110297616 [dir=none]
	140107110297616 [label="v
 (128, 128, 11)" fillcolor=orange]
	140107182850592 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182851024 -> 140107182850592
	140107110297616 [label="dec.resblocks.5.convs1.1.parametrizations.weight.original1
 (128, 128, 11)" fillcolor=lightblue]
	140107110297616 -> 140107182851024
	140107182851024 [label=AccumulateGrad]
	140107182850736 -> 140107182850592
	140107110297456 [label="dec.resblocks.5.convs1.1.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110297456 -> 140107182850736
	140107182850736 [label=AccumulateGrad]
	140107182850448 -> 140107182850544
	140107110297216 [label="dec.resblocks.5.convs1.1.bias
 (128)" fillcolor=lightblue]
	140107110297216 -> 140107182850448
	140107182850448 [label=AccumulateGrad]
	140107182850304 -> 140107182850208
	140107182850304 -> 140107110301456 [dir=none]
	140107110301456 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107182850304 -> 140106725082512 [dir=none]
	140106725082512 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107182850304 -> 140107110301536 [dir=none]
	140107110301536 [label="v
 (128, 128, 11)" fillcolor=orange]
	140107182850304 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182851504 -> 140107182850304
	140107110301536 [label="dec.resblocks.5.convs2.1.parametrizations.weight.original1
 (128, 128, 11)" fillcolor=lightblue]
	140107110301536 -> 140107182851504
	140107182851504 [label=AccumulateGrad]
	140107182850688 -> 140107182850304
	140107110301456 [label="dec.resblocks.5.convs2.1.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110301456 -> 140107182850688
	140107182850688 [label=AccumulateGrad]
	140107182850256 -> 140107182850208
	140107110301216 [label="dec.resblocks.5.convs2.1.bias
 (128)" fillcolor=lightblue]
	140107110301216 -> 140107182850256
	140107182850256 [label=AccumulateGrad]
	140107182850112 -> 140107604678240
	140107508481856 -> 140107508481808
	140107508481856 -> 140107110298496 [dir=none]
	140107110298496 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508481856 -> 140106725082592 [dir=none]
	140106725082592 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508481856 -> 140107110298576 [dir=none]
	140107110298576 [label="v
 (128, 128, 11)" fillcolor=orange]
	140107508481856 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508482000 -> 140107508481856
	140107110298576 [label="dec.resblocks.5.convs1.2.parametrizations.weight.original1
 (128, 128, 11)" fillcolor=lightblue]
	140107110298576 -> 140107508482000
	140107508482000 [label=AccumulateGrad]
	140107182850400 -> 140107508481856
	140107110298496 [label="dec.resblocks.5.convs1.2.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107110298496 -> 140107182850400
	140107182850400 [label=AccumulateGrad]
	140107508481280 -> 140107508481808
	140107110298336 [label="dec.resblocks.5.convs1.2.bias
 (128)" fillcolor=lightblue]
	140107110298336 -> 140107508481280
	140107508481280 [label=AccumulateGrad]
	140107508480992 -> 140107604678384
	140107508480992 -> 140107103437664 [dir=none]
	140107103437664 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107508480992 -> 140106725082672 [dir=none]
	140106725082672 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107508480992 -> 140107103438224 [dir=none]
	140107103438224 [label="v
 (128, 128, 11)" fillcolor=orange]
	140107508480992 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508481952 -> 140107508480992
	140107103438224 [label="dec.resblocks.5.convs2.2.parametrizations.weight.original1
 (128, 128, 11)" fillcolor=lightblue]
	140107103438224 -> 140107508481952
	140107508481952 [label=AccumulateGrad]
	140107508481760 -> 140107508480992
	140107103437664 [label="dec.resblocks.5.convs2.2.parametrizations.weight.original0
 (128, 1, 1)" fillcolor=lightblue]
	140107103437664 -> 140107508481760
	140107508481760 [label=AccumulateGrad]
	140107508480320 -> 140107604678384
	140107103437984 [label="dec.resblocks.5.convs2.2.bias
 (128)" fillcolor=lightblue]
	140107103437984 -> 140107508480320
	140107508480320 [label=AccumulateGrad]
	140107604678240 -> 140107604678144
	140107604677568 -> 140107604676752
	140107604677568 -> 140107125192592 [dir=none]
	140107125192592 [label="g
 (128, 1, 1)" fillcolor=orange]
	140107604677568 -> 140106725082752 [dir=none]
	140106725082752 [label="result1
 (128, 1, 1)" fillcolor=orange]
	140107604677568 -> 140107125193072 [dir=none]
	140107125193072 [label="v
 (128, 64, 8)" fillcolor=orange]
	140107604677568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604677856 -> 140107604677568
	140107125193072 [label="dec.ups.2.weight_v
 (128, 64, 8)" fillcolor=lightblue]
	140107125193072 -> 140107604677856
	140107604677856 [label=AccumulateGrad]
	140107604678000 -> 140107604677568
	140107125192592 [label="dec.ups.2.weight_g
 (128, 1, 1)" fillcolor=lightblue]
	140107125192592 -> 140107604678000
	140107604678000 [label=AccumulateGrad]
	140107604677616 -> 140107604676752
	140107125192832 [label="dec.ups.2.bias
 (64)" fillcolor=lightblue]
	140107125192832 -> 140107604677616
	140107604677616 [label=AccumulateGrad]
	140107604677376 -> 140107604677328
	140107604677376 -> 140107103439024 [dir=none]
	140107103439024 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604677376 -> 140106725083232 [dir=none]
	140106725083232 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604677376 -> 140107103439184 [dir=none]
	140107103439184 [label="v
 (64, 64, 3)" fillcolor=orange]
	140107604677376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107508479888 -> 140107604677376
	140107103439184 [label="dec.resblocks.6.convs1.0.parametrizations.weight.original1
 (64, 64, 3)" fillcolor=lightblue]
	140107103439184 -> 140107508479888
	140107508479888 [label=AccumulateGrad]
	140107508480656 -> 140107604677376
	140107103439024 [label="dec.resblocks.6.convs1.0.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103439024 -> 140107508480656
	140107508480656 [label=AccumulateGrad]
	140107604677232 -> 140107604677328
	140107103438704 [label="dec.resblocks.6.convs1.0.bias
 (64)" fillcolor=lightblue]
	140107103438704 -> 140107604677232
	140107604677232 [label=AccumulateGrad]
	140107604676992 -> 140107604676896
	140107604676992 -> 140107103444304 [dir=none]
	140107103444304 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604676992 -> 140106725083392 [dir=none]
	140106725083392 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604676992 -> 140107103444384 [dir=none]
	140107103444384 [label="v
 (64, 64, 3)" fillcolor=orange]
	140107604676992 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107522424064 -> 140107604676992
	140107103444384 [label="dec.resblocks.6.convs2.0.parametrizations.weight.original1
 (64, 64, 3)" fillcolor=lightblue]
	140107103444384 -> 140107522424064
	140107522424064 [label=AccumulateGrad]
	140107604677520 -> 140107604676992
	140107103444304 [label="dec.resblocks.6.convs2.0.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103444304 -> 140107604677520
	140107604677520 [label=AccumulateGrad]
	140107604676944 -> 140107604676896
	140107103443824 [label="dec.resblocks.6.convs2.0.bias
 (64)" fillcolor=lightblue]
	140107103443824 -> 140107604676944
	140107604676944 [label=AccumulateGrad]
	140107604676752 -> 140107604675984
	140107604676560 -> 140107604676512
	140107604676560 -> 140107103441424 [dir=none]
	140107103441424 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604676560 -> 140106725083472 [dir=none]
	140106725083472 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604676560 -> 140107103441584 [dir=none]
	140107103441584 [label="v
 (64, 64, 3)" fillcolor=orange]
	140107604676560 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604677184 -> 140107604676560
	140107103441584 [label="dec.resblocks.6.convs1.1.parametrizations.weight.original1
 (64, 64, 3)" fillcolor=lightblue]
	140107103441584 -> 140107604677184
	140107604677184 [label=AccumulateGrad]
	140107604676800 -> 140107604676560
	140107103441424 [label="dec.resblocks.6.convs1.1.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103441424 -> 140107604676800
	140107604676800 [label=AccumulateGrad]
	140107604676416 -> 140107604676512
	140107103440944 [label="dec.resblocks.6.convs1.1.bias
 (64)" fillcolor=lightblue]
	140107103440944 -> 140107604676416
	140107604676416 [label=AccumulateGrad]
	140107604676224 -> 140107604676128
	140107604676224 -> 140107103445264 [dir=none]
	140107103445264 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604676224 -> 140106725082032 [dir=none]
	140106725082032 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604676224 -> 140107103445344 [dir=none]
	140107103445344 [label="v
 (64, 64, 3)" fillcolor=orange]
	140107604676224 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604677904 -> 140107604676224
	140107103445344 [label="dec.resblocks.6.convs2.1.parametrizations.weight.original1
 (64, 64, 3)" fillcolor=lightblue]
	140107103445344 -> 140107604677904
	140107604677904 [label=AccumulateGrad]
	140107604676704 -> 140107604676224
	140107103445264 [label="dec.resblocks.6.convs2.1.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103445264 -> 140107604676704
	140107604676704 [label=AccumulateGrad]
	140107604676176 -> 140107604676128
	140107103445024 [label="dec.resblocks.6.convs2.1.bias
 (64)" fillcolor=lightblue]
	140107103445024 -> 140107604676176
	140107604676176 [label=AccumulateGrad]
	140107604675984 -> 140107604675360
	140107604675792 -> 140107604675744
	140107604675792 -> 140107103443184 [dir=none]
	140107103443184 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604675792 -> 140106725083632 [dir=none]
	140106725083632 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604675792 -> 140107103443264 [dir=none]
	140107103443264 [label="v
 (64, 64, 3)" fillcolor=orange]
	140107604675792 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604676368 -> 140107604675792
	140107103443264 [label="dec.resblocks.6.convs1.2.parametrizations.weight.original1
 (64, 64, 3)" fillcolor=lightblue]
	140107103443264 -> 140107604676368
	140107604676368 [label=AccumulateGrad]
	140107604676032 -> 140107604675792
	140107103443184 [label="dec.resblocks.6.convs1.2.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103443184 -> 140107604676032
	140107604676032 [label=AccumulateGrad]
	140107604675648 -> 140107604675744
	140107103442624 [label="dec.resblocks.6.convs1.2.bias
 (64)" fillcolor=lightblue]
	140107103442624 -> 140107604675648
	140107604675648 [label=AccumulateGrad]
	140107604675504 -> 140107604675408
	140107604675504 -> 140107103446064 [dir=none]
	140107103446064 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604675504 -> 140106725083552 [dir=none]
	140106725083552 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604675504 -> 140107103446224 [dir=none]
	140107103446224 [label="v
 (64, 64, 3)" fillcolor=orange]
	140107604675504 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604677664 -> 140107604675504
	140107103446224 [label="dec.resblocks.6.convs2.2.parametrizations.weight.original1
 (64, 64, 3)" fillcolor=lightblue]
	140107103446224 -> 140107604677664
	140107604677664 [label=AccumulateGrad]
	140107604675936 -> 140107604675504
	140107103446064 [label="dec.resblocks.6.convs2.2.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103446064 -> 140107604675936
	140107604675936 [label=AccumulateGrad]
	140107604675456 -> 140107604675408
	140107103445824 [label="dec.resblocks.6.convs2.2.bias
 (64)" fillcolor=lightblue]
	140107103445824 -> 140107604675456
	140107604675456 [label=AccumulateGrad]
	140107604675360 -> 140107604675264
	140107604675216 -> 140107604675120
	140107604675216 [label="AddBackward0
------------
alpha: 1"]
	140107604676080 -> 140107604675216
	140107604676080 -> 140106788254672 [dir=none]
	140106788254672 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604676080 -> 140106788254752 [dir=none]
	140106788254752 [label="weight
 (64, 64, 7)" fillcolor=orange]
	140107604676080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604676464 -> 140107604676080
	140107604676464 -> 140106788254592 [dir=none]
	140106788254592 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107604676464 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182851456 -> 140107604676464
	140107182851456 -> 140106788251072 [dir=none]
	140106788251072 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182851456 -> 140106788250672 [dir=none]
	140106788250672 [label="weight
 (64, 64, 7)" fillcolor=orange]
	140107182851456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182851120 -> 140107182851456
	140107182851120 -> 140106788250832 [dir=none]
	140106788250832 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182851120 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604675600 -> 140107182851120
	140107604675600 [label="AddBackward0
------------
alpha: 1"]
	140107182851792 -> 140107604675600
	140107182851792 -> 140106788248832 [dir=none]
	140106788248832 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182851792 -> 140106788250032 [dir=none]
	140106788250032 [label="weight
 (64, 64, 7)" fillcolor=orange]
	140107182851792 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182851936 -> 140107182851792
	140107182851936 -> 140107058265952 [dir=none]
	140107058265952 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182851936 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182852128 -> 140107182851936
	140107182852128 -> 140106788248112 [dir=none]
	140106788248112 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182852128 -> 140106788246192 [dir=none]
	140106788246192 [label="weight
 (64, 64, 7)" fillcolor=orange]
	140107182852128 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (9,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182852224 -> 140107182852128
	140107182852224 -> 140106788246272 [dir=none]
	140106788246272 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182852224 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182851648 -> 140107182852224
	140107182851648 [label="AddBackward0
------------
alpha: 1"]
	140107182852464 -> 140107182851648
	140107182852464 -> 140106788244352 [dir=none]
	140106788244352 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182852464 -> 140106788245392 [dir=none]
	140106788245392 [label="weight
 (64, 64, 7)" fillcolor=orange]
	140107182852464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182852560 -> 140107182852464
	140107182852560 -> 140106788244192 [dir=none]
	140106788244192 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182852560 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182852752 -> 140107182852560
	140107182852752 -> 140107011974592 [dir=none]
	140107011974592 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182852752 -> 140106788241632 [dir=none]
	140106788241632 [label="weight
 (64, 64, 7)" fillcolor=orange]
	140107182852752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182852848 -> 140107182852752
	140107182852848 -> 140107125192992 [dir=none]
	140107125192992 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182852848 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604676752 -> 140107182852848
	140107182852800 -> 140107182852752
	140107182852800 -> 140107103447024 [dir=none]
	140107103447024 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182852800 -> 140106725083792 [dir=none]
	140106725083792 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182852800 -> 140107103447104 [dir=none]
	140107103447104 [label="v
 (64, 64, 7)" fillcolor=orange]
	140107182852800 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182852944 -> 140107182852800
	140107103447104 [label="dec.resblocks.7.convs1.0.parametrizations.weight.original1
 (64, 64, 7)" fillcolor=lightblue]
	140107103447104 -> 140107182852944
	140107182852944 [label=AccumulateGrad]
	140107182852992 -> 140107182852800
	140107103447024 [label="dec.resblocks.7.convs1.0.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103447024 -> 140107182852992
	140107182852992 [label=AccumulateGrad]
	140107182852656 -> 140107182852752
	140107103446864 [label="dec.resblocks.7.convs1.0.bias
 (64)" fillcolor=lightblue]
	140107103446864 -> 140107182852656
	140107182852656 [label=AccumulateGrad]
	140107182852512 -> 140107182852464
	140107182852512 -> 140107103450304 [dir=none]
	140107103450304 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182852512 -> 140106725084512 [dir=none]
	140106725084512 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182852512 -> 140107103450384 [dir=none]
	140107103450384 [label="v
 (64, 64, 7)" fillcolor=orange]
	140107182852512 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182853040 -> 140107182852512
	140107103450384 [label="dec.resblocks.7.convs2.0.parametrizations.weight.original1
 (64, 64, 7)" fillcolor=lightblue]
	140107103450384 -> 140107182853040
	140107182853040 [label=AccumulateGrad]
	140107182852896 -> 140107182852512
	140107103450304 [label="dec.resblocks.7.convs2.0.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103450304 -> 140107182852896
	140107182852896 [label=AccumulateGrad]
	140107182852368 -> 140107182852464
	140107103449904 [label="dec.resblocks.7.convs2.0.bias
 (64)" fillcolor=lightblue]
	140107103449904 -> 140107182852368
	140107182852368 [label=AccumulateGrad]
	140107604676752 -> 140107182851648
	140107182852176 -> 140107182852128
	140107182852176 -> 140107103448064 [dir=none]
	140107103448064 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182852176 -> 140106725084752 [dir=none]
	140106725084752 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182852176 -> 140107103448144 [dir=none]
	140107103448144 [label="v
 (64, 64, 7)" fillcolor=orange]
	140107182852176 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182852608 -> 140107182852176
	140107103448144 [label="dec.resblocks.7.convs1.1.parametrizations.weight.original1
 (64, 64, 7)" fillcolor=lightblue]
	140107103448144 -> 140107182852608
	140107182852608 [label=AccumulateGrad]
	140107182852320 -> 140107182852176
	140107103448064 [label="dec.resblocks.7.convs1.1.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103448064 -> 140107182852320
	140107182852320 [label=AccumulateGrad]
	140107182852032 -> 140107182852128
	140107103447744 [label="dec.resblocks.7.convs1.1.bias
 (64)" fillcolor=lightblue]
	140107103447744 -> 140107182852032
	140107182852032 [label=AccumulateGrad]
	140107182851888 -> 140107182851792
	140107182851888 -> 140107103451424 [dir=none]
	140107103451424 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182851888 -> 140106725084832 [dir=none]
	140106725084832 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182851888 -> 140107103451664 [dir=none]
	140107103451664 [label="v
 (64, 64, 7)" fillcolor=orange]
	140107182851888 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182853184 -> 140107182851888
	140107103451664 [label="dec.resblocks.7.convs2.1.parametrizations.weight.original1
 (64, 64, 7)" fillcolor=lightblue]
	140107103451664 -> 140107182853184
	140107182853184 [label=AccumulateGrad]
	140107182852272 -> 140107182851888
	140107103451424 [label="dec.resblocks.7.convs2.1.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103451424 -> 140107182852272
	140107182852272 [label=AccumulateGrad]
	140107182851840 -> 140107182851792
	140107103451104 [label="dec.resblocks.7.convs2.1.bias
 (64)" fillcolor=lightblue]
	140107103451104 -> 140107182851840
	140107182851840 [label=AccumulateGrad]
	140107182851648 -> 140107604675600
	140107182850496 -> 140107182851456
	140107182850496 -> 140107103448944 [dir=none]
	140107103448944 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182850496 -> 140106725084992 [dir=none]
	140106725084992 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182850496 -> 140107103449024 [dir=none]
	140107103449024 [label="v
 (64, 64, 7)" fillcolor=orange]
	140107182850496 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182851984 -> 140107182850496
	140107103449024 [label="dec.resblocks.7.convs1.2.parametrizations.weight.original1
 (64, 64, 7)" fillcolor=lightblue]
	140107103449024 -> 140107182851984
	140107182851984 [label=AccumulateGrad]
	140107182851696 -> 140107182850496
	140107103448944 [label="dec.resblocks.7.convs1.2.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107103448944 -> 140107182851696
	140107182851696 [label=AccumulateGrad]
	140107182850160 -> 140107182851456
	140107103448704 [label="dec.resblocks.7.convs1.2.bias
 (64)" fillcolor=lightblue]
	140107103448704 -> 140107182850160
	140107182850160 [label=AccumulateGrad]
	140107604675696 -> 140107604676080
	140107604675696 -> 140107099111728 [dir=none]
	140107099111728 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604675696 -> 140106725084352 [dir=none]
	140106725084352 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604675696 -> 140107099112048 [dir=none]
	140107099112048 [label="v
 (64, 64, 7)" fillcolor=orange]
	140107604675696 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182853232 -> 140107604675696
	140107099112048 [label="dec.resblocks.7.convs2.2.parametrizations.weight.original1
 (64, 64, 7)" fillcolor=lightblue]
	140107099112048 -> 140107182853232
	140107182853232 [label=AccumulateGrad]
	140107182851600 -> 140107604675696
	140107099111728 [label="dec.resblocks.7.convs2.2.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099111728 -> 140107182851600
	140107182851600 [label=AccumulateGrad]
	140107604676848 -> 140107604676080
	140107103453104 [label="dec.resblocks.7.convs2.2.bias
 (64)" fillcolor=lightblue]
	140107103453104 -> 140107604676848
	140107604676848 [label=AccumulateGrad]
	140107604675600 -> 140107604675216
	140107604675072 -> 140107604675024
	140107604675072 [label="AddBackward0
------------
alpha: 1"]
	140107604677280 -> 140107604675072
	140107604677280 -> 140106788242432 [dir=none]
	140106788242432 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107604677280 -> 140107125189872 [dir=none]
	140107125189872 [label="weight
 (64, 64, 11)" fillcolor=orange]
	140107604677280 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182850832 -> 140107604677280
	140107182850832 -> 140106788242672 [dir=none]
	140106788242672 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182850832 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182853328 -> 140107182850832
	140107182853328 -> 140106788251632 [dir=none]
	140106788251632 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182853328 -> 140106788242112 [dir=none]
	140106788242112 [label="weight
 (64, 64, 11)" fillcolor=orange]
	140107182853328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (25,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182853424 -> 140107182853328
	140107182853424 -> 140106788255952 [dir=none]
	140106788255952 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182853424 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604675312 -> 140107182853424
	140107604675312 [label="AddBackward0
------------
alpha: 1"]
	140107182853664 -> 140107604675312
	140107182853664 -> 140106794050128 [dir=none]
	140106794050128 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182853664 -> 140106788257232 [dir=none]
	140106788257232 [label="weight
 (64, 64, 11)" fillcolor=orange]
	140107182853664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182853808 -> 140107182853664
	140107182853808 -> 140106794052848 [dir=none]
	140106794052848 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182853808 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182854000 -> 140107182853808
	140107182854000 -> 140106788251392 [dir=none]
	140106788251392 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182854000 -> 140107125189472 [dir=none]
	140107125189472 [label="weight
 (64, 64, 11)" fillcolor=orange]
	140107182854000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182854096 -> 140107182854000
	140107182854096 -> 140107031826400 [dir=none]
	140107031826400 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182854096 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182853520 -> 140107182854096
	140107182853520 [label="AddBackward0
------------
alpha: 1"]
	140107182854336 -> 140107182853520
	140107182854336 -> 140106788257152 [dir=none]
	140106788257152 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182854336 -> 140106788241472 [dir=none]
	140106788241472 [label="weight
 (64, 64, 11)" fillcolor=orange]
	140107182854336 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182854432 -> 140107182854336
	140107182854432 -> 140106788257312 [dir=none]
	140106788257312 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182854432 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182854624 -> 140107182854432
	140107182854624 -> 140106794047888 [dir=none]
	140106794047888 [label="input
 (7, 64, 6400)" fillcolor=orange]
	140107182854624 -> 140106788254992 [dir=none]
	140106788254992 [label="weight
 (64, 64, 11)" fillcolor=orange]
	140107182854624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (64,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182854720 -> 140107182854624
	140107182854720 -> 140107125192992 [dir=none]
	140107125192992 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140107182854720 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604676752 -> 140107182854720
	140107182854672 -> 140107182854624
	140107182854672 -> 140107099113648 [dir=none]
	140107099113648 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182854672 -> 140107133962816 [dir=none]
	140107133962816 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182854672 -> 140107099113808 [dir=none]
	140107099113808 [label="v
 (64, 64, 11)" fillcolor=orange]
	140107182854672 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182854816 -> 140107182854672
	140107099113808 [label="dec.resblocks.8.convs1.0.parametrizations.weight.original1
 (64, 64, 11)" fillcolor=lightblue]
	140107099113808 -> 140107182854816
	140107182854816 [label=AccumulateGrad]
	140107182854864 -> 140107182854672
	140107099113648 [label="dec.resblocks.8.convs1.0.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099113648 -> 140107182854864
	140107182854864 [label=AccumulateGrad]
	140107182854528 -> 140107182854624
	140107099113328 [label="dec.resblocks.8.convs1.0.bias
 (64)" fillcolor=lightblue]
	140107099113328 -> 140107182854528
	140107182854528 [label=AccumulateGrad]
	140107182854384 -> 140107182854336
	140107182854384 -> 140107099118368 [dir=none]
	140107099118368 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182854384 -> 140106725084912 [dir=none]
	140106725084912 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182854384 -> 140107099118528 [dir=none]
	140107099118528 [label="v
 (64, 64, 11)" fillcolor=orange]
	140107182854384 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182855008 -> 140107182854384
	140107099118528 [label="dec.resblocks.8.convs2.0.parametrizations.weight.original1
 (64, 64, 11)" fillcolor=lightblue]
	140107099118528 -> 140107182855008
	140107182855008 [label=AccumulateGrad]
	140107182854768 -> 140107182854384
	140107099118368 [label="dec.resblocks.8.convs2.0.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099118368 -> 140107182854768
	140107182854768 [label=AccumulateGrad]
	140107182854240 -> 140107182854336
	140107099117728 [label="dec.resblocks.8.convs2.0.bias
 (64)" fillcolor=lightblue]
	140107099117728 -> 140107182854240
	140107182854240 [label=AccumulateGrad]
	140107604676752 -> 140107182853520
	140107182854048 -> 140107182854000
	140107182854048 -> 140107099114448 [dir=none]
	140107099114448 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182854048 -> 140106725084592 [dir=none]
	140106725084592 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182854048 -> 140107099114528 [dir=none]
	140107099114528 [label="v
 (64, 64, 11)" fillcolor=orange]
	140107182854048 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182854480 -> 140107182854048
	140107099114528 [label="dec.resblocks.8.convs1.1.parametrizations.weight.original1
 (64, 64, 11)" fillcolor=lightblue]
	140107099114528 -> 140107182854480
	140107182854480 [label=AccumulateGrad]
	140107182854192 -> 140107182854048
	140107099114448 [label="dec.resblocks.8.convs1.1.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099114448 -> 140107182854192
	140107182854192 [label=AccumulateGrad]
	140107182853904 -> 140107182854000
	140107099114288 [label="dec.resblocks.8.convs1.1.bias
 (64)" fillcolor=lightblue]
	140107099114288 -> 140107182853904
	140107182853904 [label=AccumulateGrad]
	140107182853760 -> 140107182853664
	140107182853760 -> 140107099119168 [dir=none]
	140107099119168 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182853760 -> 140106725084032 [dir=none]
	140106725084032 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182853760 -> 140107099119248 [dir=none]
	140107099119248 [label="v
 (64, 64, 11)" fillcolor=orange]
	140107182853760 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182854960 -> 140107182853760
	140107099119248 [label="dec.resblocks.8.convs2.1.parametrizations.weight.original1
 (64, 64, 11)" fillcolor=lightblue]
	140107099119248 -> 140107182854960
	140107182854960 [label=AccumulateGrad]
	140107182854144 -> 140107182853760
	140107099119168 [label="dec.resblocks.8.convs2.1.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099119168 -> 140107182854144
	140107182854144 [label=AccumulateGrad]
	140107182853712 -> 140107182853664
	140107099119008 [label="dec.resblocks.8.convs2.1.bias
 (64)" fillcolor=lightblue]
	140107099119008 -> 140107182853712
	140107182853712 [label=AccumulateGrad]
	140107182853520 -> 140107604675312
	140107182853376 -> 140107182853328
	140107182853376 -> 140107099115728 [dir=none]
	140107099115728 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182853376 -> 140106725085392 [dir=none]
	140106725085392 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182853376 -> 140107099116048 [dir=none]
	140107099116048 [label="v
 (64, 64, 11)" fillcolor=orange]
	140107182853376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182853856 -> 140107182853376
	140107099116048 [label="dec.resblocks.8.convs1.2.parametrizations.weight.original1
 (64, 64, 11)" fillcolor=lightblue]
	140107099116048 -> 140107182853856
	140107182853856 [label=AccumulateGrad]
	140107182853568 -> 140107182853376
	140107099115728 [label="dec.resblocks.8.convs1.2.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099115728 -> 140107182853568
	140107182853568 [label=AccumulateGrad]
	140107182852704 -> 140107182853328
	140107099115488 [label="dec.resblocks.8.convs1.2.bias
 (64)" fillcolor=lightblue]
	140107099115488 -> 140107182852704
	140107182852704 [label=AccumulateGrad]
	140107182852416 -> 140107604677280
	140107182852416 -> 140107099120208 [dir=none]
	140107099120208 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107182852416 -> 140106725085152 [dir=none]
	140106725085152 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107182852416 -> 140107099120288 [dir=none]
	140107099120288 [label="v
 (64, 64, 11)" fillcolor=orange]
	140107182852416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182855056 -> 140107182852416
	140107099120288 [label="dec.resblocks.8.convs2.2.parametrizations.weight.original1
 (64, 64, 11)" fillcolor=lightblue]
	140107099120288 -> 140107182855056
	140107182855056 [label=AccumulateGrad]
	140107182853472 -> 140107182852416
	140107099120208 [label="dec.resblocks.8.convs2.2.parametrizations.weight.original0
 (64, 1, 1)" fillcolor=lightblue]
	140107099120208 -> 140107182853472
	140107182853472 [label=AccumulateGrad]
	140107182851744 -> 140107604677280
	140107099119888 [label="dec.resblocks.8.convs2.2.bias
 (64)" fillcolor=lightblue]
	140107099119888 -> 140107182851744
	140107182851744 [label=AccumulateGrad]
	140107604675312 -> 140107604675072
	140107604674496 -> 140107604673728
	140107604674496 -> 140107125192912 [dir=none]
	140107125192912 [label="g
 (64, 1, 1)" fillcolor=orange]
	140107604674496 -> 140106725084112 [dir=none]
	140106725084112 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140107604674496 -> 140107125193392 [dir=none]
	140107125193392 [label="v
 (64, 32, 2)" fillcolor=orange]
	140107604674496 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604674784 -> 140107604674496
	140107125193392 [label="dec.ups.3.weight_v
 (64, 32, 2)" fillcolor=lightblue]
	140107125193392 -> 140107604674784
	140107604674784 [label=AccumulateGrad]
	140107604674928 -> 140107604674496
	140107125192912 [label="dec.ups.3.weight_g
 (64, 1, 1)" fillcolor=lightblue]
	140107125192912 -> 140107604674928
	140107604674928 [label=AccumulateGrad]
	140107604674544 -> 140107604673728
	140107125193152 [label="dec.ups.3.bias
 (32)" fillcolor=lightblue]
	140107125193152 -> 140107604674544
	140107604674544 [label=AccumulateGrad]
	140107604674304 -> 140107604674256
	140107604674304 -> 140107099122048 [dir=none]
	140107099122048 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107604674304 -> 140106725085232 [dir=none]
	140106725085232 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107604674304 -> 140107099122448 [dir=none]
	140107099122448 [label="v
 (32, 32, 3)" fillcolor=orange]
	140107604674304 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604675168 -> 140107604674304
	140107099122448 [label="dec.resblocks.9.convs1.0.parametrizations.weight.original1
 (32, 32, 3)" fillcolor=lightblue]
	140107099122448 -> 140107604675168
	140107604675168 [label=AccumulateGrad]
	140107604674736 -> 140107604674304
	140107099122048 [label="dec.resblocks.9.convs1.0.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107099122048 -> 140107604674736
	140107604674736 [label=AccumulateGrad]
	140107604674160 -> 140107604674256
	140107099121248 [label="dec.resblocks.9.convs1.0.bias
 (32)" fillcolor=lightblue]
	140107099121248 -> 140107604674160
	140107604674160 [label=AccumulateGrad]
	140107604673968 -> 140107604673872
	140107604673968 -> 140107099125888 [dir=none]
	140107099125888 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107604673968 -> 140106725085472 [dir=none]
	140106725085472 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107604673968 -> 140107099125968 [dir=none]
	140107099125968 [label="v
 (32, 32, 3)" fillcolor=orange]
	140107604673968 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604674832 -> 140107604673968
	140107099125968 [label="dec.resblocks.9.convs2.0.parametrizations.weight.original1
 (32, 32, 3)" fillcolor=lightblue]
	140107099125968 -> 140107604674832
	140107604674832 [label=AccumulateGrad]
	140107604674448 -> 140107604673968
	140107099125888 [label="dec.resblocks.9.convs2.0.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107099125888 -> 140107604674448
	140107604674448 [label=AccumulateGrad]
	140107604673920 -> 140107604673872
	140107099125728 [label="dec.resblocks.9.convs2.0.bias
 (32)" fillcolor=lightblue]
	140107099125728 -> 140107604673920
	140107604673920 [label=AccumulateGrad]
	140107604673728 -> 140107604672960
	140107604673536 -> 140107604673488
	140107604673536 -> 140107099123728 [dir=none]
	140107099123728 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107604673536 -> 140106725085632 [dir=none]
	140106725085632 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107604673536 -> 140107099123808 [dir=none]
	140107099123808 [label="v
 (32, 32, 3)" fillcolor=orange]
	140107604673536 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869111440 -> 140107604673536
	140107099123808 [label="dec.resblocks.9.convs1.1.parametrizations.weight.original1
 (32, 32, 3)" fillcolor=lightblue]
	140107099123808 -> 140106869111440
	140106869111440 [label=AccumulateGrad]
	140107604674112 -> 140107604673536
	140107099123728 [label="dec.resblocks.9.convs1.1.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107099123728 -> 140107604674112
	140107604674112 [label=AccumulateGrad]
	140107604673392 -> 140107604673488
	140107099123568 [label="dec.resblocks.9.convs1.1.bias
 (32)" fillcolor=lightblue]
	140107099123568 -> 140107604673392
	140107604673392 [label=AccumulateGrad]
	140107604673200 -> 140107604673104
	140107604673200 -> 140107099126848 [dir=none]
	140107099126848 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107604673200 -> 140106725085712 [dir=none]
	140106725085712 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107604673200 -> 140107099127008 [dir=none]
	140107099127008 [label="v
 (32, 32, 3)" fillcolor=orange]
	140107604673200 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604674208 -> 140107604673200
	140107099127008 [label="dec.resblocks.9.convs2.1.parametrizations.weight.original1
 (32, 32, 3)" fillcolor=lightblue]
	140107099127008 -> 140107604674208
	140107604674208 [label=AccumulateGrad]
	140107604673680 -> 140107604673200
	140107099126848 [label="dec.resblocks.9.convs2.1.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107099126848 -> 140107604673680
	140107604673680 [label=AccumulateGrad]
	140107604673152 -> 140107604673104
	140107099126688 [label="dec.resblocks.9.convs2.1.bias
 (32)" fillcolor=lightblue]
	140107099126688 -> 140107604673152
	140107604673152 [label=AccumulateGrad]
	140107604672960 -> 140106869112544
	140107604672768 -> 140107604672720
	140107604672768 -> 140107099124688 [dir=none]
	140107099124688 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107604672768 -> 140106725086032 [dir=none]
	140106725086032 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107604672768 -> 140107099124768 [dir=none]
	140107099124768 [label="v
 (32, 32, 3)" fillcolor=orange]
	140107604672768 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604673344 -> 140107604672768
	140107099124768 [label="dec.resblocks.9.convs1.2.parametrizations.weight.original1
 (32, 32, 3)" fillcolor=lightblue]
	140107099124768 -> 140107604673344
	140107604673344 [label=AccumulateGrad]
	140107604673008 -> 140107604672768
	140107099124688 [label="dec.resblocks.9.convs1.2.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107099124688 -> 140107604673008
	140107604673008 [label=AccumulateGrad]
	140107604672624 -> 140107604672720
	140107099124528 [label="dec.resblocks.9.convs1.2.bias
 (32)" fillcolor=lightblue]
	140107099124528 -> 140107604672624
	140107604672624 [label=AccumulateGrad]
	140106869112688 -> 140106869112592
	140106869112688 -> 140107099127648 [dir=none]
	140107099127648 [label="g
 (32, 1, 1)" fillcolor=orange]
	140106869112688 -> 140106725085952 [dir=none]
	140106725085952 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140106869112688 -> 140107092164672 [dir=none]
	140107092164672 [label="v
 (32, 32, 3)" fillcolor=orange]
	140106869112688 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107604674592 -> 140106869112688
	140107092164672 [label="dec.resblocks.9.convs2.2.parametrizations.weight.original1
 (32, 32, 3)" fillcolor=lightblue]
	140107092164672 -> 140107604674592
	140107604674592 [label=AccumulateGrad]
	140107604672912 -> 140106869112688
	140107099127648 [label="dec.resblocks.9.convs2.2.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107099127648 -> 140107604672912
	140107604672912 [label=AccumulateGrad]
	140106869112640 -> 140106869112592
	140107099127488 [label="dec.resblocks.9.convs2.2.bias
 (32)" fillcolor=lightblue]
	140107099127488 -> 140106869112640
	140106869112640 [label=AccumulateGrad]
	140106869112544 -> 140106869112448
	140106869112400 -> 140106869112304
	140106869112400 [label="AddBackward0
------------
alpha: 1"]
	140106869112496 -> 140106869112400
	140106869112496 -> 140106788247472 [dir=none]
	140106788247472 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140106869112496 -> 140107130476304 [dir=none]
	140107130476304 [label="weight
 (32, 32, 7)" fillcolor=orange]
	140106869112496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604673440 -> 140106869112496
	140107604673440 -> 140106788247792 [dir=none]
	140106788247792 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107604673440 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182853280 -> 140107604673440
	140107182853280 -> 140106788246832 [dir=none]
	140106788246832 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182853280 -> 140106788255872 [dir=none]
	140106788255872 [label="weight
 (32, 32, 7)" fillcolor=orange]
	140107182853280 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182855152 -> 140107182853280
	140107182855152 -> 140106788246992 [dir=none]
	140106788246992 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182855152 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604673056 -> 140107182855152
	140107604673056 [label="AddBackward0
------------
alpha: 1"]
	140107182855392 -> 140107604673056
	140107182855392 -> 140106788244752 [dir=none]
	140106788244752 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182855392 -> 140106788243872 [dir=none]
	140106788243872 [label="weight
 (32, 32, 7)" fillcolor=orange]
	140107182855392 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182855536 -> 140107182855392
	140107182855536 -> 140107031828160 [dir=none]
	140107031828160 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182855536 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182855728 -> 140107182855536
	140107182855728 -> 140106788250432 [dir=none]
	140106788250432 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182855728 -> 140107065186864 [dir=none]
	140107065186864 [label="weight
 (32, 32, 7)" fillcolor=orange]
	140107182855728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (9,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182855824 -> 140107182855728
	140107182855824 -> 140107099111488 [dir=none]
	140107099111488 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182855824 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182855248 -> 140107182855824
	140107182855248 [label="AddBackward0
------------
alpha: 1"]
	140107182856064 -> 140107182855248
	140107182856064 -> 140106788249952 [dir=none]
	140106788249952 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182856064 -> 140107065189024 [dir=none]
	140107065189024 [label="weight
 (32, 32, 7)" fillcolor=orange]
	140107182856064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182856160 -> 140107182856064
	140107182856160 -> 140106788249792 [dir=none]
	140106788249792 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182856160 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182856352 -> 140107182856160
	140107182856352 -> 140106788245232 [dir=none]
	140106788245232 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182856352 -> 140106788249552 [dir=none]
	140106788249552 [label="weight
 (32, 32, 7)" fillcolor=orange]
	140107182856352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182856448 -> 140107182856352
	140107182856448 -> 140106788244272 [dir=none]
	140106788244272 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182856448 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604673728 -> 140107182856448
	140107182856400 -> 140107182856352
	140107182856400 -> 140107092165952 [dir=none]
	140107092165952 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182856400 -> 140106725086832 [dir=none]
	140106725086832 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182856400 -> 140107092166032 [dir=none]
	140107092166032 [label="v
 (32, 32, 7)" fillcolor=orange]
	140107182856400 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182856544 -> 140107182856400
	140107092166032 [label="dec.resblocks.10.convs1.0.parametrizations.weight.original1
 (32, 32, 7)" fillcolor=lightblue]
	140107092166032 -> 140107182856544
	140107182856544 [label=AccumulateGrad]
	140107182856592 -> 140107182856400
	140107092165952 [label="dec.resblocks.10.convs1.0.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092165952 -> 140107182856592
	140107182856592 [label=AccumulateGrad]
	140107182856256 -> 140107182856352
	140107092165712 [label="dec.resblocks.10.convs1.0.bias
 (32)" fillcolor=lightblue]
	140107092165712 -> 140107182856256
	140107182856256 [label=AccumulateGrad]
	140107182856112 -> 140107182856064
	140107182856112 -> 140107092173872 [dir=none]
	140107092173872 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182856112 -> 140106725086992 [dir=none]
	140106725086992 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182856112 -> 140107092173952 [dir=none]
	140107092173952 [label="v
 (32, 32, 7)" fillcolor=orange]
	140107182856112 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182856736 -> 140107182856112
	140107092173952 [label="dec.resblocks.10.convs2.0.parametrizations.weight.original1
 (32, 32, 7)" fillcolor=lightblue]
	140107092173952 -> 140107182856736
	140107182856736 [label=AccumulateGrad]
	140107182856496 -> 140107182856112
	140107092173872 [label="dec.resblocks.10.convs2.0.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092173872 -> 140107182856496
	140107182856496 [label=AccumulateGrad]
	140107182855968 -> 140107182856064
	140107092173312 [label="dec.resblocks.10.convs2.0.bias
 (32)" fillcolor=lightblue]
	140107092173312 -> 140107182855968
	140107182855968 [label=AccumulateGrad]
	140107604673728 -> 140107182855248
	140107182855776 -> 140107182855728
	140107182855776 -> 140107092168832 [dir=none]
	140107092168832 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182855776 -> 140106725086592 [dir=none]
	140106725086592 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182855776 -> 140107092168912 [dir=none]
	140107092168912 [label="v
 (32, 32, 7)" fillcolor=orange]
	140107182855776 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182856208 -> 140107182855776
	140107092168912 [label="dec.resblocks.10.convs1.1.parametrizations.weight.original1
 (32, 32, 7)" fillcolor=lightblue]
	140107092168912 -> 140107182856208
	140107182856208 [label=AccumulateGrad]
	140107182855920 -> 140107182855776
	140107092168832 [label="dec.resblocks.10.convs1.1.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092168832 -> 140107182855920
	140107182855920 [label=AccumulateGrad]
	140107182855632 -> 140107182855728
	140107092167792 [label="dec.resblocks.10.convs1.1.bias
 (32)" fillcolor=lightblue]
	140107092167792 -> 140107182855632
	140107182855632 [label=AccumulateGrad]
	140107182855488 -> 140107182855392
	140107182855488 -> 140107092175312 [dir=none]
	140107092175312 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182855488 -> 140106725086912 [dir=none]
	140106725086912 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182855488 -> 140107092175472 [dir=none]
	140107092175472 [label="v
 (32, 32, 7)" fillcolor=orange]
	140107182855488 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182856688 -> 140107182855488
	140107092175472 [label="dec.resblocks.10.convs2.1.parametrizations.weight.original1
 (32, 32, 7)" fillcolor=lightblue]
	140107092175472 -> 140107182856688
	140107182856688 [label=AccumulateGrad]
	140107182855872 -> 140107182855488
	140107092175312 [label="dec.resblocks.10.convs2.1.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092175312 -> 140107182855872
	140107182855872 [label=AccumulateGrad]
	140107182855440 -> 140107182855392
	140107092175072 [label="dec.resblocks.10.convs2.1.bias
 (32)" fillcolor=lightblue]
	140107092175072 -> 140107182855440
	140107182855440 [label=AccumulateGrad]
	140107182855248 -> 140107604673056
	140107182855104 -> 140107182853280
	140107182855104 -> 140107092171232 [dir=none]
	140107092171232 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182855104 -> 140106725087152 [dir=none]
	140106725087152 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182855104 -> 140107092171552 [dir=none]
	140107092171552 [label="v
 (32, 32, 7)" fillcolor=orange]
	140107182855104 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182855584 -> 140107182855104
	140107092171552 [label="dec.resblocks.10.convs1.2.parametrizations.weight.original1
 (32, 32, 7)" fillcolor=lightblue]
	140107092171552 -> 140107182855584
	140107182855584 [label=AccumulateGrad]
	140107182855296 -> 140107182855104
	140107092171232 [label="dec.resblocks.10.convs1.2.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092171232 -> 140107182855296
	140107182855296 [label=AccumulateGrad]
	140107182854912 -> 140107182853280
	140107092170992 [label="dec.resblocks.10.convs1.2.bias
 (32)" fillcolor=lightblue]
	140107092170992 -> 140107182854912
	140107182854912 [label=AccumulateGrad]
	140107604672672 -> 140106869112496
	140107604672672 -> 140107092176752 [dir=none]
	140107092176752 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107604672672 -> 140106725086752 [dir=none]
	140106725086752 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107604672672 -> 140107092176832 [dir=none]
	140107092176832 [label="v
 (32, 32, 7)" fillcolor=orange]
	140107604672672 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182856016 -> 140107604672672
	140107092176832 [label="dec.resblocks.10.convs2.2.parametrizations.weight.original1
 (32, 32, 7)" fillcolor=lightblue]
	140107092176832 -> 140107182856016
	140107182856016 [label=AccumulateGrad]
	140107182855200 -> 140107604672672
	140107092176752 [label="dec.resblocks.10.convs2.2.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092176752 -> 140107182855200
	140107182855200 [label=AccumulateGrad]
	140107604673776 -> 140106869112496
	140107092176512 [label="dec.resblocks.10.convs2.2.bias
 (32)" fillcolor=lightblue]
	140107092176512 -> 140107604673776
	140107604673776 [label=AccumulateGrad]
	140107604673056 -> 140106869112400
	140106869112256 -> 140106869112208
	140106869112256 [label="AddBackward0
------------
alpha: 1"]
	140107604672576 -> 140106869112256
	140107604672576 -> 140106788254352 [dir=none]
	140106788254352 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107604672576 -> 140106788241792 [dir=none]
	140106788241792 [label="weight
 (32, 32, 11)" fillcolor=orange]
	140107604672576 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182854576 -> 140107604672576
	140107182854576 -> 140106788254192 [dir=none]
	140106788254192 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182854576 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182856784 -> 140107182854576
	140107182856784 -> 140106788254112 [dir=none]
	140106788254112 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182856784 -> 140106788253952 [dir=none]
	140106788253952 [label="weight
 (32, 32, 11)" fillcolor=orange]
	140107182856784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (25,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182856880 -> 140107182856784
	140107182856880 -> 140106788254032 [dir=none]
	140106788254032 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182856880 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869112352 -> 140107182856880
	140106869112352 [label="AddBackward0
------------
alpha: 1"]
	140107182857120 -> 140106869112352
	140107182857120 -> 140106788253552 [dir=none]
	140106788253552 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182857120 -> 140106788253872 [dir=none]
	140106788253872 [label="weight
 (32, 32, 11)" fillcolor=orange]
	140107182857120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182857264 -> 140107182857120
	140107182857264 -> 140106788253472 [dir=none]
	140106788253472 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182857264 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182857456 -> 140107182857264
	140107182857456 -> 140106788253232 [dir=none]
	140106788253232 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182857456 -> 140106788247552 [dir=none]
	140106788247552 [label="weight
 (32, 32, 11)" fillcolor=orange]
	140107182857456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182857552 -> 140107182857456
	140107182857552 -> 140106788252192 [dir=none]
	140106788252192 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182857552 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182856976 -> 140107182857552
	140107182856976 [label="AddBackward0
------------
alpha: 1"]
	140107182857792 -> 140107182856976
	140107182857792 -> 140106788252112 [dir=none]
	140106788252112 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182857792 -> 140106788252832 [dir=none]
	140106788252832 [label="weight
 (32, 32, 11)" fillcolor=orange]
	140107182857792 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182857888 -> 140107182857792
	140107182857888 -> 140106788241952 [dir=none]
	140106788241952 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182857888 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182858080 -> 140107182857888
	140107182858080 -> 140106788246912 [dir=none]
	140106788246912 [label="input
 (7, 32, 12800)" fillcolor=orange]
	140107182858080 -> 140106788249392 [dir=none]
	140106788249392 [label="weight
 (32, 32, 11)" fillcolor=orange]
	140107182858080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182858176 -> 140107182858080
	140107182858176 -> 140106788244272 [dir=none]
	140106788244272 [label="self
 (7, 32, 12800)" fillcolor=orange]
	140107182858176 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107604673728 -> 140107182858176
	140107182858128 -> 140107182858080
	140107182858128 -> 140107092178672 [dir=none]
	140107092178672 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182858128 -> 140106745662816 [dir=none]
	140106745662816 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182858128 -> 140107092178752 [dir=none]
	140107092178752 [label="v
 (32, 32, 11)" fillcolor=orange]
	140107182858128 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182858320 -> 140107182858128
	140107092178752 [label="dec.resblocks.11.convs1.0.parametrizations.weight.original1
 (32, 32, 11)" fillcolor=lightblue]
	140107092178752 -> 140107182858320
	140107182858320 [label=AccumulateGrad]
	140107182858368 -> 140107182858128
	140107092178672 [label="dec.resblocks.11.convs1.0.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092178672 -> 140107182858368
	140107182858368 [label=AccumulateGrad]
	140107182857984 -> 140107182858080
	140107092177792 [label="dec.resblocks.11.convs1.0.bias
 (32)" fillcolor=lightblue]
	140107092177792 -> 140107182857984
	140107182857984 [label=AccumulateGrad]
	140107182857840 -> 140107182857792
	140107182857840 -> 140107086973264 [dir=none]
	140107086973264 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182857840 -> 140106725087072 [dir=none]
	140106725087072 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182857840 -> 140107086973424 [dir=none]
	140107086973424 [label="v
 (32, 32, 11)" fillcolor=orange]
	140107182857840 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182858512 -> 140107182857840
	140107086973424 [label="dec.resblocks.11.convs2.0.parametrizations.weight.original1
 (32, 32, 11)" fillcolor=lightblue]
	140107086973424 -> 140107182858512
	140107182858512 [label=AccumulateGrad]
	140107182858272 -> 140107182857840
	140107086973264 [label="dec.resblocks.11.convs2.0.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107086973264 -> 140107182858272
	140107182858272 [label=AccumulateGrad]
	140107182857696 -> 140107182857792
	140107086972784 [label="dec.resblocks.11.convs2.0.bias
 (32)" fillcolor=lightblue]
	140107086972784 -> 140107182857696
	140107182857696 [label=AccumulateGrad]
	140107604673728 -> 140107182856976
	140107182857504 -> 140107182857456
	140107182857504 -> 140107092180272 [dir=none]
	140107092180272 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182857504 -> 140106725085072 [dir=none]
	140106725085072 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182857504 -> 140107092180352 [dir=none]
	140107092180352 [label="v
 (32, 32, 11)" fillcolor=orange]
	140107182857504 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182857936 -> 140107182857504
	140107092180352 [label="dec.resblocks.11.convs1.1.parametrizations.weight.original1
 (32, 32, 11)" fillcolor=lightblue]
	140107092180352 -> 140107182857936
	140107182857936 [label=AccumulateGrad]
	140107182857648 -> 140107182857504
	140107092180272 [label="dec.resblocks.11.convs1.1.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107092180272 -> 140107182857648
	140107182857648 [label=AccumulateGrad]
	140107182857360 -> 140107182857456
	140107092180032 [label="dec.resblocks.11.convs1.1.bias
 (32)" fillcolor=lightblue]
	140107092180032 -> 140107182857360
	140107182857360 [label=AccumulateGrad]
	140107182857216 -> 140107182857120
	140107182857216 -> 140107086975744 [dir=none]
	140107086975744 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182857216 -> 140106725086352 [dir=none]
	140106725086352 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182857216 -> 140107086976144 [dir=none]
	140107086976144 [label="v
 (32, 32, 11)" fillcolor=orange]
	140107182857216 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182858464 -> 140107182857216
	140107086976144 [label="dec.resblocks.11.convs2.1.parametrizations.weight.original1
 (32, 32, 11)" fillcolor=lightblue]
	140107086976144 -> 140107182858464
	140107182858464 [label=AccumulateGrad]
	140107182857600 -> 140107182857216
	140107086975744 [label="dec.resblocks.11.convs2.1.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107086975744 -> 140107182857600
	140107182857600 [label=AccumulateGrad]
	140107182857168 -> 140107182857120
	140107086975424 [label="dec.resblocks.11.convs2.1.bias
 (32)" fillcolor=lightblue]
	140107086975424 -> 140107182857168
	140107182857168 [label=AccumulateGrad]
	140107182856976 -> 140106869112352
	140107182856832 -> 140107182856784
	140107182856832 -> 140107086971744 [dir=none]
	140107086971744 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182856832 -> 140106725087312 [dir=none]
	140106725087312 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182856832 -> 140107086971984 [dir=none]
	140107086971984 [label="v
 (32, 32, 11)" fillcolor=orange]
	140107182856832 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182857312 -> 140107182856832
	140107086971984 [label="dec.resblocks.11.convs1.2.parametrizations.weight.original1
 (32, 32, 11)" fillcolor=lightblue]
	140107086971984 -> 140107182857312
	140107182857312 [label=AccumulateGrad]
	140107182857024 -> 140107182856832
	140107086971744 [label="dec.resblocks.11.convs1.2.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107086971744 -> 140107182857024
	140107182857024 [label=AccumulateGrad]
	140107182856640 -> 140107182856784
	140107086971584 [label="dec.resblocks.11.convs1.2.bias
 (32)" fillcolor=lightblue]
	140107086971584 -> 140107182856640
	140107182856640 [label=AccumulateGrad]
	140107182856304 -> 140107604672576
	140107182856304 -> 140107086977424 [dir=none]
	140107086977424 [label="g
 (32, 1, 1)" fillcolor=orange]
	140107182856304 -> 140106725086672 [dir=none]
	140106725086672 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140107182856304 -> 140107086977504 [dir=none]
	140107086977504 [label="v
 (32, 32, 11)" fillcolor=orange]
	140107182856304 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182858416 -> 140107182856304
	140107086977504 [label="dec.resblocks.11.convs2.2.parametrizations.weight.original1
 (32, 32, 11)" fillcolor=lightblue]
	140107086977504 -> 140107182858416
	140107182858416 [label=AccumulateGrad]
	140107182856928 -> 140107182856304
	140107086977424 [label="dec.resblocks.11.convs2.2.parametrizations.weight.original0
 (32, 1, 1)" fillcolor=lightblue]
	140107086977424 -> 140107182856928
	140107182856928 [label=AccumulateGrad]
	140107182855344 -> 140107604672576
	140107086976944 [label="dec.resblocks.11.convs2.2.bias
 (32)" fillcolor=lightblue]
	140107086976944 -> 140107182855344
	140107182855344 [label=AccumulateGrad]
	140106869112352 -> 140106869112256
	140106869111680 -> 140106869110672
	140106869111680 -> 140107125193232 [dir=none]
	140107125193232 [label="g
 (32, 1, 1)" fillcolor=orange]
	140106869111680 -> 140106725087392 [dir=none]
	140106725087392 [label="result1
 (32, 1, 1)" fillcolor=orange]
	140106869111680 -> 140107125194112 [dir=none]
	140107125194112 [label="v
 (32, 16, 2)" fillcolor=orange]
	140106869111680 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869111968 -> 140106869111680
	140107125194112 [label="dec.ups.4.weight_v
 (32, 16, 2)" fillcolor=lightblue]
	140107125194112 -> 140106869111968
	140106869111968 [label=AccumulateGrad]
	140106869112112 -> 140106869111680
	140107125193232 [label="dec.ups.4.weight_g
 (32, 1, 1)" fillcolor=lightblue]
	140107125193232 -> 140106869112112
	140106869112112 [label=AccumulateGrad]
	140106869111728 -> 140106869110672
	140107125193552 [label="dec.ups.4.bias
 (16)" fillcolor=lightblue]
	140107125193552 -> 140106869111728
	140106869111728 [label=AccumulateGrad]
	140106869111488 -> 140106869111200
	140106869111488 -> 140107086978624 [dir=none]
	140107086978624 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869111488 -> 140106725087632 [dir=none]
	140106725087632 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869111488 -> 140107086978784 [dir=none]
	140107086978784 [label="v
 (16, 16, 3)" fillcolor=orange]
	140106869111488 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869112016 -> 140106869111488
	140107086978784 [label="dec.resblocks.12.convs1.0.parametrizations.weight.original1
 (16, 16, 3)" fillcolor=lightblue]
	140107086978784 -> 140106869112016
	140106869112016 [label=AccumulateGrad]
	140106869111920 -> 140106869111488
	140107086978624 [label="dec.resblocks.12.convs1.0.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086978624 -> 140106869111920
	140106869111920 [label=AccumulateGrad]
	140106869111104 -> 140106869111200
	140107086978144 [label="dec.resblocks.12.convs1.0.bias
 (16)" fillcolor=lightblue]
	140107086978144 -> 140106869111104
	140106869111104 [label=AccumulateGrad]
	140106869110912 -> 140106869110816
	140106869110912 -> 140107086981504 [dir=none]
	140107086981504 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869110912 -> 140106725087952 [dir=none]
	140106725087952 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869110912 -> 140107086981584 [dir=none]
	140107086981584 [label="v
 (16, 16, 3)" fillcolor=orange]
	140106869110912 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869111776 -> 140106869110912
	140107086981584 [label="dec.resblocks.12.convs2.0.parametrizations.weight.original1
 (16, 16, 3)" fillcolor=lightblue]
	140107086981584 -> 140106869111776
	140106869111776 [label=AccumulateGrad]
	140106869111632 -> 140106869110912
	140107086981504 [label="dec.resblocks.12.convs2.0.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086981504 -> 140106869111632
	140106869111632 [label=AccumulateGrad]
	140106869110864 -> 140106869110816
	140107086981344 [label="dec.resblocks.12.convs2.0.bias
 (16)" fillcolor=lightblue]
	140107086981344 -> 140106869110864
	140106869110864 [label=AccumulateGrad]
	140106869110672 -> 140106869109616
	140106869110480 -> 140106869110432
	140106869110480 -> 140107086979824 [dir=none]
	140107086979824 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869110480 -> 140106725087232 [dir=none]
	140106725087232 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869110480 -> 140107086980064 [dir=none]
	140107086980064 [label="v
 (16, 16, 3)" fillcolor=orange]
	140106869110480 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869111056 -> 140106869110480
	140107086980064 [label="dec.resblocks.12.convs1.1.parametrizations.weight.original1
 (16, 16, 3)" fillcolor=lightblue]
	140107086980064 -> 140106869111056
	140106869111056 [label=AccumulateGrad]
	140106869110720 -> 140106869110480
	140107086979824 [label="dec.resblocks.12.convs1.1.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086979824 -> 140106869110720
	140106869110720 [label=AccumulateGrad]
	140106869110336 -> 140106869110432
	140107086979584 [label="dec.resblocks.12.convs1.1.bias
 (16)" fillcolor=lightblue]
	140107086979584 -> 140106869110336
	140106869110336 [label=AccumulateGrad]
	140106869110144 -> 140106869110048
	140106869110144 -> 140107086982224 [dir=none]
	140107086982224 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869110144 -> 140106725088272 [dir=none]
	140106725088272 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869110144 -> 140107086982464 [dir=none]
	140107086982464 [label="v
 (16, 16, 3)" fillcolor=orange]
	140106869110144 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869111152 -> 140106869110144
	140107086982464 [label="dec.resblocks.12.convs2.1.parametrizations.weight.original1
 (16, 16, 3)" fillcolor=lightblue]
	140107086982464 -> 140106869111152
	140106869111152 [label=AccumulateGrad]
	140106869110624 -> 140106869110144
	140107086982224 [label="dec.resblocks.12.convs2.1.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086982224 -> 140106869110624
	140106869110624 [label=AccumulateGrad]
	140106869110096 -> 140106869110048
	140107086981984 [label="dec.resblocks.12.convs2.1.bias
 (16)" fillcolor=lightblue]
	140107086981984 -> 140106869110096
	140106869110096 [label=AccumulateGrad]
	140106869109616 -> 140106869096752
	140106869109328 -> 140106869109232
	140106869109328 -> 140107086980864 [dir=none]
	140107086980864 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869109328 -> 140106725089872 [dir=none]
	140106725089872 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869109328 -> 140107086980944 [dir=none]
	140107086980944 [label="v
 (16, 16, 3)" fillcolor=orange]
	140106869109328 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869110288 -> 140106869109328
	140107086980944 [label="dec.resblocks.12.convs1.2.parametrizations.weight.original1
 (16, 16, 3)" fillcolor=lightblue]
	140107086980944 -> 140106869110288
	140106869110288 [label=AccumulateGrad]
	140106869104864 -> 140106869109328
	140107086980864 [label="dec.resblocks.12.convs1.2.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086980864 -> 140106869104864
	140106869104864 [label=AccumulateGrad]
	140106869104960 -> 140106869109232
	140107086980704 [label="dec.resblocks.12.convs1.2.bias
 (16)" fillcolor=lightblue]
	140107086980704 -> 140106869104960
	140106869104960 [label=AccumulateGrad]
	140106869108944 -> 140106869107984
	140106869108944 -> 140107086983424 [dir=none]
	140107086983424 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869108944 -> 140106725090112 [dir=none]
	140106725090112 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869108944 -> 140107086983664 [dir=none]
	140107086983664 [label="v
 (16, 16, 3)" fillcolor=orange]
	140106869108944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869110384 -> 140106869108944
	140107086983664 [label="dec.resblocks.12.convs2.2.parametrizations.weight.original1
 (16, 16, 3)" fillcolor=lightblue]
	140107086983664 -> 140106869110384
	140106869110384 [label=AccumulateGrad]
	140106869105152 -> 140106869108944
	140107086983424 [label="dec.resblocks.12.convs2.2.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086983424 -> 140106869105152
	140106869105152 [label=AccumulateGrad]
	140106869109184 -> 140106869107984
	140107086983104 [label="dec.resblocks.12.convs2.2.bias
 (16)" fillcolor=lightblue]
	140107086983104 -> 140106869109184
	140106869109184 [label=AccumulateGrad]
	140106869096752 -> 140106869109040
	140106869108464 -> 140106869105296
	140106869108464 [label="AddBackward0
------------
alpha: 1"]
	140106869109520 -> 140106869108464
	140106869109520 -> 140106784283584 [dir=none]
	140106784283584 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869109520 -> 140107103437344 [dir=none]
	140107103437344 [label="weight
 (16, 16, 7)" fillcolor=orange]
	140106869109520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106869109952 -> 140106869109520
	140106869109952 -> 140106784283424 [dir=none]
	140106784283424 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106869109952 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182853952 -> 140106869109952
	140107182853952 -> 140106784283024 [dir=none]
	140106784283024 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182853952 -> 140106784282704 [dir=none]
	140106784282704 [label="weight
 (16, 16, 7)" fillcolor=orange]
	140107182853952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182858032 -> 140107182853952
	140107182858032 -> 140106784282864 [dir=none]
	140106784282864 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182858032 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869103904 -> 140107182858032
	140106869103904 [label="AddBackward0
------------
alpha: 1"]
	140107182858752 -> 140106869103904
	140107182858752 -> 140106784282224 [dir=none]
	140106784282224 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182858752 -> 140106784282304 [dir=none]
	140106784282304 [label="weight
 (16, 16, 7)" fillcolor=orange]
	140107182858752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182858896 -> 140107182858752
	140107182858896 -> 140106784281264 [dir=none]
	140106784281264 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182858896 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182859088 -> 140107182858896
	140107182859088 -> 140106784282064 [dir=none]
	140106784282064 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182859088 -> 140107099112128 [dir=none]
	140107099112128 [label="weight
 (16, 16, 7)" fillcolor=orange]
	140107182859088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (9,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182859184 -> 140107182859088
	140107182859184 -> 140106784281824 [dir=none]
	140106784281824 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182859184 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182858608 -> 140107182859184
	140107182858608 [label="AddBackward0
------------
alpha: 1"]
	140107182859424 -> 140107182858608
	140107182859424 -> 140106784280864 [dir=none]
	140106784280864 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182859424 -> 140106784281104 [dir=none]
	140106784281104 [label="weight
 (16, 16, 7)" fillcolor=orange]
	140107182859424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182859520 -> 140107182859424
	140107182859520 -> 140106784280384 [dir=none]
	140106784280384 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182859520 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182859760 -> 140107182859520
	140107182859760 -> 140106784279184 [dir=none]
	140106784279184 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182859760 -> 140106784279424 [dir=none]
	140106784279424 [label="weight
 (16, 16, 7)" fillcolor=orange]
	140107182859760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (3,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182859856 -> 140107182859760
	140107182859856 -> 140107125193312 [dir=none]
	140107125193312 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182859856 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869110672 -> 140107182859856
	140107182859808 -> 140107182859760
	140107182859808 -> 140107086984784 [dir=none]
	140107086984784 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182859808 -> 140106725088992 [dir=none]
	140106725088992 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182859808 -> 140107086985264 [dir=none]
	140107086985264 [label="v
 (16, 16, 7)" fillcolor=orange]
	140107182859808 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182859952 -> 140107182859808
	140107086985264 [label="dec.resblocks.13.convs1.0.parametrizations.weight.original1
 (16, 16, 7)" fillcolor=lightblue]
	140107086985264 -> 140107182859952
	140107182859952 [label=AccumulateGrad]
	140107182860000 -> 140107182859808
	140107086984784 [label="dec.resblocks.13.convs1.0.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086984784 -> 140107182860000
	140107182860000 [label=AccumulateGrad]
	140107182859664 -> 140107182859760
	140107086984624 [label="dec.resblocks.13.convs1.0.bias
 (16)" fillcolor=lightblue]
	140107086984624 -> 140107182859664
	140107182859664 [label=AccumulateGrad]
	140107182859472 -> 140107182859424
	140107182859472 -> 140107082861680 [dir=none]
	140107082861680 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182859472 -> 140106725090272 [dir=none]
	140106725090272 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182859472 -> 140107082862080 [dir=none]
	140107082862080 [label="v
 (16, 16, 7)" fillcolor=orange]
	140107182859472 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182860144 -> 140107182859472
	140107082862080 [label="dec.resblocks.13.convs2.0.parametrizations.weight.original1
 (16, 16, 7)" fillcolor=lightblue]
	140107082862080 -> 140107182860144
	140107182860144 [label=AccumulateGrad]
	140107182859904 -> 140107182859472
	140107082861680 [label="dec.resblocks.13.convs2.0.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082861680 -> 140107182859904
	140107182859904 [label=AccumulateGrad]
	140107182859328 -> 140107182859424
	140107082861520 [label="dec.resblocks.13.convs2.0.bias
 (16)" fillcolor=lightblue]
	140107082861520 -> 140107182859328
	140107182859328 [label=AccumulateGrad]
	140106869110672 -> 140107182858608
	140107182859136 -> 140107182859088
	140107182859136 -> 140107086987024 [dir=none]
	140107086987024 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182859136 -> 140106725090512 [dir=none]
	140106725090512 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182859136 -> 140107086987104 [dir=none]
	140107086987104 [label="v
 (16, 16, 7)" fillcolor=orange]
	140107182859136 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182859568 -> 140107182859136
	140107086987104 [label="dec.resblocks.13.convs1.1.parametrizations.weight.original1
 (16, 16, 7)" fillcolor=lightblue]
	140107086987104 -> 140107182859568
	140107182859568 [label=AccumulateGrad]
	140107182859280 -> 140107182859136
	140107086987024 [label="dec.resblocks.13.convs1.1.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107086987024 -> 140107182859280
	140107182859280 [label=AccumulateGrad]
	140107182858992 -> 140107182859088
	140107086986464 [label="dec.resblocks.13.convs1.1.bias
 (16)" fillcolor=lightblue]
	140107086986464 -> 140107182858992
	140107182858992 [label=AccumulateGrad]
	140107182858848 -> 140107182858752
	140107182858848 -> 140107082863600 [dir=none]
	140107082863600 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182858848 -> 140106725088832 [dir=none]
	140106725088832 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182858848 -> 140107082863680 [dir=none]
	140107082863680 [label="v
 (16, 16, 7)" fillcolor=orange]
	140107182858848 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182860096 -> 140107182858848
	140107082863680 [label="dec.resblocks.13.convs2.1.parametrizations.weight.original1
 (16, 16, 7)" fillcolor=lightblue]
	140107082863680 -> 140107182860096
	140107182860096 [label=AccumulateGrad]
	140107182859232 -> 140107182858848
	140107082863600 [label="dec.resblocks.13.convs2.1.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082863600 -> 140107182859232
	140107182859232 [label=AccumulateGrad]
	140107182858800 -> 140107182858752
	140107082863200 [label="dec.resblocks.13.convs2.1.bias
 (16)" fillcolor=lightblue]
	140107082863200 -> 140107182858800
	140107182858800 [label=AccumulateGrad]
	140107182858608 -> 140106869103904
	140107182857408 -> 140107182853952
	140107182857408 -> 140107082860240 [dir=none]
	140107082860240 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182857408 -> 140106725090432 [dir=none]
	140106725090432 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182857408 -> 140107082860320 [dir=none]
	140107082860320 [label="v
 (16, 16, 7)" fillcolor=orange]
	140107182857408 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182858944 -> 140107182857408
	140107082860320 [label="dec.resblocks.13.convs1.2.parametrizations.weight.original1
 (16, 16, 7)" fillcolor=lightblue]
	140107082860320 -> 140107182858944
	140107182858944 [label=AccumulateGrad]
	140107182858656 -> 140107182857408
	140107082860240 [label="dec.resblocks.13.convs1.2.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082860240 -> 140107182858656
	140107182858656 [label=AccumulateGrad]
	140107182857072 -> 140107182853952
	140107082859600 [label="dec.resblocks.13.convs1.2.bias
 (16)" fillcolor=lightblue]
	140107082859600 -> 140107182857072
	140107182857072 [label=AccumulateGrad]
	140107182855680 -> 140106869109520
	140107182855680 -> 140107082864960 [dir=none]
	140107082864960 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182855680 -> 140106725086432 [dir=none]
	140106725086432 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182855680 -> 140107082865040 [dir=none]
	140107082865040 [label="v
 (16, 16, 7)" fillcolor=orange]
	140107182855680 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182860048 -> 140107182855680
	140107082865040 [label="dec.resblocks.13.convs2.2.parametrizations.weight.original1
 (16, 16, 7)" fillcolor=lightblue]
	140107082865040 -> 140107182860048
	140107182860048 [label=AccumulateGrad]
	140107182858560 -> 140107182855680
	140107082864960 [label="dec.resblocks.13.convs2.2.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082864960 -> 140107182858560
	140107182858560 [label=AccumulateGrad]
	140107182858224 -> 140106869109520
	140107082864640 [label="dec.resblocks.13.convs2.2.bias
 (16)" fillcolor=lightblue]
	140107082864640 -> 140107182858224
	140107182858224 [label=AccumulateGrad]
	140106869103904 -> 140106869108464
	140106869108896 -> 140106869108512
	140106869108896 [label="AddBackward0
------------
alpha: 1"]
	140106869103232 -> 140106869108896
	140106869103232 -> 140106784277264 [dir=none]
	140106784277264 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140106869103232 -> 140106784277344 [dir=none]
	140106784277344 [label="weight
 (16, 16, 11)" fillcolor=orange]
	140106869103232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182857744 -> 140106869103232
	140107182857744 -> 140106784290784 [dir=none]
	140106784290784 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182857744 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182860240 -> 140107182857744
	140107182860240 -> 140106784276704 [dir=none]
	140106784276704 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182860240 -> 140106784291184 [dir=none]
	140106784291184 [label="weight
 (16, 16, 11)" fillcolor=orange]
	140107182860240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (5,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (25,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182860336 -> 140107182860240
	140107182860336 -> 140106784291584 [dir=none]
	140106784291584 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182860336 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869105200 -> 140107182860336
	140106869105200 [label="AddBackward0
------------
alpha: 1"]
	140107182860576 -> 140106869105200
	140107182860576 -> 140106784279584 [dir=none]
	140106784279584 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182860576 -> 140106784289744 [dir=none]
	140106784289744 [label="weight
 (16, 16, 11)" fillcolor=orange]
	140107182860576 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182860720 -> 140107182860576
	140107182860720 -> 140106784289024 [dir=none]
	140106784289024 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182860720 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182860912 -> 140107182860720
	140107182860912 -> 140106784288464 [dir=none]
	140106784288464 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182860912 -> 140106784286304 [dir=none]
	140106784286304 [label="weight
 (16, 16, 11)" fillcolor=orange]
	140107182860912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (3,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :          (15,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182861008 -> 140107182860912
	140107182861008 -> 140106784288144 [dir=none]
	140106784288144 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182861008 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182860432 -> 140107182861008
	140107182860432 [label="AddBackward0
------------
alpha: 1"]
	140107182861248 -> 140107182860432
	140107182861248 -> 140106784286224 [dir=none]
	140106784286224 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182861248 -> 140107125187072 [dir=none]
	140107125187072 [label="weight
 (16, 16, 11)" fillcolor=orange]
	140107182861248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182861344 -> 140107182861248
	140107182861344 -> 140106784285904 [dir=none]
	140106784285904 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182861344 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182861536 -> 140107182861344
	140107182861536 -> 140106788255632 [dir=none]
	140106788255632 [label="input
 (7, 16, 25600)" fillcolor=orange]
	140107182861536 -> 140106784285104 [dir=none]
	140106784285104 [label="weight
 (16, 16, 11)" fillcolor=orange]
	140107182861536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (16,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (5,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107182861632 -> 140107182861536
	140107182861632 -> 140107125193312 [dir=none]
	140107125193312 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140107182861632 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869110672 -> 140107182861632
	140107182861584 -> 140107182861536
	140107182861584 -> 140107082865920 [dir=none]
	140107082865920 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182861584 -> 140106725088912 [dir=none]
	140106725088912 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182861584 -> 140107082866080 [dir=none]
	140107082866080 [label="v
 (16, 16, 11)" fillcolor=orange]
	140107182861584 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182861776 -> 140107182861584
	140107082866080 [label="dec.resblocks.14.convs1.0.parametrizations.weight.original1
 (16, 16, 11)" fillcolor=lightblue]
	140107082866080 -> 140107182861776
	140107182861776 [label=AccumulateGrad]
	140107182861824 -> 140107182861584
	140107082865920 [label="dec.resblocks.14.convs1.0.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082865920 -> 140107182861824
	140107182861824 [label=AccumulateGrad]
	140107182861440 -> 140107182861536
	140107082865680 [label="dec.resblocks.14.convs1.0.bias
 (16)" fillcolor=lightblue]
	140107082865680 -> 140107182861440
	140107182861440 [label=AccumulateGrad]
	140107182861296 -> 140107182861248
	140107182861296 -> 140107082869440 [dir=none]
	140107082869440 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182861296 -> 140106725090592 [dir=none]
	140106725090592 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182861296 -> 140107082869520 [dir=none]
	140107082869520 [label="v
 (16, 16, 11)" fillcolor=orange]
	140107182861296 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182861872 -> 140107182861296
	140107082869520 [label="dec.resblocks.14.convs2.0.parametrizations.weight.original1
 (16, 16, 11)" fillcolor=lightblue]
	140107082869520 -> 140107182861872
	140107182861872 [label=AccumulateGrad]
	140107182861680 -> 140107182861296
	140107082869440 [label="dec.resblocks.14.convs2.0.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082869440 -> 140107182861680
	140107182861680 [label=AccumulateGrad]
	140107182861152 -> 140107182861248
	140107082869120 [label="dec.resblocks.14.convs2.0.bias
 (16)" fillcolor=lightblue]
	140107082869120 -> 140107182861152
	140107182861152 [label=AccumulateGrad]
	140106869110672 -> 140107182860432
	140107182860960 -> 140107182860912
	140107182860960 -> 140107082867440 [dir=none]
	140107082867440 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182860960 -> 140106725091392 [dir=none]
	140106725091392 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182860960 -> 140107082867520 [dir=none]
	140107082867520 [label="v
 (16, 16, 11)" fillcolor=orange]
	140107182860960 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182861392 -> 140107182860960
	140107082867520 [label="dec.resblocks.14.convs1.1.parametrizations.weight.original1
 (16, 16, 11)" fillcolor=lightblue]
	140107082867520 -> 140107182861392
	140107182861392 [label=AccumulateGrad]
	140107182861104 -> 140107182860960
	140107082867440 [label="dec.resblocks.14.convs1.1.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082867440 -> 140107182861104
	140107182861104 [label=AccumulateGrad]
	140107182860816 -> 140107182860912
	140107082867120 [label="dec.resblocks.14.convs1.1.bias
 (16)" fillcolor=lightblue]
	140107082867120 -> 140107182860816
	140107182860816 [label=AccumulateGrad]
	140107182860672 -> 140107182860576
	140107182860672 -> 140107082870240 [dir=none]
	140107082870240 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182860672 -> 140106725091472 [dir=none]
	140106725091472 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182860672 -> 140107082870400 [dir=none]
	140107082870400 [label="v
 (16, 16, 11)" fillcolor=orange]
	140107182860672 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182861728 -> 140107182860672
	140107082870400 [label="dec.resblocks.14.convs2.1.parametrizations.weight.original1
 (16, 16, 11)" fillcolor=lightblue]
	140107082870400 -> 140107182861728
	140107182861728 [label=AccumulateGrad]
	140107182861056 -> 140107182860672
	140107082870240 [label="dec.resblocks.14.convs2.1.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082870240 -> 140107182861056
	140107182861056 [label=AccumulateGrad]
	140107182860624 -> 140107182860576
	140107082870000 [label="dec.resblocks.14.convs2.1.bias
 (16)" fillcolor=lightblue]
	140107082870000 -> 140107182860624
	140107182860624 [label=AccumulateGrad]
	140107182860432 -> 140106869105200
	140107182860288 -> 140107182860240
	140107182860288 -> 140107082868400 [dir=none]
	140107082868400 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182860288 -> 140106725088432 [dir=none]
	140106725088432 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182860288 -> 140107082868480 [dir=none]
	140107082868480 [label="v
 (16, 16, 11)" fillcolor=orange]
	140107182860288 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182860768 -> 140107182860288
	140107082868480 [label="dec.resblocks.14.convs1.2.parametrizations.weight.original1
 (16, 16, 11)" fillcolor=lightblue]
	140107082868480 -> 140107182860768
	140107182860768 [label=AccumulateGrad]
	140107182860480 -> 140107182860288
	140107082868400 [label="dec.resblocks.14.convs1.2.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082868400 -> 140107182860480
	140107182860480 [label=AccumulateGrad]
	140107182859712 -> 140107182860240
	140107082868240 [label="dec.resblocks.14.convs1.2.bias
 (16)" fillcolor=lightblue]
	140107082868240 -> 140107182859712
	140107182859712 [label=AccumulateGrad]
	140107182859376 -> 140106869103232
	140107182859376 -> 140107082871840 [dir=none]
	140107082871840 [label="g
 (16, 1, 1)" fillcolor=orange]
	140107182859376 -> 140106725091072 [dir=none]
	140106725091072 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140107182859376 -> 140107082872080 [dir=none]
	140107082872080 [label="v
 (16, 16, 11)" fillcolor=orange]
	140107182859376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182859616 -> 140107182859376
	140107082872080 [label="dec.resblocks.14.convs2.2.parametrizations.weight.original1
 (16, 16, 11)" fillcolor=lightblue]
	140107082872080 -> 140107182859616
	140107182859616 [label=AccumulateGrad]
	140107182860384 -> 140107182859376
	140107082871840 [label="dec.resblocks.14.convs2.2.parametrizations.weight.original0
 (16, 1, 1)" fillcolor=lightblue]
	140107082871840 -> 140107182860384
	140107182860384 [label=AccumulateGrad]
	140107182858704 -> 140106869103232
	140107082871200 [label="dec.resblocks.14.convs2.2.bias
 (16)" fillcolor=lightblue]
	140107082871200 -> 140107182858704
	140107182858704 [label=AccumulateGrad]
	140106869105200 -> 140106869108896
	140106869108560 -> 140106869104912
	140107082872480 [label="dec.conv_post.weight
 (1, 16, 7)" fillcolor=lightblue]
	140107082872480 -> 140106869108560
	140106869108560 [label=AccumulateGrad]
	140106869107408 -> 140106869105008
	140106869107408 -> 140107003795456 [dir=none]
	140107003795456 [label="g
 (16, 1, 1)" fillcolor=orange]
	140106869107408 -> 140106725090992 [dir=none]
	140106725090992 [label="result1
 (16, 1, 1)" fillcolor=orange]
	140106869107408 -> 140107003795536 [dir=none]
	140107003795536 [label="v
 (16, 1, 15)" fillcolor=orange]
	140106869107408 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869101984 -> 140106869107408
	140107003795536 [label="
 (16, 1, 15)" fillcolor=lightblue]
	140107003795536 -> 140106869101984
	140106869101984 [label=AccumulateGrad]
	140106869104240 -> 140106869107408
	140107003795456 [label="
 (16, 1, 1)" fillcolor=lightblue]
	140107003795456 -> 140106869104240
	140106869104240 [label=AccumulateGrad]
	140106869107792 -> 140106869105008
	140107003795136 [label="
 (16)" fillcolor=lightblue]
	140107003795136 -> 140106869107792
	140106869107792 [label=AccumulateGrad]
	140106869106064 -> 140106869102608
	140106869106064 -> 140107003796656 [dir=none]
	140107003796656 [label="g
 (64, 1, 1)" fillcolor=orange]
	140106869106064 -> 140106725090832 [dir=none]
	140106725090832 [label="result1
 (64, 1, 1)" fillcolor=orange]
	140106869106064 -> 140107003796736 [dir=none]
	140107003796736 [label="v
 (64, 4, 41)" fillcolor=orange]
	140106869106064 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869101936 -> 140106869106064
	140107003796736 [label="
 (64, 4, 41)" fillcolor=lightblue]
	140107003796736 -> 140106869101936
	140106869101936 [label=AccumulateGrad]
	140106869106448 -> 140106869106064
	140107003796656 [label="
 (64, 1, 1)" fillcolor=lightblue]
	140107003796656 -> 140106869106448
	140106869106448 [label=AccumulateGrad]
	140106869107024 -> 140106869102608
	140107003795936 [label="
 (64)" fillcolor=lightblue]
	140107003795936 -> 140106869107024
	140106869107024 [label=AccumulateGrad]
	140106869107504 -> 140106869103040
	140106869107504 -> 140107003797856 [dir=none]
	140107003797856 [label="g
 (256, 1, 1)" fillcolor=orange]
	140106869107504 -> 140106725092192 [dir=none]
	140106725092192 [label="result1
 (256, 1, 1)" fillcolor=orange]
	140106869107504 -> 140107003797936 [dir=none]
	140107003797936 [label="v
 (256, 4, 41)" fillcolor=orange]
	140106869107504 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869104672 -> 140106869107504
	140107003797936 [label="
 (256, 4, 41)" fillcolor=lightblue]
	140107003797936 -> 140106869104672
	140106869104672 [label=AccumulateGrad]
	140106869107696 -> 140106869107504
	140107003797856 [label="
 (256, 1, 1)" fillcolor=lightblue]
	140107003797856 -> 140106869107696
	140106869107696 [label=AccumulateGrad]
	140106869103376 -> 140106869103040
	140107003797616 [label="
 (256)" fillcolor=lightblue]
	140107003797616 -> 140106869103376
	140106869103376 [label=AccumulateGrad]
	140106869107312 -> 140106869105824
	140106869107312 -> 140107003798976 [dir=none]
	140107003798976 [label="g
 (1024, 1, 1)" fillcolor=orange]
	140106869107312 -> 140106725091632 [dir=none]
	140106725091632 [label="result1
 (1024, 1, 1)" fillcolor=orange]
	140106869107312 -> 140107003799056 [dir=none]
	140107003799056 [label="v
 (1024, 4, 41)" fillcolor=orange]
	140106869107312 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869108080 -> 140106869107312
	140107003799056 [label="
 (1024, 4, 41)" fillcolor=lightblue]
	140107003799056 -> 140106869108080
	140106869108080 [label=AccumulateGrad]
	140106869107744 -> 140106869107312
	140107003798976 [label="
 (1024, 1, 1)" fillcolor=lightblue]
	140107003798976 -> 140106869107744
	140106869107744 [label=AccumulateGrad]
	140106869105056 -> 140106869105824
	140107003798816 [label="
 (1024)" fillcolor=lightblue]
	140107003798816 -> 140106869105056
	140106869105056 [label=AccumulateGrad]
	140106869106688 -> 140106869106208
	140106869106688 -> 140107003800816 [dir=none]
	140107003800816 [label="g
 (1024, 1, 1)" fillcolor=orange]
	140106869106688 -> 140106725090912 [dir=none]
	140106725090912 [label="result1
 (1024, 1, 1)" fillcolor=orange]
	140106869106688 -> 140107003800976 [dir=none]
	140107003800976 [label="v
 (1024, 4, 41)" fillcolor=orange]
	140106869106688 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869101888 -> 140106869106688
	140107003800976 [label="
 (1024, 4, 41)" fillcolor=lightblue]
	140107003800976 -> 140106869101888
	140106869101888 [label=AccumulateGrad]
	140106869106736 -> 140106869106688
	140107003800816 [label="
 (1024, 1, 1)" fillcolor=lightblue]
	140107003800816 -> 140106869106736
	140106869106736 [label=AccumulateGrad]
	140106869106880 -> 140106869106208
	140107003800576 [label="
 (1024)" fillcolor=lightblue]
	140107003800576 -> 140106869106880
	140106869106880 [label=AccumulateGrad]
	140106869102464 -> 140106869106832
	140106869102464 -> 140107003803856 [dir=none]
	140107003803856 [label="g
 (1024, 1, 1)" fillcolor=orange]
	140106869102464 -> 140106725089552 [dir=none]
	140106725089552 [label="result1
 (1024, 1, 1)" fillcolor=orange]
	140106869102464 -> 140107003804256 [dir=none]
	140107003804256 [label="v
 (1024, 1024, 5)" fillcolor=orange]
	140106869102464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869106496 -> 140106869102464
	140107003804256 [label="
 (1024, 1024, 5)" fillcolor=lightblue]
	140107003804256 -> 140106869106496
	140106869106496 [label=AccumulateGrad]
	140106869107360 -> 140106869102464
	140107003803856 [label="
 (1024, 1, 1)" fillcolor=lightblue]
	140107003803856 -> 140106869107360
	140106869107360 [label=AccumulateGrad]
	140106869106256 -> 140106869106832
	140107003803136 [label="
 (1024)" fillcolor=lightblue]
	140107003803136 -> 140106869106256
	140106869106256 [label=AccumulateGrad]
	140106869105680 -> 140106869103712
	140106869105680 -> 140106876218608 [dir=none]
	140106876218608 [label="g
 (1, 1, 1)" fillcolor=orange]
	140106869105680 -> 140106725092032 [dir=none]
	140106725092032 [label="result1
 (1, 1, 1)" fillcolor=orange]
	140106869105680 -> 140106876221168 [dir=none]
	140106876221168 [label="v
 (1, 1024, 3)" fillcolor=orange]
	140106869105680 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869106304 -> 140106869105680
	140106876221168 [label="
 (1, 1024, 3)" fillcolor=lightblue]
	140106876221168 -> 140106869106304
	140106869106304 [label=AccumulateGrad]
	140106869106544 -> 140106869105680
	140106876218608 [label="
 (1, 1, 1)" fillcolor=lightblue]
	140106876218608 -> 140106869106544
	140106869106544 [label=AccumulateGrad]
	140106869103664 -> 140106869103712
	140107003804816 [label="
 (1)" fillcolor=lightblue]
	140107003804816 -> 140106869103664
	140106869103664 [label=AccumulateGrad]
	140106869105536 -> 140106869102944
	140106869105536 [label="MeanBackward0
------------------------
self_sym_numel:     2226
self_sym_sizes: (7, 318)"]
	140106869102848 -> 140106869105536
	140106869102848 -> 140106745665696 [dir=none]
	140106745665696 [label="self
 (7, 318)" fillcolor=orange]
	140106869102848 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106869106400 -> 140106869102848
	140106869106400 [label="RsubBackward1
-------------
alpha: 1"]
	140106869107264 -> 140106869106400
	140106869107264 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 1, 159, 2)"]
	140106869105728 -> 140106869107264
	140106869105728 -> 140106752172144 [dir=none]
	140106752172144 [label="input
 (7, 1024, 159, 2)" fillcolor=orange]
	140106869105728 -> 140106752165984 [dir=none]
	140106752165984 [label="weight
 (1, 1024, 3, 1)" fillcolor=orange]
	140106869105728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182859040 -> 140106869105728
	140107182859040 -> 140106752171904 [dir=none]
	140106752171904 [label="self
 (7, 1024, 159, 2)" fillcolor=orange]
	140107182859040 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182860864 -> 140107182859040
	140107182860864 -> 140106752171584 [dir=none]
	140106752171584 [label="input
 (7, 1024, 159, 2)" fillcolor=orange]
	140107182860864 -> 140106752171824 [dir=none]
	140106752171824 [label="weight
 (1024, 1024, 5, 1)" fillcolor=orange]
	140107182860864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182861920 -> 140107182860864
	140107182861920 -> 140106774831296 [dir=none]
	140106774831296 [label="self
 (7, 1024, 159, 2)" fillcolor=orange]
	140107182861920 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182862112 -> 140107182861920
	140107182862112 -> 140106752171104 [dir=none]
	140106752171104 [label="input
 (7, 512, 475, 2)" fillcolor=orange]
	140107182862112 -> 140106752171184 [dir=none]
	140106752171184 [label="weight
 (1024, 512, 5, 1)" fillcolor=orange]
	140107182862112 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182862208 -> 140107182862112
	140107182862208 -> 140106752170464 [dir=none]
	140106752170464 [label="self
 (7, 512, 475, 2)" fillcolor=orange]
	140107182862208 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182862400 -> 140107182862208
	140107182862400 -> 140106752170304 [dir=none]
	140106752170304 [label="input
 (7, 128, 1423, 2)" fillcolor=orange]
	140107182862400 -> 140106752170384 [dir=none]
	140106752170384 [label="weight
 (512, 128, 5, 1)" fillcolor=orange]
	140107182862400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182862496 -> 140107182862400
	140107182862496 -> 140106752170144 [dir=none]
	140106752170144 [label="self
 (7, 128, 1423, 2)" fillcolor=orange]
	140107182862496 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182862688 -> 140107182862496
	140107182862688 -> 140106752169824 [dir=none]
	140106752169824 [label="input
 (7, 32, 4267, 2)" fillcolor=orange]
	140107182862688 -> 140106752170064 [dir=none]
	140106752170064 [label="weight
 (128, 32, 5, 1)" fillcolor=orange]
	140107182862688 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182862784 -> 140107182862688
	140107182862784 -> 140106752169584 [dir=none]
	140106752169584 [label="self
 (7, 32, 4267, 2)" fillcolor=orange]
	140107182862784 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182862976 -> 140107182862784
	140107182862976 -> 140106752169744 [dir=none]
	140106752169744 [label="input
 (7, 1, 12800, 2)" fillcolor=orange]
	140107182862976 -> 140106752165024 [dir=none]
	140106752165024 [label="weight
 (32, 1, 5, 1)" fillcolor=orange]
	140107182862976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182863072 -> 140107182862976
	140107182863072 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 25600)"]
	140106869108320 -> 140107182863072
	140107182863024 -> 140107182862976
	140107182863024 -> 140106875941040 [dir=none]
	140106875941040 [label="g
 (32, 1, 1, 1)" fillcolor=orange]
	140107182863024 -> 140106725093392 [dir=none]
	140106725093392 [label="result1
 (32, 1, 1, 1)" fillcolor=orange]
	140107182863024 -> 140106875941760 [dir=none]
	140106875941760 [label="v
 (32, 1, 5, 1)" fillcolor=orange]
	140107182863024 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182863168 -> 140107182863024
	140106875941760 [label="
 (32, 1, 5, 1)" fillcolor=lightblue]
	140106875941760 -> 140107182863168
	140107182863168 [label=AccumulateGrad]
	140107182863216 -> 140107182863024
	140106875941040 [label="
 (32, 1, 1, 1)" fillcolor=lightblue]
	140106875941040 -> 140107182863216
	140107182863216 [label=AccumulateGrad]
	140107182862880 -> 140107182862976
	140106875938560 [label="
 (32)" fillcolor=lightblue]
	140106875938560 -> 140107182862880
	140107182862880 [label=AccumulateGrad]
	140107182862736 -> 140107182862688
	140107182862736 -> 140106871760080 [dir=none]
	140106871760080 [label="g
 (128, 1, 1, 1)" fillcolor=orange]
	140107182862736 -> 140106725093632 [dir=none]
	140106725093632 [label="result1
 (128, 1, 1, 1)" fillcolor=orange]
	140107182862736 -> 140106871762320 [dir=none]
	140106871762320 [label="v
 (128, 32, 5, 1)" fillcolor=orange]
	140107182862736 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182863360 -> 140107182862736
	140106871762320 [label="
 (128, 32, 5, 1)" fillcolor=lightblue]
	140106871762320 -> 140107182863360
	140107182863360 [label=AccumulateGrad]
	140107182863120 -> 140107182862736
	140106871760080 [label="
 (128, 1, 1, 1)" fillcolor=lightblue]
	140106871760080 -> 140107182863120
	140107182863120 [label=AccumulateGrad]
	140107182862592 -> 140107182862688
	140106871759040 [label="
 (128)" fillcolor=lightblue]
	140106871759040 -> 140107182862592
	140107182862592 [label=AccumulateGrad]
	140107182862448 -> 140107182862400
	140107182862448 -> 140106868551936 [dir=none]
	140106868551936 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	140107182862448 -> 140106725094272 [dir=none]
	140106725094272 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	140107182862448 -> 140106866475312 [dir=none]
	140106866475312 [label="v
 (512, 128, 5, 1)" fillcolor=orange]
	140107182862448 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182863264 -> 140107182862448
	140106866475312 [label="
 (512, 128, 5, 1)" fillcolor=lightblue]
	140106866475312 -> 140107182863264
	140107182863264 [label=AccumulateGrad]
	140107182862832 -> 140107182862448
	140106868551936 [label="
 (512, 1, 1, 1)" fillcolor=lightblue]
	140106868551936 -> 140107182862832
	140107182862832 [label=AccumulateGrad]
	140107182862304 -> 140107182862400
	140106868548896 [label="
 (512)" fillcolor=lightblue]
	140106868548896 -> 140107182862304
	140107182862304 [label=AccumulateGrad]
	140107182862160 -> 140107182862112
	140107182862160 -> 140106863703872 [dir=none]
	140106863703872 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182862160 -> 140106725094512 [dir=none]
	140106725094512 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182862160 -> 140106863099968 [dir=none]
	140106863099968 [label="v
 (1024, 512, 5, 1)" fillcolor=orange]
	140107182862160 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182862928 -> 140107182862160
	140106863099968 [label="
 (1024, 512, 5, 1)" fillcolor=lightblue]
	140106863099968 -> 140107182862928
	140107182862928 [label=AccumulateGrad]
	140107182862544 -> 140107182862160
	140106863703872 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106863703872 -> 140107182862544
	140107182862544 [label=AccumulateGrad]
	140107182862016 -> 140107182862112
	140106863703712 [label="
 (1024)" fillcolor=lightblue]
	140106863703712 -> 140107182862016
	140107182862016 [label=AccumulateGrad]
	140107182861488 -> 140107182860864
	140107182861488 -> 140106863116208 [dir=none]
	140106863116208 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182861488 -> 140106725094592 [dir=none]
	140106725094592 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182861488 -> 140106861792288 [dir=none]
	140106861792288 [label="v
 (1024, 1024, 5, 1)" fillcolor=orange]
	140107182861488 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182862640 -> 140107182861488
	140106861792288 [label="
 (1024, 1024, 5, 1)" fillcolor=lightblue]
	140106861792288 -> 140107182862640
	140107182862640 [label=AccumulateGrad]
	140107182862256 -> 140107182861488
	140106863116208 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106863116208 -> 140107182862256
	140107182862256 [label=AccumulateGrad]
	140107182861200 -> 140107182860864
	140106863106288 [label="
 (1024)" fillcolor=lightblue]
	140106863106288 -> 140107182861200
	140107182861200 [label=AccumulateGrad]
	140107182852080 -> 140106869105728
	140107182852080 -> 140106856381104 [dir=none]
	140106856381104 [label="g
 (1, 1, 1, 1)" fillcolor=orange]
	140107182852080 -> 140106725093552 [dir=none]
	140106725093552 [label="result1
 (1, 1, 1, 1)" fillcolor=orange]
	140107182852080 -> 140106854220000 [dir=none]
	140106854220000 [label="v
 (1, 1024, 3, 1)" fillcolor=orange]
	140107182852080 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182862352 -> 140107182852080
	140106854220000 [label="
 (1, 1024, 3, 1)" fillcolor=lightblue]
	140106854220000 -> 140107182862352
	140107182862352 [label=AccumulateGrad]
	140107182861968 -> 140107182852080
	140106856381104 [label="
 (1, 1, 1, 1)" fillcolor=lightblue]
	140106856381104 -> 140107182861968
	140107182861968 [label=AccumulateGrad]
	140107182853616 -> 140106869105728
	140106856378784 [label="
 (1)" fillcolor=lightblue]
	140106856378784 -> 140107182853616
	140107182853616 [label=AccumulateGrad]
	140106869101792 -> 140106869104432
	140106869101792 [label="MeanBackward0
------------------------
self_sym_numel:     2226
self_sym_sizes: (7, 318)"]
	140107604677808 -> 140106869101792
	140107604677808 -> 140106745665936 [dir=none]
	140106745665936 [label="self
 (7, 318)" fillcolor=orange]
	140107604677808 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140107522420224 -> 140107604677808
	140107522420224 [label="RsubBackward1
-------------
alpha: 1"]
	140107522420896 -> 140107522420224
	140107522420896 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 1, 106, 3)"]
	140107522423104 -> 140107522420896
	140107522423104 -> 140106752169024 [dir=none]
	140106752169024 [label="input
 (7, 1024, 106, 3)" fillcolor=orange]
	140107522423104 -> 140106752169344 [dir=none]
	140106752169344 [label="weight
 (1, 1024, 3, 1)" fillcolor=orange]
	140107522423104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107522422960 -> 140107522423104
	140107522422960 -> 140106752168784 [dir=none]
	140106752168784 [label="self
 (7, 1024, 106, 3)" fillcolor=orange]
	140107522422960 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869105920 -> 140107522422960
	140106869105920 -> 140106752166544 [dir=none]
	140106752166544 [label="input
 (7, 1024, 106, 3)" fillcolor=orange]
	140106869105920 -> 140106752168704 [dir=none]
	140106752168704 [label="weight
 (1024, 1024, 5, 1)" fillcolor=orange]
	140106869105920 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106869111248 -> 140106869105920
	140106869111248 -> 140106752166384 [dir=none]
	140106752166384 [label="self
 (7, 1024, 106, 3)" fillcolor=orange]
	140106869111248 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106869106592 -> 140106869111248
	140106869106592 -> 140106752164144 [dir=none]
	140106752164144 [label="input
 (7, 512, 317, 3)" fillcolor=orange]
	140106869106592 -> 140106752166224 [dir=none]
	140106752166224 [label="weight
 (1024, 512, 5, 1)" fillcolor=orange]
	140106869106592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182860192 -> 140106869106592
	140107182860192 -> 140106752163984 [dir=none]
	140106752163984 [label="self
 (7, 512, 317, 3)" fillcolor=orange]
	140107182860192 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182862064 -> 140107182860192
	140107182862064 -> 140106752179344 [dir=none]
	140106752179344 [label="input
 (7, 128, 949, 3)" fillcolor=orange]
	140107182862064 -> 140106752164064 [dir=none]
	140106752164064 [label="weight
 (512, 128, 5, 1)" fillcolor=orange]
	140107182862064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182863456 -> 140107182862064
	140107182863456 -> 140106752179264 [dir=none]
	140106752179264 [label="self
 (7, 128, 949, 3)" fillcolor=orange]
	140107182863456 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182863648 -> 140107182863456
	140107182863648 -> 140106752178544 [dir=none]
	140106752178544 [label="input
 (7, 32, 2845, 3)" fillcolor=orange]
	140107182863648 -> 140106752178944 [dir=none]
	140106752178944 [label="weight
 (128, 32, 5, 1)" fillcolor=orange]
	140107182863648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182863744 -> 140107182863648
	140107182863744 -> 140106752175904 [dir=none]
	140106752175904 [label="self
 (7, 32, 2845, 3)" fillcolor=orange]
	140107182863744 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182863936 -> 140107182863744
	140107182863936 -> 140106752176224 [dir=none]
	140106752176224 [label="input
 (7, 1, 8534, 3)" fillcolor=orange]
	140107182863936 -> 140106752177824 [dir=none]
	140106752177824 [label="weight
 (32, 1, 5, 1)" fillcolor=orange]
	140107182863936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182864032 -> 140107182863936
	140107182864032 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 25602)"]
	140107182864224 -> 140107182864032
	140107182864224 -> 140106784278864 [dir=none]
	140106784278864 [label="self
 (7, 1, 25600)" fillcolor=orange]
	140107182864224 [label="ReflectionPad1DBackward0
------------------------
padding:         (0, 2)
self   : [saved tensor]"]
	140106869108320 -> 140107182864224
	140107182863984 -> 140107182863936
	140107182863984 -> 140106853837264 [dir=none]
	140106853837264 [label="g
 (32, 1, 1, 1)" fillcolor=orange]
	140107182863984 -> 140107071075840 [dir=none]
	140107071075840 [label="result1
 (32, 1, 1, 1)" fillcolor=orange]
	140107182863984 -> 140106853841824 [dir=none]
	140106853841824 [label="v
 (32, 1, 5, 1)" fillcolor=orange]
	140107182863984 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182864272 -> 140107182863984
	140106853841824 [label="
 (32, 1, 5, 1)" fillcolor=lightblue]
	140106853841824 -> 140107182864272
	140107182864272 [label=AccumulateGrad]
	140107182864320 -> 140107182863984
	140106853837264 [label="
 (32, 1, 1, 1)" fillcolor=lightblue]
	140106853837264 -> 140107182864320
	140107182864320 [label=AccumulateGrad]
	140107182863840 -> 140107182863936
	140106853836224 [label="
 (32)" fillcolor=lightblue]
	140106853836224 -> 140107182863840
	140107182863840 [label=AccumulateGrad]
	140107182863696 -> 140107182863648
	140107182863696 -> 140106850823408 [dir=none]
	140106850823408 [label="g
 (128, 1, 1, 1)" fillcolor=orange]
	140107182863696 -> 140106752176064 [dir=none]
	140106752176064 [label="result1
 (128, 1, 1, 1)" fillcolor=orange]
	140107182863696 -> 140106850824048 [dir=none]
	140106850824048 [label="v
 (128, 32, 5, 1)" fillcolor=orange]
	140107182863696 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182864368 -> 140107182863696
	140106850824048 [label="
 (128, 32, 5, 1)" fillcolor=lightblue]
	140106850824048 -> 140107182864368
	140107182864368 [label=AccumulateGrad]
	140107182864080 -> 140107182863696
	140106850823408 [label="
 (128, 1, 1, 1)" fillcolor=lightblue]
	140106850823408 -> 140107182864080
	140107182864080 [label=AccumulateGrad]
	140107182863552 -> 140107182863648
	140106850823248 [label="
 (128)" fillcolor=lightblue]
	140106850823248 -> 140107182863552
	140107182863552 [label=AccumulateGrad]
	140107182863312 -> 140107182862064
	140107182863312 -> 140106847931344 [dir=none]
	140106847931344 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	140107182863312 -> 140106752171264 [dir=none]
	140106752171264 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	140107182863312 -> 140106847936384 [dir=none]
	140106847936384 [label="v
 (512, 128, 5, 1)" fillcolor=orange]
	140107182863312 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182864176 -> 140107182863312
	140106847936384 [label="
 (512, 128, 5, 1)" fillcolor=lightblue]
	140106847936384 -> 140107182864176
	140107182864176 [label=AccumulateGrad]
	140107182863792 -> 140107182863312
	140106847931344 [label="
 (512, 1, 1, 1)" fillcolor=lightblue]
	140106847931344 -> 140107182863792
	140107182863792 [label=AccumulateGrad]
	140107182853136 -> 140107182862064
	140106847931104 [label="
 (512)" fillcolor=lightblue]
	140106847931104 -> 140107182853136
	140107182853136 [label=AccumulateGrad]
	140107182860528 -> 140106869106592
	140107182860528 -> 140106847136208 [dir=none]
	140106847136208 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182860528 -> 140106731502416 [dir=none]
	140106731502416 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182860528 -> 140106847138768 [dir=none]
	140106847138768 [label="v
 (1024, 512, 5, 1)" fillcolor=orange]
	140107182860528 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182863888 -> 140107182860528
	140106847138768 [label="
 (1024, 512, 5, 1)" fillcolor=lightblue]
	140106847138768 -> 140107182863888
	140107182863888 [label=AccumulateGrad]
	140107182863504 -> 140107182860528
	140106847136208 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106847136208 -> 140107182863504
	140107182863504 [label=AccumulateGrad]
	140107182854288 -> 140106869106592
	140106847135648 [label="
 (1024)" fillcolor=lightblue]
	140106847135648 -> 140107182854288
	140107182854288 [label=AccumulateGrad]
	140106869106352 -> 140106869105920
	140106869106352 -> 140106846600736 [dir=none]
	140106846600736 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140106869106352 -> 140106731492896 [dir=none]
	140106731492896 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140106869106352 -> 140106846601056 [dir=none]
	140106846601056 [label="v
 (1024, 1024, 5, 1)" fillcolor=orange]
	140106869106352 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869110768 -> 140106869106352
	140106846601056 [label="
 (1024, 1024, 5, 1)" fillcolor=lightblue]
	140106846601056 -> 140106869110768
	140106869110768 [label=AccumulateGrad]
	140107182863600 -> 140106869106352
	140106846600736 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106846600736 -> 140107182863600
	140107182863600 [label=AccumulateGrad]
	140106869105776 -> 140106869105920
	140106846600176 [label="
 (1024)" fillcolor=lightblue]
	140106846600176 -> 140106869105776
	140106869105776 [label=AccumulateGrad]
	140107522418880 -> 140107522423104
	140107522418880 -> 140106841515696 [dir=none]
	140106841515696 [label="g
 (1, 1, 1, 1)" fillcolor=orange]
	140107522418880 -> 140106731494816 [dir=none]
	140106731494816 [label="result1
 (1, 1, 1, 1)" fillcolor=orange]
	140107522418880 -> 140106841515856 [dir=none]
	140106841515856 [label="v
 (1, 1024, 3, 1)" fillcolor=orange]
	140107522418880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106869111296 -> 140107522418880
	140106841515856 [label="
 (1, 1024, 3, 1)" fillcolor=lightblue]
	140106841515856 -> 140106869111296
	140106869111296 [label=AccumulateGrad]
	140106869106640 -> 140107522418880
	140106841515696 [label="
 (1, 1, 1, 1)" fillcolor=lightblue]
	140106841515696 -> 140106869106640
	140106869106640 [label=AccumulateGrad]
	140106869105344 -> 140107522423104
	140106841513216 [label="
 (1)" fillcolor=lightblue]
	140106841513216 -> 140106869105344
	140106869105344 [label=AccumulateGrad]
	140106869103952 -> 140106869109472
	140106869103952 [label="MeanBackward0
------------------------
self_sym_numel:     2240
self_sym_sizes: (7, 320)"]
	140107522419600 -> 140106869103952
	140107522419600 -> 140106745666176 [dir=none]
	140106745666176 [label="self
 (7, 320)" fillcolor=orange]
	140107522419600 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140107522423200 -> 140107522419600
	140107522423200 [label="RsubBackward1
-------------
alpha: 1"]
	140106869105872 -> 140107522423200
	140106869105872 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 64, 5)"]
	140107182853088 -> 140106869105872
	140107182853088 -> 140106752176944 [dir=none]
	140106752176944 [label="input
 (7, 1024, 64, 5)" fillcolor=orange]
	140107182853088 -> 140106752172384 [dir=none]
	140106752172384 [label="weight
 (1, 1024, 3, 1)" fillcolor=orange]
	140107182853088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182864464 -> 140107182853088
	140107182864464 -> 140106752176864 [dir=none]
	140106752176864 [label="self
 (7, 1024, 64, 5)" fillcolor=orange]
	140107182864464 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182864656 -> 140107182864464
	140107182864656 -> 140106752176784 [dir=none]
	140106752176784 [label="input
 (7, 1024, 64, 5)" fillcolor=orange]
	140107182864656 -> 140106752177024 [dir=none]
	140106752177024 [label="weight
 (1024, 1024, 5, 1)" fillcolor=orange]
	140107182864656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182864752 -> 140107182864656
	140107182864752 -> 140106752176624 [dir=none]
	140106752176624 [label="self
 (7, 1024, 64, 5)" fillcolor=orange]
	140107182864752 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182864944 -> 140107182864752
	140107182864944 -> 140106752173744 [dir=none]
	140106752173744 [label="input
 (7, 512, 190, 5)" fillcolor=orange]
	140107182864944 -> 140106752174064 [dir=none]
	140106752174064 [label="weight
 (1024, 512, 5, 1)" fillcolor=orange]
	140107182864944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182865040 -> 140107182864944
	140107182865040 -> 140106752173504 [dir=none]
	140106752173504 [label="self
 (7, 512, 190, 5)" fillcolor=orange]
	140107182865040 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182865232 -> 140107182865040
	140107182865232 -> 140106752173344 [dir=none]
	140106752173344 [label="input
 (7, 128, 569, 5)" fillcolor=orange]
	140107182865232 -> 140106752173984 [dir=none]
	140106752173984 [label="weight
 (512, 128, 5, 1)" fillcolor=orange]
	140107182865232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182865328 -> 140107182865232
	140107182865328 -> 140106752173184 [dir=none]
	140106752173184 [label="self
 (7, 128, 569, 5)" fillcolor=orange]
	140107182865328 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182865520 -> 140107182865328
	140107182865520 -> 140106752164784 [dir=none]
	140106752164784 [label="input
 (7, 32, 1707, 5)" fillcolor=orange]
	140107182865520 -> 140106752172864 [dir=none]
	140106752172864 [label="weight
 (128, 32, 5, 1)" fillcolor=orange]
	140107182865520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182865616 -> 140107182865520
	140107182865616 -> 140106752167184 [dir=none]
	140106752167184 [label="self
 (7, 32, 1707, 5)" fillcolor=orange]
	140107182865616 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182865808 -> 140107182865616
	140107182865808 -> 140106752175664 [dir=none]
	140106752175664 [label="input
 (7, 1, 5120, 5)" fillcolor=orange]
	140107182865808 -> 140107071075440 [dir=none]
	140107071075440 [label="weight
 (32, 1, 5, 1)" fillcolor=orange]
	140107182865808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182865904 -> 140107182865808
	140107182865904 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 25600)"]
	140106869108320 -> 140107182865904
	140107182865856 -> 140107182865808
	140107182865856 -> 140106838053392 [dir=none]
	140106838053392 [label="g
 (32, 1, 1, 1)" fillcolor=orange]
	140107182865856 -> 140106725095872 [dir=none]
	140106725095872 [label="result1
 (32, 1, 1, 1)" fillcolor=orange]
	140107182865856 -> 140106838056832 [dir=none]
	140106838056832 [label="v
 (32, 1, 5, 1)" fillcolor=orange]
	140107182865856 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182866000 -> 140107182865856
	140106838056832 [label="
 (32, 1, 5, 1)" fillcolor=lightblue]
	140106838056832 -> 140107182866000
	140107182866000 [label=AccumulateGrad]
	140107182866048 -> 140107182865856
	140106838053392 [label="
 (32, 1, 1, 1)" fillcolor=lightblue]
	140106838053392 -> 140107182866048
	140107182866048 [label=AccumulateGrad]
	140107182865712 -> 140107182865808
	140106840324544 [label="
 (32)" fillcolor=lightblue]
	140106840324544 -> 140107182865712
	140107182865712 [label=AccumulateGrad]
	140107182865568 -> 140107182865520
	140107182865568 -> 140106834986944 [dir=none]
	140106834986944 [label="g
 (128, 1, 1, 1)" fillcolor=orange]
	140107182865568 -> 140106725094432 [dir=none]
	140106725094432 [label="result1
 (128, 1, 1, 1)" fillcolor=orange]
	140107182865568 -> 140106834988144 [dir=none]
	140106834988144 [label="v
 (128, 32, 5, 1)" fillcolor=orange]
	140107182865568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182866192 -> 140107182865568
	140106834988144 [label="
 (128, 32, 5, 1)" fillcolor=lightblue]
	140106834988144 -> 140107182866192
	140107182866192 [label=AccumulateGrad]
	140107182865952 -> 140107182865568
	140106834986944 [label="
 (128, 1, 1, 1)" fillcolor=lightblue]
	140106834986944 -> 140107182865952
	140107182865952 [label=AccumulateGrad]
	140107182865424 -> 140107182865520
	140106834987504 [label="
 (128)" fillcolor=lightblue]
	140106834987504 -> 140107182865424
	140107182865424 [label=AccumulateGrad]
	140107182865280 -> 140107182865232
	140107182865280 -> 140106832467648 [dir=none]
	140106832467648 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	140107182865280 -> 140106725093872 [dir=none]
	140106725093872 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	140107182865280 -> 140106832469408 [dir=none]
	140106832469408 [label="v
 (512, 128, 5, 1)" fillcolor=orange]
	140107182865280 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182866096 -> 140107182865280
	140106832469408 [label="
 (512, 128, 5, 1)" fillcolor=lightblue]
	140106832469408 -> 140107182866096
	140107182866096 [label=AccumulateGrad]
	140107182865664 -> 140107182865280
	140106832467648 [label="
 (512, 1, 1, 1)" fillcolor=lightblue]
	140106832467648 -> 140107182865664
	140107182865664 [label=AccumulateGrad]
	140107182865136 -> 140107182865232
	140106832463248 [label="
 (512)" fillcolor=lightblue]
	140106832463248 -> 140107182865136
	140107182865136 [label=AccumulateGrad]
	140107182864992 -> 140107182864944
	140107182864992 -> 140106827086832 [dir=none]
	140106827086832 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182864992 -> 140106725092432 [dir=none]
	140106725092432 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182864992 -> 140106824991344 [dir=none]
	140106824991344 [label="v
 (1024, 512, 5, 1)" fillcolor=orange]
	140107182864992 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182865760 -> 140107182864992
	140106824991344 [label="
 (1024, 512, 5, 1)" fillcolor=lightblue]
	140106824991344 -> 140107182865760
	140107182865760 [label=AccumulateGrad]
	140107182865376 -> 140107182864992
	140106827086832 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106827086832 -> 140107182865376
	140107182865376 [label=AccumulateGrad]
	140107182864848 -> 140107182864944
	140106827080512 [label="
 (1024)" fillcolor=lightblue]
	140106827080512 -> 140107182864848
	140107182864848 [label=AccumulateGrad]
	140107182864704 -> 140107182864656
	140107182864704 -> 140106819321056 [dir=none]
	140106819321056 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182864704 -> 140106725096112 [dir=none]
	140106725096112 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140107182864704 -> 140106819321616 [dir=none]
	140106819321616 [label="v
 (1024, 1024, 5, 1)" fillcolor=orange]
	140107182864704 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182865472 -> 140107182864704
	140106819321616 [label="
 (1024, 1024, 5, 1)" fillcolor=lightblue]
	140106819321616 -> 140107182865472
	140107182865472 [label=AccumulateGrad]
	140107182865088 -> 140107182864704
	140106819321056 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106819321056 -> 140107182865088
	140107182865088 [label=AccumulateGrad]
	140107182864560 -> 140107182864656
	140106819316736 [label="
 (1024)" fillcolor=lightblue]
	140106819316736 -> 140107182864560
	140107182864560 [label=AccumulateGrad]
	140107182864416 -> 140107182853088
	140107182864416 -> 140106815162064 [dir=none]
	140106815162064 [label="g
 (1, 1, 1, 1)" fillcolor=orange]
	140107182864416 -> 140106725094992 [dir=none]
	140106725094992 [label="result1
 (1, 1, 1, 1)" fillcolor=orange]
	140107182864416 -> 140106815163584 [dir=none]
	140106815163584 [label="v
 (1, 1024, 3, 1)" fillcolor=orange]
	140107182864416 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140107182865184 -> 140107182864416
	140106815163584 [label="
 (1, 1024, 3, 1)" fillcolor=lightblue]
	140106815163584 -> 140107182865184
	140107182865184 [label=AccumulateGrad]
	140107182864800 -> 140107182864416
	140106815162064 [label="
 (1, 1, 1, 1)" fillcolor=lightblue]
	140106815162064 -> 140107182864800
	140107182864800 [label=AccumulateGrad]
	140107182863408 -> 140107182853088
	140106815161904 [label="
 (1)" fillcolor=lightblue]
	140106815161904 -> 140107182863408
	140107182863408 [label=AccumulateGrad]
	140106869097904 -> 140106869101648
	140106869097904 [label="MeanBackward0
------------------------
self_sym_numel:     2254
self_sym_sizes: (7, 322)"]
	140107522418784 -> 140106869097904
	140107522418784 -> 140106745666336 [dir=none]
	140106745666336 [label="self
 (7, 322)" fillcolor=orange]
	140107522418784 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106869105632 -> 140107522418784
	140106869105632 [label="RsubBackward1
-------------
alpha: 1"]
	140107182864896 -> 140106869105632
	140107182864896 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 46, 7)"]
	140107182864608 -> 140107182864896
	140107182864608 -> 140106745668416 [dir=none]
	140106745668416 [label="input
 (7, 1024, 46, 7)" fillcolor=orange]
	140107182864608 -> 140106745660816 [dir=none]
	140106745660816 [label="weight
 (1, 1024, 3, 1)" fillcolor=orange]
	140107182864608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140107182866288 -> 140107182864608
	140107182866288 -> 140106745659456 [dir=none]
	140106745659456 [label="self
 (7, 1024, 46, 7)" fillcolor=orange]
	140107182866288 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140107182866384 -> 140107182866288
	140107182866384 -> 140107065184864 [dir=none]
	140107065184864 [label="input
 (7, 1024, 46, 7)" fillcolor=orange]
	140107182866384 -> 140106745668336 [dir=none]
	140106745668336 [label="weight
 (1024, 1024, 5, 1)" fillcolor=orange]
	140107182866384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857840848 -> 140107182866384
	140106857840848 -> 140107110292576 [dir=none]
	140107110292576 [label="self
 (7, 1024, 46, 7)" fillcolor=orange]
	140106857840848 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857841040 -> 140106857840848
	140106857841040 -> 140106745664576 [dir=none]
	140106745664576 [label="input
 (7, 512, 136, 7)" fillcolor=orange]
	140106857841040 -> 140106745665296 [dir=none]
	140106745665296 [label="weight
 (1024, 512, 5, 1)" fillcolor=orange]
	140106857841040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857841136 -> 140106857841040
	140106857841136 -> 140106745664416 [dir=none]
	140106745664416 [label="self
 (7, 512, 136, 7)" fillcolor=orange]
	140106857841136 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857841328 -> 140106857841136
	140106857841328 -> 140106745663856 [dir=none]
	140106745663856 [label="input
 (7, 128, 407, 7)" fillcolor=orange]
	140106857841328 -> 140106745664016 [dir=none]
	140106745664016 [label="weight
 (512, 128, 5, 1)" fillcolor=orange]
	140106857841328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857841424 -> 140106857841328
	140106857841424 -> 140106745663776 [dir=none]
	140106745663776 [label="self
 (7, 128, 407, 7)" fillcolor=orange]
	140106857841424 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857841616 -> 140106857841424
	140106857841616 -> 140106745663056 [dir=none]
	140106745663056 [label="input
 (7, 32, 1220, 7)" fillcolor=orange]
	140106857841616 -> 140106745663296 [dir=none]
	140106745663296 [label="weight
 (128, 32, 5, 1)" fillcolor=orange]
	140106857841616 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857841712 -> 140106857841616
	140106857841712 -> 140106745662976 [dir=none]
	140106745662976 [label="self
 (7, 32, 1220, 7)" fillcolor=orange]
	140106857841712 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857841904 -> 140106857841712
	140106857841904 -> 140106745661456 [dir=none]
	140106745661456 [label="input
 (7, 1, 3658, 7)" fillcolor=orange]
	140106857841904 -> 140106745662896 [dir=none]
	140106745662896 [label="weight
 (32, 1, 5, 1)" fillcolor=orange]
	140106857841904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857842000 -> 140106857841904
	140106857842000 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 25606)"]
	140106857842192 -> 140106857842000
	140106857842192 -> 140106784278864 [dir=none]
	140106784278864 [label="self
 (7, 1, 25600)" fillcolor=orange]
	140106857842192 [label="ReflectionPad1DBackward0
------------------------
padding:         (0, 6)
self   : [saved tensor]"]
	140106869108320 -> 140106857842192
	140106857841952 -> 140106857841904
	140106857841952 -> 140106812695600 [dir=none]
	140106812695600 [label="g
 (32, 1, 1, 1)" fillcolor=orange]
	140106857841952 -> 140106725095632 [dir=none]
	140106725095632 [label="result1
 (32, 1, 1, 1)" fillcolor=orange]
	140106857841952 -> 140106809591184 [dir=none]
	140106809591184 [label="v
 (32, 1, 5, 1)" fillcolor=orange]
	140106857841952 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857842240 -> 140106857841952
	140106809591184 [label="
 (32, 1, 5, 1)" fillcolor=lightblue]
	140106809591184 -> 140106857842240
	140106857842240 [label=AccumulateGrad]
	140106857842288 -> 140106857841952
	140106812695600 [label="
 (32, 1, 1, 1)" fillcolor=lightblue]
	140106812695600 -> 140106857842288
	140106857842288 [label=AccumulateGrad]
	140106857841808 -> 140106857841904
	140106812689440 [label="
 (32)" fillcolor=lightblue]
	140106812689440 -> 140106857841808
	140106857841808 [label=AccumulateGrad]
	140106857841664 -> 140106857841616
	140106857841664 -> 140106803957568 [dir=none]
	140106803957568 [label="g
 (128, 1, 1, 1)" fillcolor=orange]
	140106857841664 -> 140106725096032 [dir=none]
	140106725096032 [label="result1
 (128, 1, 1, 1)" fillcolor=orange]
	140106857841664 -> 140106803960288 [dir=none]
	140106803960288 [label="v
 (128, 32, 5, 1)" fillcolor=orange]
	140106857841664 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857842336 -> 140106857841664
	140106803960288 [label="
 (128, 32, 5, 1)" fillcolor=lightblue]
	140106803960288 -> 140106857842336
	140106857842336 [label=AccumulateGrad]
	140106857842048 -> 140106857841664
	140106803957568 [label="
 (128, 1, 1, 1)" fillcolor=lightblue]
	140106803957568 -> 140106857842048
	140106857842048 [label=AccumulateGrad]
	140106857841520 -> 140106857841616
	140106806766912 [label="
 (128)" fillcolor=lightblue]
	140106806766912 -> 140106857841520
	140106857841520 [label=AccumulateGrad]
	140106857841376 -> 140106857841328
	140106857841376 -> 140106800820800 [dir=none]
	140106800820800 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	140106857841376 -> 140106725096512 [dir=none]
	140106725096512 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	140106857841376 -> 140106800821840 [dir=none]
	140106800821840 [label="v
 (512, 128, 5, 1)" fillcolor=orange]
	140106857841376 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857842144 -> 140106857841376
	140106800821840 [label="
 (512, 128, 5, 1)" fillcolor=lightblue]
	140106800821840 -> 140106857842144
	140106857842144 [label=AccumulateGrad]
	140106857841760 -> 140106857841376
	140106800820800 [label="
 (512, 1, 1, 1)" fillcolor=lightblue]
	140106800820800 -> 140106857841760
	140106857841760 [label=AccumulateGrad]
	140106857841232 -> 140106857841328
	140106800815680 [label="
 (512)" fillcolor=lightblue]
	140106800815680 -> 140106857841232
	140106857841232 [label=AccumulateGrad]
	140106857841088 -> 140106857841040
	140106857841088 -> 140106800229376 [dir=none]
	140106800229376 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857841088 -> 140106725096192 [dir=none]
	140106725096192 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857841088 -> 140106800231056 [dir=none]
	140106800231056 [label="v
 (1024, 512, 5, 1)" fillcolor=orange]
	140106857841088 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857841856 -> 140106857841088
	140106800231056 [label="
 (1024, 512, 5, 1)" fillcolor=lightblue]
	140106800231056 -> 140106857841856
	140106857841856 [label=AccumulateGrad]
	140106857841472 -> 140106857841088
	140106800229376 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106800229376 -> 140106857841472
	140106857841472 [label=AccumulateGrad]
	140106857840944 -> 140106857841040
	140106800229136 [label="
 (1024)" fillcolor=lightblue]
	140106800229136 -> 140106857840944
	140106857840944 [label=AccumulateGrad]
	140106857840800 -> 140107182866384
	140106857840800 -> 140106798113040 [dir=none]
	140106798113040 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857840800 -> 140106725096752 [dir=none]
	140106725096752 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857840800 -> 140106798113120 [dir=none]
	140106798113120 [label="v
 (1024, 1024, 5, 1)" fillcolor=orange]
	140106857840800 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857841568 -> 140106857840800
	140106798113120 [label="
 (1024, 1024, 5, 1)" fillcolor=lightblue]
	140106798113120 -> 140106857841568
	140106857841568 [label=AccumulateGrad]
	140106857841184 -> 140106857840800
	140106798113040 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106798113040 -> 140106857841184
	140106857841184 [label=AccumulateGrad]
	140106857840704 -> 140107182866384
	140106798112880 [label="
 (1024)" fillcolor=lightblue]
	140106798112880 -> 140106857840704
	140106857840704 [label=AccumulateGrad]
	140107182866240 -> 140107182864608
	140107182866240 -> 140106798113920 [dir=none]
	140106798113920 [label="g
 (1, 1, 1, 1)" fillcolor=orange]
	140107182866240 -> 140106725093312 [dir=none]
	140106725093312 [label="result1
 (1, 1, 1, 1)" fillcolor=orange]
	140107182866240 -> 140106798114080 [dir=none]
	140106798114080 [label="v
 (1, 1024, 3, 1)" fillcolor=orange]
	140107182866240 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857841280 -> 140107182866240
	140106798114080 [label="
 (1, 1024, 3, 1)" fillcolor=lightblue]
	140106798114080 -> 140106857841280
	140106857841280 [label=AccumulateGrad]
	140106857840896 -> 140107182866240
	140106798113920 [label="
 (1, 1, 1, 1)" fillcolor=lightblue]
	140106798113920 -> 140106857840896
	140106857840896 [label=AccumulateGrad]
	140107182864128 -> 140107182864608
	140106798113840 [label="
 (1)" fillcolor=lightblue]
	140106798113840 -> 140107182864128
	140107182864128 [label=AccumulateGrad]
	140106869109088 -> 140106869102320
	140106869109088 [label="MeanBackward0
------------------------
self_sym_numel:     2233
self_sym_sizes: (7, 319)"]
	140106869106784 -> 140106869109088
	140106869106784 -> 140106745666576 [dir=none]
	140106745666576 [label="self
 (7, 319)" fillcolor=orange]
	140106869106784 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140107522420272 -> 140106869106784
	140107522420272 [label="RsubBackward1
-------------
alpha: 1"]
	140107182866336 -> 140107522420272
	140107182866336 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 1, 29, 11)"]
	140106857840752 -> 140107182866336
	140106857840752 -> 140106745660096 [dir=none]
	140106745660096 [label="input
 (7, 1024, 29, 11)" fillcolor=orange]
	140106857840752 -> 140106745673136 [dir=none]
	140106745673136 [label="weight
 (1, 1024, 3, 1)" fillcolor=orange]
	140106857840752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (1, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857842432 -> 140106857840752
	140106857842432 -> 140106745659936 [dir=none]
	140106745659936 [label="self
 (7, 1024, 29, 11)" fillcolor=orange]
	140106857842432 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106871775264 -> 140106857842432
	140106871775264 -> 140106745659536 [dir=none]
	140106745659536 [label="input
 (7, 1024, 29, 11)" fillcolor=orange]
	140106871775264 -> 140106745659776 [dir=none]
	140106745659776 [label="weight
 (1024, 1024, 5, 1)" fillcolor=orange]
	140106871775264 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (1, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857842528 -> 140106871775264
	140106857842528 -> 140106745675456 [dir=none]
	140106745675456 [label="self
 (7, 1024, 29, 11)" fillcolor=orange]
	140106857842528 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857842816 -> 140106857842528
	140106857842816 -> 140106745674656 [dir=none]
	140106745674656 [label="input
 (7, 512, 87, 11)" fillcolor=orange]
	140106857842816 -> 140106745674976 [dir=none]
	140106745674976 [label="weight
 (1024, 512, 5, 1)" fillcolor=orange]
	140106857842816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1024,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857842912 -> 140106857842816
	140106857842912 -> 140106745674576 [dir=none]
	140106745674576 [label="self
 (7, 512, 87, 11)" fillcolor=orange]
	140106857842912 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857843104 -> 140106857842912
	140106857843104 -> 140106745674176 [dir=none]
	140106745674176 [label="input
 (7, 128, 259, 11)" fillcolor=orange]
	140106857843104 -> 140106745674416 [dir=none]
	140106745674416 [label="weight
 (512, 128, 5, 1)" fillcolor=orange]
	140106857843104 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (512,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857843200 -> 140106857843104
	140106857843200 -> 140106745674096 [dir=none]
	140106745674096 [label="self
 (7, 128, 259, 11)" fillcolor=orange]
	140106857843200 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857843392 -> 140106857843200
	140106857843392 -> 140106745673536 [dir=none]
	140106745673536 [label="input
 (7, 32, 776, 11)" fillcolor=orange]
	140106857843392 -> 140106745674016 [dir=none]
	140106745674016 [label="weight
 (128, 32, 5, 1)" fillcolor=orange]
	140106857843392 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (128,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857843488 -> 140106857843392
	140106857843488 -> 140106745662736 [dir=none]
	140106745662736 [label="self
 (7, 32, 776, 11)" fillcolor=orange]
	140106857843488 [label="LeakyReluBackward0
------------------------------
negative_slope:            0.1
self          : [saved tensor]"]
	140106857843680 -> 140106857843488
	140106857843680 -> 140106745673296 [dir=none]
	140106745673296 [label="input
 (7, 1, 2328, 11)" fillcolor=orange]
	140106857843680 -> 140106745673376 [dir=none]
	140106745673376 [label="weight
 (32, 1, 5, 1)" fillcolor=orange]
	140106857843680 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (32,)
dilation          :         (1, 1)
groups            :              1
input             : [saved tensor]
output_padding    :         (0, 0)
padding           :         (2, 0)
stride            :         (3, 1)
transposed        :          False
weight            : [saved tensor]"]
	140106857843776 -> 140106857843680
	140106857843776 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1, 25608)"]
	140106857843968 -> 140106857843776
	140106857843968 -> 140106784278864 [dir=none]
	140106784278864 [label="self
 (7, 1, 25600)" fillcolor=orange]
	140106857843968 [label="ReflectionPad1DBackward0
------------------------
padding:         (0, 8)
self   : [saved tensor]"]
	140106869108320 -> 140106857843968
	140106857843728 -> 140106857843680
	140106857843728 -> 140106798115920 [dir=none]
	140106798115920 [label="g
 (32, 1, 1, 1)" fillcolor=orange]
	140106857843728 -> 140106752179664 [dir=none]
	140106752179664 [label="result1
 (32, 1, 1, 1)" fillcolor=orange]
	140106857843728 -> 140106798116000 [dir=none]
	140106798116000 [label="v
 (32, 1, 5, 1)" fillcolor=orange]
	140106857843728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857844016 -> 140106857843728
	140106798116000 [label="
 (32, 1, 5, 1)" fillcolor=lightblue]
	140106798116000 -> 140106857844016
	140106857844016 [label=AccumulateGrad]
	140106857844064 -> 140106857843728
	140106798115920 [label="
 (32, 1, 1, 1)" fillcolor=lightblue]
	140106798115920 -> 140106857844064
	140106857844064 [label=AccumulateGrad]
	140106857843584 -> 140106857843680
	140106798115760 [label="
 (32)" fillcolor=lightblue]
	140106798115760 -> 140106857843584
	140106857843584 [label=AccumulateGrad]
	140106857843440 -> 140106857843392
	140106857843440 -> 140106798117280 [dir=none]
	140106798117280 [label="g
 (128, 1, 1, 1)" fillcolor=orange]
	140106857843440 -> 140106761900240 [dir=none]
	140106761900240 [label="result1
 (128, 1, 1, 1)" fillcolor=orange]
	140106857843440 -> 140106798117360 [dir=none]
	140106798117360 [label="v
 (128, 32, 5, 1)" fillcolor=orange]
	140106857843440 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857844112 -> 140106857843440
	140106798117360 [label="
 (128, 32, 5, 1)" fillcolor=lightblue]
	140106798117360 -> 140106857844112
	140106857844112 [label=AccumulateGrad]
	140106857843824 -> 140106857843440
	140106798117280 [label="
 (128, 1, 1, 1)" fillcolor=lightblue]
	140106798117280 -> 140106857843824
	140106857843824 [label=AccumulateGrad]
	140106857843296 -> 140106857843392
	140106798117200 [label="
 (128)" fillcolor=lightblue]
	140106798117200 -> 140106857843296
	140106857843296 [label=AccumulateGrad]
	140106857843152 -> 140106857843104
	140106857843152 -> 140106798118400 [dir=none]
	140106798118400 [label="g
 (512, 1, 1, 1)" fillcolor=orange]
	140106857843152 -> 140106725096432 [dir=none]
	140106725096432 [label="result1
 (512, 1, 1, 1)" fillcolor=orange]
	140106857843152 -> 140106798118320 [dir=none]
	140106798118320 [label="v
 (512, 128, 5, 1)" fillcolor=orange]
	140106857843152 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857843920 -> 140106857843152
	140106798118320 [label="
 (512, 128, 5, 1)" fillcolor=lightblue]
	140106798118320 -> 140106857843920
	140106857843920 [label=AccumulateGrad]
	140106857843536 -> 140106857843152
	140106798118400 [label="
 (512, 1, 1, 1)" fillcolor=lightblue]
	140106798118400 -> 140106857843536
	140106857843536 [label=AccumulateGrad]
	140106857843008 -> 140106857843104
	140106798118160 [label="
 (512)" fillcolor=lightblue]
	140106798118160 -> 140106857843008
	140106857843008 [label=AccumulateGrad]
	140106857842864 -> 140106857842816
	140106857842864 -> 140106798119760 [dir=none]
	140106798119760 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857842864 -> 140106725096352 [dir=none]
	140106725096352 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857842864 -> 140106798120080 [dir=none]
	140106798120080 [label="v
 (1024, 512, 5, 1)" fillcolor=orange]
	140106857842864 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857843632 -> 140106857842864
	140106798120080 [label="
 (1024, 512, 5, 1)" fillcolor=lightblue]
	140106798120080 -> 140106857843632
	140106857843632 [label=AccumulateGrad]
	140106857843248 -> 140106857842864
	140106798119760 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106798119760 -> 140106857843248
	140106857843248 [label=AccumulateGrad]
	140106857842720 -> 140106857842816
	140106798119680 [label="
 (1024)" fillcolor=lightblue]
	140106798119680 -> 140106857842720
	140106857842720 [label=AccumulateGrad]
	140106857842576 -> 140106871775264
	140106857842576 -> 140106794041808 [dir=none]
	140106794041808 [label="g
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857842576 -> 140106731492096 [dir=none]
	140106731492096 [label="result1
 (1024, 1, 1, 1)" fillcolor=orange]
	140106857842576 -> 140106794041888 [dir=none]
	140106794041888 [label="v
 (1024, 1024, 5, 1)" fillcolor=orange]
	140106857842576 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106857843344 -> 140106857842576
	140106794041888 [label="
 (1024, 1024, 5, 1)" fillcolor=lightblue]
	140106794041888 -> 140106857843344
	140106857843344 [label=AccumulateGrad]
	140106857842960 -> 140106857842576
	140106794041808 [label="
 (1024, 1, 1, 1)" fillcolor=lightblue]
	140106794041808 -> 140106857842960
	140106857842960 [label=AccumulateGrad]
	140106857842624 -> 140106871775264
	140106794041728 [label="
 (1024)" fillcolor=lightblue]
	140106794041728 -> 140106857842624
	140106857842624 [label=AccumulateGrad]
	140106857842384 -> 140106857840752
	140106857842384 -> 140106794042688 [dir=none]
	140106794042688 [label="g
 (1, 1, 1, 1)" fillcolor=orange]
	140106857842384 -> 140106725097392 [dir=none]
	140106725097392 [label="result1
 (1, 1, 1, 1)" fillcolor=orange]
	140106857842384 -> 140106794042768 [dir=none]
	140106794042768 [label="v
 (1, 1024, 3, 1)" fillcolor=orange]
	140106857842384 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106871775360 -> 140106857842384
	140106794042768 [label="
 (1, 1024, 3, 1)" fillcolor=lightblue]
	140106794042768 -> 140106871775360
	140106871775360 [label=AccumulateGrad]
	140106857843056 -> 140106857842384
	140106794042688 [label="
 (1, 1, 1, 1)" fillcolor=lightblue]
	140106794042688 -> 140106857843056
	140106857843056 [label=AccumulateGrad]
	140106857840992 -> 140106857840752
	140106794042608 [label="
 (1)" fillcolor=lightblue]
	140106794042608 -> 140106857840992
	140106857840992 [label=AccumulateGrad]
	140106869108704 -> 140106869108416
	140106869108704 -> 140106725096592 [dir=none]
	140106725096592 [label="other
 ()" fillcolor=orange]
	140106869108704 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869104768 -> 140106869108704
	140106869104768 [label="AddBackward0
------------
alpha: 1"]
	140107182864512 -> 140106869104768
	140107182864512 [label="AddBackward0
------------
alpha: 1"]
	140106857843872 -> 140107182864512
	140106857843872 [label="AddBackward0
------------
alpha: 1"]
	140106857844208 -> 140106857843872
	140106857844208 [label="AddBackward0
------------
alpha: 1"]
	140106857844352 -> 140106857844208
	140106857844352 [label="AddBackward0
------------
alpha: 1"]
	140106857844496 -> 140106857844352
	140106857844496 [label="AddBackward0
------------
alpha: 1"]
	140106857844640 -> 140106857844496
	140106857844640 [label="AddBackward0
------------
alpha: 1"]
	140106857844784 -> 140106857844640
	140106857844784 [label="AddBackward0
------------
alpha: 1"]
	140106857844928 -> 140106857844784
	140106857844928 [label="AddBackward0
------------
alpha: 1"]
	140106857845072 -> 140106857844928
	140106857845072 [label="AddBackward0
------------
alpha: 1"]
	140106857845216 -> 140106857845072
	140106857845216 [label="AddBackward0
------------
alpha: 1"]
	140106857845360 -> 140106857845216
	140106857845360 [label="AddBackward0
------------
alpha: 1"]
	140106857845504 -> 140106857845360
	140106857845504 [label="AddBackward0
------------
alpha: 1"]
	140106857845648 -> 140106857845504
	140106857845648 [label="AddBackward0
------------
alpha: 1"]
	140106857845792 -> 140106857845648
	140106857845792 [label="AddBackward0
------------
alpha: 1"]
	140106857845936 -> 140106857845792
	140106857845936 [label="AddBackward0
------------
alpha: 1"]
	140106857846080 -> 140106857845936
	140106857846080 [label="AddBackward0
------------
alpha: 1"]
	140106857846224 -> 140106857846080
	140106857846224 [label="AddBackward0
------------
alpha: 1"]
	140106857846368 -> 140106857846224
	140106857846368 [label="AddBackward0
------------
alpha: 1"]
	140106857846512 -> 140106857846368
	140106857846512 [label="AddBackward0
------------
alpha: 1"]
	140106857846656 -> 140106857846512
	140106857846656 [label="AddBackward0
------------
alpha: 1"]
	140106857846800 -> 140106857846656
	140106857846800 [label="AddBackward0
------------
alpha: 1"]
	140106857846944 -> 140106857846800
	140106857846944 [label="AddBackward0
------------
alpha: 1"]
	140106857847088 -> 140106857846944
	140106857847088 [label="AddBackward0
------------
alpha: 1"]
	140106857847232 -> 140106857847088
	140106857847232 [label="AddBackward0
------------
alpha: 1"]
	140106857847376 -> 140106857847232
	140106857847376 [label="AddBackward0
------------
alpha: 1"]
	140106857847520 -> 140106857847376
	140106857847520 [label="AddBackward0
------------
alpha: 1"]
	140106857847664 -> 140106857847520
	140106857847664 [label="AddBackward0
------------
alpha: 1"]
	140106857847808 -> 140106857847664
	140106857847808 [label="AddBackward0
------------
alpha: 1"]
	140106857847952 -> 140106857847808
	140106857847952 [label="AddBackward0
------------
alpha: 1"]
	140106857848096 -> 140106857847952
	140106857848096 [label="AddBackward0
------------
alpha: 1"]
	140106857848240 -> 140106857848096
	140106857848240 [label="AddBackward0
------------
alpha: 1"]
	140106857848384 -> 140106857848240
	140106857848384 [label="AddBackward0
------------
alpha: 1"]
	140106857848528 -> 140106857848384
	140106857848528 [label="AddBackward0
------------
alpha: 1"]
	140106857848672 -> 140106857848528
	140106857848672 [label="AddBackward0
------------
alpha: 1"]
	140106857848816 -> 140106857848672
	140106857848816 [label="AddBackward0
------------
alpha: 1"]
	140106857848960 -> 140106857848816
	140106857848960 [label="AddBackward0
------------
alpha: 1"]
	140106857849104 -> 140106857848960
	140106857849104 [label="MeanBackward0
------------------------------
self_sym_numel:        2867200
self_sym_sizes: (7, 16, 25600)"]
	140106857849200 -> 140106857849104
	140106857849200 -> 140106778580832 [dir=none]
	140106778580832 [label="self
 (7, 16, 25600)" fillcolor=orange]
	140106857849200 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857849296 -> 140106857849200
	140106857849296 [label="SubBackward0
------------
alpha: 1"]
	140106869106976 -> 140106857849296
	140106857848912 -> 140106857848816
	140106857848912 [label="MeanBackward0
-----------------------------
self_sym_numel:       2867200
self_sym_sizes: (7, 64, 6400)"]
	140106857849248 -> 140106857848912
	140106857849248 -> 140107058264512 [dir=none]
	140107058264512 [label="self
 (7, 64, 6400)" fillcolor=orange]
	140106857849248 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857849344 -> 140106857849248
	140106857849344 [label="SubBackward0
------------
alpha: 1"]
	140106869102896 -> 140106857849344
	140106857848768 -> 140106857848672
	140106857848768 [label="MeanBackward0
------------------------------
self_sym_numel:        2867200
self_sym_sizes: (7, 256, 1600)"]
	140106857849392 -> 140106857848768
	140106857849392 -> 140106774825216 [dir=none]
	140106774825216 [label="self
 (7, 256, 1600)" fillcolor=orange]
	140106857849392 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857849008 -> 140106857849392
	140106857849008 [label="SubBackward0
------------
alpha: 1"]
	140106869107600 -> 140106857849008
	140106857848624 -> 140106857848528
	140106857848624 [label="MeanBackward0
------------------------------
self_sym_numel:        2867200
self_sym_sizes: (7, 1024, 400)"]
	140106857849440 -> 140106857848624
	140106857849440 -> 140106774824176 [dir=none]
	140106774824176 [label="self
 (7, 1024, 400)" fillcolor=orange]
	140106857849440 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857849056 -> 140106857849440
	140106857849056 [label="SubBackward0
------------
alpha: 1"]
	140106869107072 -> 140106857849056
	140106857848480 -> 140106857848384
	140106857848480 [label="MeanBackward0
------------------------------
self_sym_numel:         716800
self_sym_sizes: (7, 1024, 100)"]
	140106857849488 -> 140106857848480
	140106857849488 -> 140106774823936 [dir=none]
	140106774823936 [label="self
 (7, 1024, 100)" fillcolor=orange]
	140106857849488 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848864 -> 140106857849488
	140106857848864 [label="SubBackward0
------------
alpha: 1"]
	140106869106016 -> 140106857848864
	140106857848336 -> 140106857848240
	140106857848336 [label="MeanBackward0
------------------------------
self_sym_numel:         716800
self_sym_sizes: (7, 1024, 100)"]
	140106857849536 -> 140106857848336
	140106857849536 -> 140106774829136 [dir=none]
	140106774829136 [label="self
 (7, 1024, 100)" fillcolor=orange]
	140106857849536 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848720 -> 140106857849536
	140106857848720 [label="SubBackward0
------------
alpha: 1"]
	140106869104288 -> 140106857848720
	140106857848192 -> 140106857848096
	140106857848192 [label="MeanBackward0
---------------------------
self_sym_numel:         700
self_sym_sizes: (7, 1, 100)"]
	140106857849584 -> 140106857848192
	140106857849584 -> 140106774825616 [dir=none]
	140106774825616 [label="self
 (7, 1, 100)" fillcolor=orange]
	140106857849584 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848576 -> 140106857849584
	140106857848576 [label="SubBackward0
------------
alpha: 1"]
	140106869103712 -> 140106857848576
	140106857848048 -> 140106857847952
	140106857848048 [label="MeanBackward0
--------------------------------
self_sym_numel:          1911616
self_sym_sizes: (7, 32, 4267, 2)"]
	140106857849632 -> 140106857848048
	140106857849632 -> 140106774832816 [dir=none]
	140106774832816 [label="self
 (7, 32, 4267, 2)" fillcolor=orange]
	140106857849632 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848432 -> 140106857849632
	140106857848432 [label="SubBackward0
------------
alpha: 1"]
	140107182862784 -> 140106857848432
	140106857847904 -> 140106857847808
	140106857847904 [label="MeanBackward0
---------------------------------
self_sym_numel:           2550016
self_sym_sizes: (7, 128, 1423, 2)"]
	140106857849680 -> 140106857847904
	140106857849680 -> 140106798116720 [dir=none]
	140106798116720 [label="self
 (7, 128, 1423, 2)" fillcolor=orange]
	140106857849680 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848288 -> 140106857849680
	140106857848288 [label="SubBackward0
------------
alpha: 1"]
	140107182862496 -> 140106857848288
	140106857847760 -> 140106857847664
	140106857847760 [label="MeanBackward0
--------------------------------
self_sym_numel:          3404800
self_sym_sizes: (7, 512, 475, 2)"]
	140106857849728 -> 140106857847760
	140106857849728 -> 140107031834640 [dir=none]
	140107031834640 [label="self
 (7, 512, 475, 2)" fillcolor=orange]
	140106857849728 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848144 -> 140106857849728
	140106857848144 [label="SubBackward0
------------
alpha: 1"]
	140107182862208 -> 140106857848144
	140106857847616 -> 140106857847520
	140106857847616 [label="MeanBackward0
---------------------------------
self_sym_numel:           2279424
self_sym_sizes: (7, 1024, 159, 2)"]
	140106857849776 -> 140106857847616
	140106857849776 -> 140107031822880 [dir=none]
	140107031822880 [label="self
 (7, 1024, 159, 2)" fillcolor=orange]
	140106857849776 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857848000 -> 140106857849776
	140106857848000 [label="SubBackward0
------------
alpha: 1"]
	140107182861920 -> 140106857848000
	140106857847472 -> 140106857847376
	140106857847472 [label="MeanBackward0
---------------------------------
self_sym_numel:           2279424
self_sym_sizes: (7, 1024, 159, 2)"]
	140106857849824 -> 140106857847472
	140106857849824 -> 140106745660496 [dir=none]
	140106745660496 [label="self
 (7, 1024, 159, 2)" fillcolor=orange]
	140106857849824 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857847856 -> 140106857849824
	140106857847856 [label="SubBackward0
------------
alpha: 1"]
	140107182859040 -> 140106857847856
	140106857847328 -> 140106857847232
	140106857847328 [label="MeanBackward0
------------------------------
self_sym_numel:           2226
self_sym_sizes: (7, 1, 159, 2)"]
	140106857849872 -> 140106857847328
	140106857849872 -> 140106745660656 [dir=none]
	140106745660656 [label="self
 (7, 1, 159, 2)" fillcolor=orange]
	140106857849872 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857847712 -> 140106857849872
	140106857847712 [label="SubBackward0
------------
alpha: 1"]
	140106869105728 -> 140106857847712
	140106857847184 -> 140106857847088
	140106857847184 [label="MeanBackward0
--------------------------------
self_sym_numel:          1911840
self_sym_sizes: (7, 32, 2845, 3)"]
	140106857849920 -> 140106857847184
	140106857849920 -> 140106745660896 [dir=none]
	140106745660896 [label="self
 (7, 32, 2845, 3)" fillcolor=orange]
	140106857849920 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857847568 -> 140106857849920
	140106857847568 [label="SubBackward0
------------
alpha: 1"]
	140107182863744 -> 140106857847568
	140106857847040 -> 140106857846944
	140106857847040 [label="MeanBackward0
--------------------------------
self_sym_numel:          2550912
self_sym_sizes: (7, 128, 949, 3)"]
	140106857849968 -> 140106857847040
	140106857849968 -> 140106745661136 [dir=none]
	140106745661136 [label="self
 (7, 128, 949, 3)" fillcolor=orange]
	140106857849968 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857847424 -> 140106857849968
	140106857847424 [label="SubBackward0
------------
alpha: 1"]
	140107182863456 -> 140106857847424
	140106857846896 -> 140106857846800
	140106857846896 [label="MeanBackward0
--------------------------------
self_sym_numel:          3408384
self_sym_sizes: (7, 512, 317, 3)"]
	140106857850016 -> 140106857846896
	140106857850016 -> 140106745661216 [dir=none]
	140106745661216 [label="self
 (7, 512, 317, 3)" fillcolor=orange]
	140106857850016 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857847280 -> 140106857850016
	140106857847280 [label="SubBackward0
------------
alpha: 1"]
	140107182860192 -> 140106857847280
	140106857846752 -> 140106857846656
	140106857846752 [label="MeanBackward0
---------------------------------
self_sym_numel:           2279424
self_sym_sizes: (7, 1024, 106, 3)"]
	140106857850064 -> 140106857846752
	140106857850064 -> 140106745661376 [dir=none]
	140106745661376 [label="self
 (7, 1024, 106, 3)" fillcolor=orange]
	140106857850064 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857847136 -> 140106857850064
	140106857847136 [label="SubBackward0
------------
alpha: 1"]
	140106869111248 -> 140106857847136
	140106857846608 -> 140106857846512
	140106857846608 [label="MeanBackward0
---------------------------------
self_sym_numel:           2279424
self_sym_sizes: (7, 1024, 106, 3)"]
	140106857850112 -> 140106857846608
	140106857850112 -> 140106745661616 [dir=none]
	140106745661616 [label="self
 (7, 1024, 106, 3)" fillcolor=orange]
	140106857850112 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846992 -> 140106857850112
	140106857846992 [label="SubBackward0
------------
alpha: 1"]
	140107522422960 -> 140106857846992
	140106857846464 -> 140106857846368
	140106857846464 [label="MeanBackward0
------------------------------
self_sym_numel:           2226
self_sym_sizes: (7, 1, 106, 3)"]
	140106857850160 -> 140106857846464
	140106857850160 -> 140106745661776 [dir=none]
	140106745661776 [label="self
 (7, 1, 106, 3)" fillcolor=orange]
	140106857850160 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846848 -> 140106857850160
	140106857846848 [label="SubBackward0
------------
alpha: 1"]
	140107522423104 -> 140106857846848
	140106857846320 -> 140106857846224
	140106857846320 [label="MeanBackward0
--------------------------------
self_sym_numel:          1911840
self_sym_sizes: (7, 32, 1707, 5)"]
	140106857850208 -> 140106857846320
	140106857850208 -> 140106745661856 [dir=none]
	140106745661856 [label="self
 (7, 32, 1707, 5)" fillcolor=orange]
	140106857850208 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846704 -> 140106857850208
	140106857846704 [label="SubBackward0
------------
alpha: 1"]
	140107182865616 -> 140106857846704
	140106857846176 -> 140106857846080
	140106857846176 [label="MeanBackward0
--------------------------------
self_sym_numel:          2549120
self_sym_sizes: (7, 128, 569, 5)"]
	140106857850256 -> 140106857846176
	140106857850256 -> 140106745662016 [dir=none]
	140106745662016 [label="self
 (7, 128, 569, 5)" fillcolor=orange]
	140106857850256 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846560 -> 140106857850256
	140106857846560 [label="SubBackward0
------------
alpha: 1"]
	140107182865328 -> 140106857846560
	140106857846032 -> 140106857845936
	140106857846032 [label="MeanBackward0
--------------------------------
self_sym_numel:          3404800
self_sym_sizes: (7, 512, 190, 5)"]
	140106857850304 -> 140106857846032
	140106857850304 -> 140106745662176 [dir=none]
	140106745662176 [label="self
 (7, 512, 190, 5)" fillcolor=orange]
	140106857850304 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846416 -> 140106857850304
	140106857846416 [label="SubBackward0
------------
alpha: 1"]
	140107182865040 -> 140106857846416
	140106857845888 -> 140106857845792
	140106857845888 [label="MeanBackward0
--------------------------------
self_sym_numel:          2293760
self_sym_sizes: (7, 1024, 64, 5)"]
	140106857850352 -> 140106857845888
	140106857850352 -> 140106745663136 [dir=none]
	140106745663136 [label="self
 (7, 1024, 64, 5)" fillcolor=orange]
	140106857850352 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846272 -> 140106857850352
	140106857846272 [label="SubBackward0
------------
alpha: 1"]
	140107182864752 -> 140106857846272
	140106857845744 -> 140106857845648
	140106857845744 [label="MeanBackward0
--------------------------------
self_sym_numel:          2293760
self_sym_sizes: (7, 1024, 64, 5)"]
	140106857850400 -> 140106857845744
	140106857850400 -> 140106745663536 [dir=none]
	140106745663536 [label="self
 (7, 1024, 64, 5)" fillcolor=orange]
	140106857850400 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857846128 -> 140106857850400
	140106857846128 [label="SubBackward0
------------
alpha: 1"]
	140107182864464 -> 140106857846128
	140106857845600 -> 140106857845504
	140106857845600 [label="MeanBackward0
-----------------------------
self_sym_numel:          2240
self_sym_sizes: (7, 1, 64, 5)"]
	140106857850448 -> 140106857845600
	140106857850448 -> 140106745663616 [dir=none]
	140106745663616 [label="self
 (7, 1, 64, 5)" fillcolor=orange]
	140106857850448 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857845984 -> 140106857850448
	140106857845984 [label="SubBackward0
------------
alpha: 1"]
	140107182853088 -> 140106857845984
	140106857845456 -> 140106857845360
	140106857845456 [label="MeanBackward0
--------------------------------
self_sym_numel:          1912960
self_sym_sizes: (7, 32, 1220, 7)"]
	140106857850496 -> 140106857845456
	140106857850496 -> 140106745663936 [dir=none]
	140106745663936 [label="self
 (7, 32, 1220, 7)" fillcolor=orange]
	140106857850496 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857845840 -> 140106857850496
	140106857845840 [label="SubBackward0
------------
alpha: 1"]
	140106857841712 -> 140106857845840
	140106857845312 -> 140106857845216
	140106857845312 [label="MeanBackward0
--------------------------------
self_sym_numel:          2552704
self_sym_sizes: (7, 128, 407, 7)"]
	140106857850544 -> 140106857845312
	140106857850544 -> 140106745664096 [dir=none]
	140106745664096 [label="self
 (7, 128, 407, 7)" fillcolor=orange]
	140106857850544 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857845696 -> 140106857850544
	140106857845696 [label="SubBackward0
------------
alpha: 1"]
	140106857841424 -> 140106857845696
	140106857845168 -> 140106857845072
	140106857845168 [label="MeanBackward0
--------------------------------
self_sym_numel:          3411968
self_sym_sizes: (7, 512, 136, 7)"]
	140106857850592 -> 140106857845168
	140106857850592 -> 140106745664256 [dir=none]
	140106745664256 [label="self
 (7, 512, 136, 7)" fillcolor=orange]
	140106857850592 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857845552 -> 140106857850592
	140106857845552 [label="SubBackward0
------------
alpha: 1"]
	140106857841136 -> 140106857845552
	140106857845024 -> 140106857844928
	140106857845024 [label="MeanBackward0
--------------------------------
self_sym_numel:          2308096
self_sym_sizes: (7, 1024, 46, 7)"]
	140106857850640 -> 140106857845024
	140106857850640 -> 140106745664496 [dir=none]
	140106745664496 [label="self
 (7, 1024, 46, 7)" fillcolor=orange]
	140106857850640 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857845408 -> 140106857850640
	140106857845408 [label="SubBackward0
------------
alpha: 1"]
	140106857840848 -> 140106857845408
	140106857844880 -> 140106857844784
	140106857844880 [label="MeanBackward0
--------------------------------
self_sym_numel:          2308096
self_sym_sizes: (7, 1024, 46, 7)"]
	140106857850832 -> 140106857844880
	140106857850832 -> 140106745664656 [dir=none]
	140106745664656 [label="self
 (7, 1024, 46, 7)" fillcolor=orange]
	140106857850832 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857850928 -> 140106857850832
	140106857850928 [label="SubBackward0
------------
alpha: 1"]
	140107182866288 -> 140106857850928
	140106857844736 -> 140106857844640
	140106857844736 [label="MeanBackward0
-----------------------------
self_sym_numel:          2254
self_sym_sizes: (7, 1, 46, 7)"]
	140106857850880 -> 140106857844736
	140106857850880 -> 140106745664736 [dir=none]
	140106745664736 [label="self
 (7, 1, 46, 7)" fillcolor=orange]
	140106857850880 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857850976 -> 140106857850880
	140106857850976 [label="SubBackward0
------------
alpha: 1"]
	140107182864608 -> 140106857850976
	140106857844592 -> 140106857844496
	140106857844592 [label="MeanBackward0
--------------------------------
self_sym_numel:          1912064
self_sym_sizes: (7, 32, 776, 11)"]
	140106857851024 -> 140106857844592
	140106857851024 -> 140106745664816 [dir=none]
	140106745664816 [label="self
 (7, 32, 776, 11)" fillcolor=orange]
	140106857851024 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857845120 -> 140106857851024
	140106857845120 [label="SubBackward0
------------
alpha: 1"]
	140106857843488 -> 140106857845120
	140106857844448 -> 140106857844352
	140106857844448 [label="MeanBackward0
---------------------------------
self_sym_numel:           2552704
self_sym_sizes: (7, 128, 259, 11)"]
	140106857851072 -> 140106857844448
	140106857851072 -> 140106745664976 [dir=none]
	140106745664976 [label="self
 (7, 128, 259, 11)" fillcolor=orange]
	140106857851072 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857844832 -> 140106857851072
	140106857844832 [label="SubBackward0
------------
alpha: 1"]
	140106857843200 -> 140106857844832
	140106857844304 -> 140106857844208
	140106857844304 [label="MeanBackward0
--------------------------------
self_sym_numel:          3429888
self_sym_sizes: (7, 512, 87, 11)"]
	140106857851120 -> 140106857844304
	140106857851120 -> 140106745665056 [dir=none]
	140106745665056 [label="self
 (7, 512, 87, 11)" fillcolor=orange]
	140106857851120 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857844688 -> 140106857851120
	140106857844688 [label="SubBackward0
------------
alpha: 1"]
	140106857842912 -> 140106857844688
	140106857844160 -> 140106857843872
	140106857844160 [label="MeanBackward0
---------------------------------
self_sym_numel:           2286592
self_sym_sizes: (7, 1024, 29, 11)"]
	140106857851168 -> 140106857844160
	140106857851168 -> 140106745665136 [dir=none]
	140106745665136 [label="self
 (7, 1024, 29, 11)" fillcolor=orange]
	140106857851168 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857844544 -> 140106857851168
	140106857844544 [label="SubBackward0
------------
alpha: 1"]
	140106857842528 -> 140106857844544
	140106857842768 -> 140107182864512
	140106857842768 [label="MeanBackward0
---------------------------------
self_sym_numel:           2286592
self_sym_sizes: (7, 1024, 29, 11)"]
	140106857851216 -> 140106857842768
	140106857851216 -> 140106745665216 [dir=none]
	140106745665216 [label="self
 (7, 1024, 29, 11)" fillcolor=orange]
	140106857851216 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857844400 -> 140106857851216
	140106857844400 [label="SubBackward0
------------
alpha: 1"]
	140106857842432 -> 140106857844400
	140107182866144 -> 140106869104768
	140107182866144 [label="MeanBackward0
------------------------------
self_sym_numel:           2233
self_sym_sizes: (7, 1, 29, 11)"]
	140106857851264 -> 140107182866144
	140106857851264 -> 140106745665376 [dir=none]
	140106745665376 [label="self
 (7, 1, 29, 11)" fillcolor=orange]
	140106857851264 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857844256 -> 140106857851264
	140106857844256 [label="SubBackward0
------------
alpha: 1"]
	140106857840752 -> 140106857844256
	140106869108032 -> 140106869107552
	140106869108032 -> 140106722771808 [dir=none]
	140106722771808 [label="other
 ()" fillcolor=orange]
	140106869108032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869108992 -> 140106869108032
	140106869108992 [label="MeanBackward0
----------------------------
self_sym_numel:        44800
self_sym_sizes: (7, 128, 50)"]
	140106857851312 -> 140106869108992
	140106857851312 -> 140106722771968 [dir=none]
	140106722771968 [label="self
 (7, 128, 50)" fillcolor=orange]
	140106857851312 [label="AbsBackward0
--------------------
self: [saved tensor]"]
	140106857842672 -> 140106857851312
	140106857842672 [label="SubBackward0
------------
alpha: 1"]
	140106857851408 -> 140106857842672
	140106857851408 -> 140106784278944 [dir=none]
	140106784278944 [label="self
 (7, 128, 50)" fillcolor=orange]
	140106857851408 [label="LogBackward0
--------------------
self: [saved tensor]"]
	140106857851504 -> 140106857851408
	140106857851504 -> 140106722771488 [dir=none]
	140106722771488 [label="other
 ()" fillcolor=orange]
	140106857851504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857851600 -> 140106857851504
	140106857851600 -> 140106784279024 [dir=none]
	140106784279024 [label="self
 (7, 128, 50)" fillcolor=orange]
	140106857851600 [label="ClampBackward1
--------------------
max :           None
min :          1e-05
self: [saved tensor]"]
	140106857851696 -> 140106857851600
	140106857851696 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (7, 128, 50)"]
	140106857845264 -> 140106857851696
	140106857845264 -> 140106722771728 [dir=none]
	140106722771728 [label="self
 (7, 128, 1025)" fillcolor=orange]
	140106857845264 [label="BmmBackward0
--------------------
mat2:           None
self: [saved tensor]"]
	140106857850736 -> 140106857845264
	140106857850736 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 1025, 50)"]
	140106857851792 -> 140106857850736
	140106857851792 [label="ExpandBackward0
-----------------------------
self_sym_sizes: (7, 1025, 50)"]
	140106857851888 -> 140106857851792
	140106857851888 -> 140106784277424 [dir=none]
	140106784277424 [label="result
 (7, 1025, 50)" fillcolor=orange]
	140106857851888 [label="SqrtBackward0
----------------------
result: [saved tensor]"]
	140106857851984 -> 140106857851888
	140106857851984 [label="AddBackward0
------------
alpha: 1"]
	140106857852080 -> 140106857851984
	140106857852080 [label="SumBackward1
---------------------------------------
dim           : (18446744073709551615,)
keepdim       :                   False
self_sym_sizes:        (7, 1025, 50, 2)"]
	140106857852224 -> 140106857852080
	140106857852224 -> 140106784278704 [dir=none]
	140106784278704 [label="self
 (7, 1025, 50, 2)" fillcolor=orange]
	140106857852224 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106857852320 -> 140106857852224
	140106857852320 [label=ViewAsRealBackward0]
	140106857852416 -> 140106857852320
	140106857852416 [label="TransposeBackward1
------------------
dim0: 1
dim1: 2"]
	140106857852512 -> 140106857852416
	140106857852512 -> 140106784283984 [dir=none]
	140106784283984 [label="self
 (7, 50, 2048)" fillcolor=orange]
	140106857852512 [label="FftR2CBackward0
-----------------------------
dim          :           (2,)
normalization:              0
onesided     :           True
self         : [saved tensor]"]
	140106857852608 -> 140106857852512
	140106857852608 -> 140107110294256 [dir=none]
	140107110294256 [label="other
 (2048)" fillcolor=orange]
	140106857852608 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857852704 -> 140106857852608
	140106857852704 [label="AsStridedBackward0
-------------------------------
size          :   (7, 50, 2048)
storage_offset:            None
stride        : (27136, 512, 1)"]
	140106857852800 -> 140106857852704
	140106857852800 [label="SqueezeBackward1
-----------------------------
dim           :             1
self_sym_sizes: (7, 1, 27136)"]
	140106857852896 -> 140106857852800
	140106857852896 -> 140106784279344 [dir=none]
	140106784279344 [label="self
 (7, 1, 25600)" fillcolor=orange]
	140106857852896 [label="ReflectionPad1DBackward0
------------------------
padding:     (768, 768)
self   : [saved tensor]"]
	140106857852992 -> 140106857852896
	140106857852992 [label="UnsqueezeBackward0
------------------
dim: 1"]
	140106857853088 -> 140106857852992
	140106857853088 [label="SqueezeBackward1
-----------------------------
dim           :             1
self_sym_sizes: (7, 1, 25600)"]
	140106869108320 -> 140106857853088
	140106869107168 -> 140106869105104
	140106869107168 -> 140106757918448 [dir=none]
	140106757918448 [label="other
 ()" fillcolor=orange]
	140106869107168 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869108176 -> 140106869107168
	140106869108176 [label="SumBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106857842480 -> 140106869108176
	140106857842480 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857842480 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857851552 -> 140106857842480
	140106857851552 [label="AddBackward0
------------
alpha: 1"]
	140106857849152 -> 140106857851552
	140106857849152 [label="SubBackward0
------------
alpha: 1"]
	140106857851840 -> 140106857849152
	140106857851840 [label="SubBackward0
------------
alpha: 1"]
	140106857852032 -> 140106857851840
	140106857852032 [label="SplitBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 384, 286)
split_size    :           192"]
	140106857852272 -> 140106857852032
	140106857852272 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857852272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857852464 -> 140106857852272
	140106857852464 -> 140106851288304 [dir=none]
	140106851288304 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106857852464 -> 140107053629360 [dir=none]
	140107053629360 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106857852464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857852656 -> 140106857852464
	140106857852656 -> 140106809604864 [dir=none]
	140106809604864 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106857852656 -> 140107053628960 [dir=none]
	140107053628960 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106857852656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857853040 -> 140106857852656
	140106857853040 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857853040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857853232 -> 140106857853040
	140106857853232 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857853376 -> 140106857853232
	140106857853376 -> 140107053628720 [dir=none]
	140107053628720 [label="bias
 (192)" fillcolor=orange]
	140106857853376 -> 140106803965008 [dir=none]
	140106803965008 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106857853376 -> 140106784285424 [dir=none]
	140106784285424 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106857853376 -> 140106722772128 [dir=none]
	140106722772128 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106857853376 -> 140107053628480 [dir=none]
	140107053628480 [label="weight
 (192)" fillcolor=orange]
	140106857853376 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106857844976 -> 140106857853376
	140106857844976 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857853712 -> 140106857844976
	140106857853712 [label="AddBackward0
------------
alpha: 1"]
	140106857853760 -> 140106857853712
	140106857853760 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857854000 -> 140106857853760
	140106857854000 -> 140107053627200 [dir=none]
	140107053627200 [label="bias
 (192)" fillcolor=orange]
	140106857854000 -> 140106806770112 [dir=none]
	140106806770112 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106857854000 -> 140106722771648 [dir=none]
	140106722771648 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106857854000 -> 140106722772608 [dir=none]
	140106722772608 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106857854000 -> 140107053627120 [dir=none]
	140107053627120 [label="weight
 (192)" fillcolor=orange]
	140106857854000 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106857854048 -> 140106857854000
	140106857854048 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857854336 -> 140106857854048
	140106857854336 [label="AddBackward0
------------
alpha: 1"]
	140106857854384 -> 140106857854336
	140106857854384 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857854624 -> 140106857854384
	140106857854624 -> 140107053626000 [dir=none]
	140107053626000 [label="bias
 (192)" fillcolor=orange]
	140106857854624 -> 140106812699040 [dir=none]
	140106812699040 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106857854624 -> 140106722772448 [dir=none]
	140106722772448 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106857854624 -> 140106722772848 [dir=none]
	140106722772848 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106857854624 -> 140107053625920 [dir=none]
	140107053625920 [label="weight
 (192)" fillcolor=orange]
	140106857854624 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106857854672 -> 140106857854624
	140106857854672 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857854960 -> 140106857854672
	140106857854960 [label="AddBackward0
------------
alpha: 1"]
	140106857855008 -> 140106857854960
	140106857855008 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857855248 -> 140106857855008
	140106857855248 -> 140107053625440 [dir=none]
	140107053625440 [label="bias
 (192)" fillcolor=orange]
	140106857855248 -> 140106815167824 [dir=none]
	140106815167824 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106857855248 -> 140106722772688 [dir=none]
	140106722772688 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106857855248 -> 140106722773168 [dir=none]
	140106722773168 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106857855248 -> 140107053625360 [dir=none]
	140107053625360 [label="weight
 (192)" fillcolor=orange]
	140106857855248 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106857855296 -> 140106857855248
	140106857855296 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857855584 -> 140106857855296
	140106857855584 [label="AddBackward0
------------
alpha: 1"]
	140106857855632 -> 140106857855584
	140106857855632 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857855872 -> 140106857855632
	140106857855872 -> 140107053622560 [dir=none]
	140107053622560 [label="bias
 (192)" fillcolor=orange]
	140106857855872 -> 140106841510736 [dir=none]
	140106841510736 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106857855872 -> 140106722773008 [dir=none]
	140106722773008 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106857855872 -> 140106722773328 [dir=none]
	140106722773328 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106857855872 -> 140107053622400 [dir=none]
	140107053622400 [label="weight
 (192)" fillcolor=orange]
	140106857855872 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106857855920 -> 140106857855872
	140106857855920 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857856208 -> 140106857855920
	140106857856208 [label="AddBackward0
------------
alpha: 1"]
	140106857856256 -> 140106857856208
	140106857856256 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857856496 -> 140106857856256
	140106857856496 -> 140107053621680 [dir=none]
	140107053621680 [label="bias
 (192)" fillcolor=orange]
	140106857856496 -> 140106840744928 [dir=none]
	140106840744928 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106857856496 -> 140106722773248 [dir=none]
	140106722773248 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106857856496 -> 140106722773488 [dir=none]
	140106722773488 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106857856496 -> 140107053621520 [dir=none]
	140107053621520 [label="weight
 (192)" fillcolor=orange]
	140106857856496 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106857856592 -> 140106857856496
	140106857856592 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106857856880 -> 140106857856592
	140106857856880 [label="AddBackward0
------------
alpha: 1"]
	140106857856928 -> 140106857856880
	140106857856928 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857856928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854695072 -> 140106857856928
	140106854695072 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106854695072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854695168 -> 140106854695072
	140106854695168 [label="AddBackward0
------------
alpha: 1"]
	140106854695264 -> 140106854695168
	140106854695264 [label="AddBackward0
------------
alpha: 1"]
	140106854695408 -> 140106854695264
	140106854695408 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106854695552 -> 140106854695408
	140106854695552 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 192)"]
	140106854695648 -> 140106854695552
	140106854695648 -> 140106722774128 [dir=none]
	140106722774128 [label="self
 (7, 286, 28)" fillcolor=orange]
	140106854695648 [label="BmmBackward0
--------------------
mat2:           None
self: [saved tensor]"]
	140106854695744 -> 140106854695648
	140106854695744 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 28, 192)"]
	140106854695840 -> 140106854695744
	140106854695840 [label="ExpandBackward0
----------------------------
self_sym_sizes: (7, 28, 192)"]
	140106854695936 -> 140106854695840
	140106854695936 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106854696032 -> 140106854695936
	140106854696032 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854696032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854696128 -> 140106854696032
	140106854696128 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854696272 -> 140106854696128
	140106854696272 -> 140107071062240 [dir=none]
	140107071062240 [label="bias
 (192)" fillcolor=orange]
	140106854696272 -> 140107077228944 [dir=none]
	140107077228944 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854696272 -> 140106722774368 [dir=none]
	140106722774368 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854696272 -> 140106722771328 [dir=none]
	140106722771328 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854696272 -> 140107071062080 [dir=none]
	140107071062080 [label="weight
 (192)" fillcolor=orange]
	140106854696272 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854696320 -> 140106854696272
	140106854696320 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854696608 -> 140106854696320
	140106854696608 [label="AddBackward0
------------
alpha: 1"]
	140106854696656 -> 140106854696608
	140106854696656 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854696896 -> 140106854696656
	140106854696896 -> 140107077236864 [dir=none]
	140107077236864 [label="bias
 (192)" fillcolor=orange]
	140106854696896 -> 140107077234624 [dir=none]
	140107077234624 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854696896 -> 140106722774448 [dir=none]
	140106722774448 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854696896 -> 140106722772368 [dir=none]
	140106722772368 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854696896 -> 140107077236624 [dir=none]
	140107077236624 [label="weight
 (192)" fillcolor=orange]
	140106854696896 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854696944 -> 140106854696896
	140106854696944 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854697232 -> 140106854696944
	140106854697232 [label="AddBackward0
------------
alpha: 1"]
	140106854697280 -> 140106854697232
	140106854697280 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854697520 -> 140106854697280
	140106854697520 -> 140107077235104 [dir=none]
	140107077235104 [label="bias
 (192)" fillcolor=orange]
	140106854697520 -> 140107077222784 [dir=none]
	140107077222784 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854697520 -> 140106722773568 [dir=none]
	140106722773568 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854697520 -> 140106722774608 [dir=none]
	140106722774608 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854697520 -> 140107077234944 [dir=none]
	140107077234944 [label="weight
 (192)" fillcolor=orange]
	140106854697520 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854697568 -> 140106854697520
	140106854697568 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854697856 -> 140106854697568
	140106854697856 [label="AddBackward0
------------
alpha: 1"]
	140106854697904 -> 140106854697856
	140106854697904 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854698144 -> 140106854697904
	140106854698144 -> 140107077234144 [dir=none]
	140107077234144 [label="bias
 (192)" fillcolor=orange]
	140106854698144 -> 140107082872800 [dir=none]
	140107082872800 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854698144 -> 140106722773408 [dir=none]
	140106722773408 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854698144 -> 140106722774528 [dir=none]
	140106722774528 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854698144 -> 140107077233984 [dir=none]
	140107077233984 [label="weight
 (192)" fillcolor=orange]
	140106854698144 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854698192 -> 140106854698144
	140106854698192 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854698480 -> 140106854698192
	140106854698480 [label="AddBackward0
------------
alpha: 1"]
	140106854698528 -> 140106854698480
	140106854698528 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854698768 -> 140106854698528
	140106854698768 -> 140107077232304 [dir=none]
	140107077232304 [label="bias
 (192)" fillcolor=orange]
	140106854698768 -> 140107082861760 [dir=none]
	140107082861760 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854698768 -> 140106722774288 [dir=none]
	140106722774288 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854698768 -> 140106722774688 [dir=none]
	140106722774688 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854698768 -> 140107077231744 [dir=none]
	140107077231744 [label="weight
 (192)" fillcolor=orange]
	140106854698768 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854698816 -> 140106854698768
	140106854698816 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854699104 -> 140106854698816
	140106854699104 [label="AddBackward0
------------
alpha: 1"]
	140106854699152 -> 140106854699104
	140106854699152 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854699392 -> 140106854699152
	140106854699392 -> 140107077230784 [dir=none]
	140107077230784 [label="bias
 (192)" fillcolor=orange]
	140106854699392 -> 140107082865280 [dir=none]
	140107082865280 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854699392 -> 140106722775008 [dir=none]
	140106722775008 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854699392 -> 140106722775248 [dir=none]
	140106722775248 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854699392 -> 140107077230704 [dir=none]
	140107077230704 [label="weight
 (192)" fillcolor=orange]
	140106854699392 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854699440 -> 140106854699392
	140106854699440 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854699728 -> 140106854699440
	140106854699728 [label="AddBackward0
------------
alpha: 1"]
	140106854699776 -> 140106854699728
	140106854699776 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854700016 -> 140106854699776
	140106854700016 -> 140107077229504 [dir=none]
	140107077229504 [label="bias
 (192)" fillcolor=orange]
	140106854700016 -> 140107086978304 [dir=none]
	140107086978304 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854700016 -> 140106722775168 [dir=none]
	140106722775168 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854700016 -> 140106722775408 [dir=none]
	140106722775408 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854700016 -> 140107077229184 [dir=none]
	140107077229184 [label="weight
 (192)" fillcolor=orange]
	140106854700016 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854700064 -> 140106854700016
	140106854700064 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854700352 -> 140106854700064
	140106854700352 [label="AddBackward0
------------
alpha: 1"]
	140106854700400 -> 140106854700352
	140106854700400 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854700640 -> 140106854700400
	140106854700640 -> 140107077228064 [dir=none]
	140107077228064 [label="bias
 (192)" fillcolor=orange]
	140106854700640 -> 140107086979264 [dir=none]
	140107086979264 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854700640 -> 140106722775328 [dir=none]
	140106722775328 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854700640 -> 140106722775568 [dir=none]
	140106722775568 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854700640 -> 140107077227984 [dir=none]
	140107077227984 [label="weight
 (192)" fillcolor=orange]
	140106854700640 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854700688 -> 140106854700640
	140106854700688 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854700976 -> 140106854700688
	140106854700976 [label="AddBackward0
------------
alpha: 1"]
	140106854701024 -> 140106854700976
	140106854701024 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854701264 -> 140106854701024
	140106854701264 -> 140107077225664 [dir=none]
	140107077225664 [label="bias
 (192)" fillcolor=orange]
	140106854701264 -> 140107086971824 [dir=none]
	140107086971824 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854701264 -> 140106722775488 [dir=none]
	140106722775488 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854701264 -> 140106722775728 [dir=none]
	140106722775728 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854701264 -> 140107077225584 [dir=none]
	140107077225584 [label="weight
 (192)" fillcolor=orange]
	140106854701264 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854701456 -> 140106854701264
	140106854701456 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854701744 -> 140106854701456
	140106854701744 [label="AddBackward0
------------
alpha: 1"]
	140106854701792 -> 140106854701744
	140106854701792 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854702032 -> 140106854701792
	140106854702032 -> 140107077224544 [dir=none]
	140107077224544 [label="bias
 (192)" fillcolor=orange]
	140106854702032 -> 140107092179312 [dir=none]
	140107092179312 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854702032 -> 140106722775968 [dir=none]
	140106722775968 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854702032 -> 140106722774848 [dir=none]
	140106722774848 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854702032 -> 140107077224384 [dir=none]
	140107077224384 [label="weight
 (192)" fillcolor=orange]
	140106854702032 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854702080 -> 140106854702032
	140106854702080 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854702368 -> 140106854702080
	140106854702368 [label="AddBackward0
------------
alpha: 1"]
	140106854702416 -> 140106854702368
	140106854702416 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854702656 -> 140106854702416
	140106854702656 -> 140107082874800 [dir=none]
	140107082874800 [label="bias
 (192)" fillcolor=orange]
	140106854702656 -> 140107092174672 [dir=none]
	140107092174672 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854702656 -> 140106722774768 [dir=none]
	140106722774768 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854702656 -> 140106722776048 [dir=none]
	140106722776048 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854702656 -> 140107082874640 [dir=none]
	140107082874640 [label="weight
 (192)" fillcolor=orange]
	140106854702656 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854702704 -> 140106854702656
	140106854702704 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854702992 -> 140106854702704
	140106854702992 [label="AddBackward0
------------
alpha: 1"]
	140106854703040 -> 140106854702992
	140106854703040 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854703280 -> 140106854703040
	140106854703280 -> 140107082874080 [dir=none]
	140107082874080 [label="bias
 (192)" fillcolor=orange]
	140106854703280 -> 140107092177952 [dir=none]
	140107092177952 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106854703280 -> 140106722775088 [dir=none]
	140106722775088 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106854703280 -> 140106722776208 [dir=none]
	140106722776208 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106854703280 -> 140107082873920 [dir=none]
	140107082873920 [label="weight
 (192)" fillcolor=orange]
	140106854703280 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854703328 -> 140106854703280
	140106854703328 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854703616 -> 140106854703328
	140106854703616 [label="AddBackward0
------------
alpha: 1"]
	140106854703664 -> 140106854703616
	140106854703664 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854703664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854703856 -> 140106854703664
	140106854703856 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854703856 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854703952 -> 140106854703856
	140106854703952 [label="AddBackward0
------------
alpha: 1"]
	140106854704048 -> 140106854703952
	140106854704048 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106854704192 -> 140106854704048
	140106854704192 -> 140107103438464 [dir=none]
	140107103438464 [label="indices
 (7, 28)" fillcolor=orange]
	140106854704192 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                 4096"]
	140106854704288 -> 140106854704192
	140107071062320 [label="t_enc.text_embedding.weight
 (4096, 192)" fillcolor=lightblue]
	140107071062320 -> 140106854704288
	140106854704288 [label=AccumulateGrad]
	140106854704000 -> 140106854703952
	140106854704000 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551615"]
	140106854704336 -> 140106854704000
	140106854704336 -> 140107103438944 [dir=none]
	140107103438944 [label="indices
 (7)" fillcolor=orange]
	140106854704336 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                   32"]
	140106854704096 -> 140106854704336
	140107071062400 [label="t_enc.lang_embedding.weight
 (32, 192)" fillcolor=lightblue]
	140107071062400 -> 140106854704096
	140106854704096 [label=AccumulateGrad]
	140106854703712 -> 140106854703616
	140106854703712 -> 140106722776448 [dir=none]
	140106722776448 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854703712 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854703760 -> 140106854703712
	140106854703760 -> 140107092178112 [dir=none]
	140107092178112 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854703760 -> 140107082873600 [dir=none]
	140107082873600 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854703760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854704384 -> 140106854703760
	140106854704384 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854704144 -> 140106854704384
	140106854704144 [label=CloneBackward0]
	140106854704624 -> 140106854704144
	140106854704624 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854704720 -> 140106854704624
	140106854704720 [label="AddBackward0
------------
alpha: 1"]
	140106854704816 -> 140106854704720
	140106854704816 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106854704960 -> 140106854704816
	140106854704960 -> 140106722775648 [dir=none]
	140106722775648 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106854704960 -> 140106722776288 [dir=none]
	140106722776288 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106854704960 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854705056 -> 140106854704960
	140106854705056 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106854705200 -> 140106854705056
	140106854705200 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106854705296 -> 140106854705200
	140106854705296 -> 140106722777648 [dir=none]
	140106722777648 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106854705296 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854705392 -> 140106854705296
	140106854705392 -> 140106722777168 [dir=none]
	140106722777168 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106854705392 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106854705488 -> 140106854705392
	140106854705488 -> 140107092174192 [dir=none]
	140107092174192 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106854705488 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106854705584 -> 140106854705488
	140106854705584 [label="AddBackward0
------------
alpha: 1"]
	140106854705680 -> 140106854705584
	140106854705680 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106854705824 -> 140106854705680
	140106854705824 -> 140106722776688 [dir=none]
	140106722776688 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106854705824 -> 140106722777808 [dir=none]
	140106722777808 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106854705824 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854705920 -> 140106854705824
	140106854705920 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854706064 -> 140106854705920
	140106854706064 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854706160 -> 140106854706064
	140106854706160 -> 140106722777888 [dir=none]
	140106722777888 [label="other
 ()" fillcolor=orange]
	140106854706160 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854706256 -> 140106854706160
	140106854706256 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854706352 -> 140106854706256
	140106854706352 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106854706448 -> 140106854706352
	140106854706448 -> 140107092172032 [dir=none]
	140107092172032 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854706448 -> 140107082872400 [dir=none]
	140107082872400 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854706448 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854703664 -> 140106854706448
	140106854706544 -> 140106854706448
	140107082872400 [label="t_enc.encoder.attn_layers.0.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107082872400 -> 140106854706544
	140106854706544 [label=AccumulateGrad]
	140106854706496 -> 140106854706448
	140107082872960 [label="t_enc.encoder.attn_layers.0.conv_q.bias
 (192)" fillcolor=lightblue]
	140107082872960 -> 140106854706496
	140106854706496 [label=AccumulateGrad]
	140106854705872 -> 140106854705824
	140106854705872 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854706208 -> 140106854705872
	140106854706208 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854706400 -> 140106854706208
	140106854706400 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106854706688 -> 140106854706400
	140106854706688 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854706736 -> 140106854706688
	140106854706736 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106854706880 -> 140106854706736
	140106854706880 -> 140107092172032 [dir=none]
	140107092172032 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854706880 -> 140107082873200 [dir=none]
	140107082873200 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854706880 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854703664 -> 140106854706880
	140106854706976 -> 140106854706880
	140107082873200 [label="t_enc.encoder.attn_layers.0.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107082873200 -> 140106854706976
	140106854706976 [label=AccumulateGrad]
	140106854706928 -> 140106854706880
	140107082873280 [label="t_enc.encoder.attn_layers.0.conv_k.bias
 (192)" fillcolor=lightblue]
	140107082873280 -> 140106854706928
	140106854706928 [label=AccumulateGrad]
	140106854705632 -> 140106854705584
	140106854705632 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106854706112 -> 140106854705632
	140106854706112 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106854706592 -> 140106854706112
	140106854706592 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106854706832 -> 140106854706592
	140106854706832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106854706016 -> 140106854706832
	140106854706016 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106854707024 -> 140106854706016
	140106854707024 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106854707216 -> 140106854707024
	140106854707216 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106854707312 -> 140106854707216
	140106854707312 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106854707408 -> 140106854707312
	140106854707408 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106854707504 -> 140106854707408
	140106854707504 -> 140106722779008 [dir=none]
	140106722779008 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106854707504 -> 140106722779248 [dir=none]
	140106722779248 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106854707504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854707600 -> 140106854707504
	140106854707600 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854707744 -> 140106854707600
	140106854707744 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854707840 -> 140106854707744
	140106854707840 -> 140106722778288 [dir=none]
	140106722778288 [label="other
 ()" fillcolor=orange]
	140106854707840 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854706256 -> 140106854707840
	140106854707552 -> 140106854707504
	140106854707552 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106854707936 -> 140106854707552
	140106854707936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106854707648 -> 140106854707936
	140106854707648 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106854708080 -> 140106854707648
	140106854708080 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106854708128 -> 140106854708080
	140106854708128 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106854708272 -> 140106854708128
	140106854708272 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106854708368 -> 140106854708272
	140107934931584 [label="t_enc.encoder.attn_layers.0.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107934931584 -> 140106854708368
	140106854708368 [label=AccumulateGrad]
	140106854705008 -> 140106854704960
	140106854705008 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854705344 -> 140106854705008
	140106854705344 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854705536 -> 140106854705344
	140106854705536 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854705728 -> 140106854705536
	140106854705728 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106854706640 -> 140106854705728
	140106854706640 -> 140107092172032 [dir=none]
	140107092172032 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854706640 -> 140107082873360 [dir=none]
	140107082873360 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854706640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854703664 -> 140106854706640
	140106854707168 -> 140106854706640
	140107082873360 [label="t_enc.encoder.attn_layers.0.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107082873360 -> 140106854707168
	140106854707168 [label=AccumulateGrad]
	140106854706784 -> 140106854706640
	140107082873440 [label="t_enc.encoder.attn_layers.0.conv_v.bias
 (192)" fillcolor=lightblue]
	140107082873440 -> 140106854706784
	140106854706784 [label=AccumulateGrad]
	140106854704768 -> 140106854704720
	140106854704768 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106854705248 -> 140106854704768
	140106854705248 -> 140106722778768 [dir=none]
	140106722778768 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106854705248 -> 140106722780048 [dir=none]
	140106722780048 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106854705248 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854707120 -> 140106854705248
	140106854707120 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106854706304 -> 140106854707120
	140106854706304 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106854707072 -> 140106854706304
	140106854707072 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106854707360 -> 140106854707072
	140106854707360 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106854705776 -> 140106854707360
	140106854705776 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106854707888 -> 140106854705776
	140106854707888 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106854707984 -> 140106854707888
	140106854707984 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106854708176 -> 140106854707984
	140106854708176 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106854708464 -> 140106854708176
	140106854708464 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106854707696 -> 140106854708464
	140106854707696 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106854705296 -> 140106854707696
	140106854705440 -> 140106854705248
	140106854705440 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106854707264 -> 140106854705440
	140106854707264 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106854707792 -> 140106854707264
	140106854707792 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106854708224 -> 140106854707792
	140106854708224 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106854708416 -> 140106854708224
	140106854708416 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106854708512 -> 140106854708416
	140107082873760 [label="t_enc.encoder.attn_layers.0.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107082873760 -> 140106854708512
	140106854708512 [label=AccumulateGrad]
	140106854704240 -> 140106854703760
	140107082873600 [label="t_enc.encoder.attn_layers.0.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107082873600 -> 140106854704240
	140106854704240 [label=AccumulateGrad]
	140106854703808 -> 140106854703760
	140107082873680 [label="t_enc.encoder.attn_layers.0.conv_o.bias
 (192)" fillcolor=lightblue]
	140107082873680 -> 140106854703808
	140106854703808 [label=AccumulateGrad]
	140106854703376 -> 140106854703280
	140107082873920 [label="t_enc.encoder.norm_layers_1.0.gamma
 (192)" fillcolor=lightblue]
	140107082873920 -> 140106854703376
	140106854703376 [label=AccumulateGrad]
	140106854703136 -> 140106854703280
	140107082874080 [label="t_enc.encoder.norm_layers_1.0.beta
 (192)" fillcolor=lightblue]
	140107082874080 -> 140106854703136
	140106854703136 [label=AccumulateGrad]
	140106854703088 -> 140106854702992
	140106854703088 -> 140106722775808 [dir=none]
	140106722775808 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854703088 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854703424 -> 140106854703088
	140106854703424 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854703424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854703568 -> 140106854703424
	140106854703568 -> 140107092178832 [dir=none]
	140107092178832 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106854703568 -> 140107082874320 [dir=none]
	140107082874320 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106854703568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854703904 -> 140106854703568
	140106854703904 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854704672 -> 140106854703904
	140106854704672 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854704672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854704864 -> 140106854704672
	140106854704864 -> 140106722780848 [dir=none]
	140106722780848 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106854704864 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854705152 -> 140106854704864
	140106854705152 -> 140106722780688 [dir=none]
	140106722780688 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106854705152 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106854708032 -> 140106854705152
	140106854708032 -> 140107092175552 [dir=none]
	140107092175552 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106854708032 -> 140107082874160 [dir=none]
	140107082874160 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106854708032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854708560 -> 140106854708032
	140106854708560 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854708656 -> 140106854708560
	140106854708656 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854708656 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854703040 -> 140106854708656
	140106854708320 -> 140106854708032
	140107082874160 [label="t_enc.encoder.ffn_layers.0.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107082874160 -> 140106854708320
	140106854708320 [label=AccumulateGrad]
	140106854704480 -> 140106854708032
	140107082874240 [label="t_enc.encoder.ffn_layers.0.conv_1.bias
 (768)" fillcolor=lightblue]
	140107082874240 -> 140106854704480
	140106854704480 [label=AccumulateGrad]
	140106854703472 -> 140106854703568
	140107082874320 [label="t_enc.encoder.ffn_layers.0.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107082874320 -> 140106854703472
	140106854703472 [label=AccumulateGrad]
	140106854703184 -> 140106854703568
	140107082874560 [label="t_enc.encoder.ffn_layers.0.conv_2.bias
 (192)" fillcolor=lightblue]
	140107082874560 -> 140106854703184
	140106854703184 [label=AccumulateGrad]
	140106854702752 -> 140106854702656
	140107082874640 [label="t_enc.encoder.norm_layers_2.0.gamma
 (192)" fillcolor=lightblue]
	140107082874640 -> 140106854702752
	140106854702752 [label=AccumulateGrad]
	140106854702512 -> 140106854702656
	140107082874800 [label="t_enc.encoder.norm_layers_2.0.beta
 (192)" fillcolor=lightblue]
	140107082874800 -> 140106854702512
	140106854702512 [label=AccumulateGrad]
	140106854702464 -> 140106854702368
	140106854702464 -> 140106722781328 [dir=none]
	140106722781328 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854702464 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854702800 -> 140106854702464
	140106854702800 -> 140107086974944 [dir=none]
	140107086974944 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854702800 -> 140107077223664 [dir=none]
	140107077223664 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854702800 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854702944 -> 140106854702800
	140106854702944 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854704432 -> 140106854702944
	140106854704432 [label=CloneBackward0]
	140106854704528 -> 140106854704432
	140106854704528 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854707456 -> 140106854704528
	140106854707456 [label="AddBackward0
------------
alpha: 1"]
	140106854705104 -> 140106854707456
	140106854705104 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106854701504 -> 140106854705104
	140106854701504 -> 140106722779568 [dir=none]
	140106722779568 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106854701504 -> 140106722779328 [dir=none]
	140106722779328 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106854701504 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854708848 -> 140106854701504
	140106854708848 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106854708992 -> 140106854708848
	140106854708992 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106854709088 -> 140106854708992
	140106854709088 -> 140106722781968 [dir=none]
	140106722781968 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106854709088 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854709184 -> 140106854709088
	140106854709184 -> 140106722781648 [dir=none]
	140106722781648 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106854709184 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106854709280 -> 140106854709184
	140106854709280 -> 140107092179232 [dir=none]
	140107092179232 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106854709280 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106854709376 -> 140106854709280
	140106854709376 [label="AddBackward0
------------
alpha: 1"]
	140106854709472 -> 140106854709376
	140106854709472 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106854709616 -> 140106854709472
	140106854709616 -> 140106722779168 [dir=none]
	140106722779168 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106854709616 -> 140106722782048 [dir=none]
	140106722782048 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106854709616 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854709712 -> 140106854709616
	140106854709712 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854709856 -> 140106854709712
	140106854709856 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854709952 -> 140106854709856
	140106854709952 -> 140106722782128 [dir=none]
	140106722782128 [label="other
 ()" fillcolor=orange]
	140106854709952 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854710048 -> 140106854709952
	140106854710048 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854710144 -> 140106854710048
	140106854710144 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106854710240 -> 140106854710144
	140106854710240 -> 140107092179152 [dir=none]
	140107092179152 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854710240 -> 140107077222544 [dir=none]
	140107077222544 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854710240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854702416 -> 140106854710240
	140106854710336 -> 140106854710240
	140107077222544 [label="t_enc.encoder.attn_layers.1.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077222544 -> 140106854710336
	140106854710336 [label=AccumulateGrad]
	140106854710288 -> 140106854710240
	140107077222624 [label="t_enc.encoder.attn_layers.1.conv_q.bias
 (192)" fillcolor=lightblue]
	140107077222624 -> 140106854710288
	140106854710288 [label=AccumulateGrad]
	140106854709664 -> 140106854709616
	140106854709664 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854710000 -> 140106854709664
	140106854710000 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854710192 -> 140106854710000
	140106854710192 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106854710432 -> 140106854710192
	140106854710432 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854710480 -> 140106854710432
	140106854710480 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106854710624 -> 140106854710480
	140106854710624 -> 140107092179152 [dir=none]
	140107092179152 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854710624 -> 140107077223104 [dir=none]
	140107077223104 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854710624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854702416 -> 140106854710624
	140106854710720 -> 140106854710624
	140107077223104 [label="t_enc.encoder.attn_layers.1.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077223104 -> 140106854710720
	140106854710720 [label=AccumulateGrad]
	140106854710672 -> 140106854710624
	140107077223264 [label="t_enc.encoder.attn_layers.1.conv_k.bias
 (192)" fillcolor=lightblue]
	140107077223264 -> 140106854710672
	140106854710672 [label=AccumulateGrad]
	140106854709424 -> 140106854709376
	140106854709424 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106854709904 -> 140106854709424
	140106854709904 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106854710384 -> 140106854709904
	140106854710384 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106854710576 -> 140106854710384
	140106854710576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106854709808 -> 140106854710576
	140106854709808 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106854710816 -> 140106854709808
	140106854710816 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106854710912 -> 140106854710816
	140106854710912 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106854711008 -> 140106854710912
	140106854711008 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106854711152 -> 140106854711008
	140106854711152 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106854711248 -> 140106854711152
	140106854711248 -> 140106722783648 [dir=none]
	140106722783648 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106854711248 -> 140106722783408 [dir=none]
	140106722783408 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106854711248 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854709568 -> 140106854711248
	140106854709568 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813161728 -> 140106854709568
	140106813161728 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813161824 -> 140106813161728
	140106813161824 -> 140106722784528 [dir=none]
	140106722784528 [label="other
 ()" fillcolor=orange]
	140106813161824 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854710048 -> 140106813161824
	140106813161584 -> 140106854711248
	140106813161584 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106813161920 -> 140106813161584
	140106813161920 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106813161632 -> 140106813161920
	140106813161632 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813162064 -> 140106813161632
	140106813162064 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813162112 -> 140106813162064
	140106813162112 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813162256 -> 140106813162112
	140106813162256 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813162352 -> 140106813162256
	140107082873840 [label="t_enc.encoder.attn_layers.1.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107082873840 -> 140106813162352
	140106813162352 [label=AccumulateGrad]
	140106854708800 -> 140106854701504
	140106854708800 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854709136 -> 140106854708800
	140106854709136 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854709328 -> 140106854709136
	140106854709328 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854709520 -> 140106854709328
	140106854709520 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106854709760 -> 140106854709520
	140106854709760 -> 140107092179152 [dir=none]
	140107092179152 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854709760 -> 140107077223344 [dir=none]
	140107077223344 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854709760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854702416 -> 140106854709760
	140106854710768 -> 140106854709760
	140107077223344 [label="t_enc.encoder.attn_layers.1.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077223344 -> 140106854710768
	140106854710768 [label=AccumulateGrad]
	140106854710528 -> 140106854709760
	140107077223584 [label="t_enc.encoder.attn_layers.1.conv_v.bias
 (192)" fillcolor=lightblue]
	140107077223584 -> 140106854710528
	140106854710528 [label=AccumulateGrad]
	140106854708608 -> 140106854707456
	140106854708608 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106854709040 -> 140106854708608
	140106854709040 -> 140106722782448 [dir=none]
	140106722782448 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106854709040 -> 140106722785888 [dir=none]
	140106722785888 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106854709040 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106854708896 -> 140106854709040
	140106854708896 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106854710864 -> 140106854708896
	140106854710864 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106854711056 -> 140106854710864
	140106854711056 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106854711200 -> 140106854711056
	140106854711200 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813161872 -> 140106854711200
	140106813161872 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813161968 -> 140106813161872
	140106813161968 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813162160 -> 140106813161968
	140106813162160 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106813162448 -> 140106813162160
	140106813162448 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106813161680 -> 140106813162448
	140106813161680 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813162544 -> 140106813161680
	140106813162544 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106854709088 -> 140106813162544
	140106854709232 -> 140106854709040
	140106854709232 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106854710096 -> 140106854709232
	140106854710096 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106854708944 -> 140106854710096
	140106854708944 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813162304 -> 140106854708944
	140106813162304 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813162496 -> 140106813162304
	140106813162496 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813162592 -> 140106813162496
	140107077224064 [label="t_enc.encoder.attn_layers.1.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107077224064 -> 140106813162592
	140106813162592 [label=AccumulateGrad]
	140106854702896 -> 140106854702800
	140107077223664 [label="t_enc.encoder.attn_layers.1.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077223664 -> 140106854702896
	140106854702896 [label=AccumulateGrad]
	140106854702560 -> 140106854702800
	140107077223904 [label="t_enc.encoder.attn_layers.1.conv_o.bias
 (192)" fillcolor=lightblue]
	140107077223904 -> 140106854702560
	140106854702560 [label=AccumulateGrad]
	140106854702128 -> 140106854702032
	140107077224384 [label="t_enc.encoder.norm_layers_1.1.gamma
 (192)" fillcolor=lightblue]
	140107077224384 -> 140106854702128
	140106854702128 [label=AccumulateGrad]
	140106854701888 -> 140106854702032
	140107077224544 [label="t_enc.encoder.norm_layers_1.1.beta
 (192)" fillcolor=lightblue]
	140107077224544 -> 140106854701888
	140106854701888 [label=AccumulateGrad]
	140106854701840 -> 140106854701744
	140106854701840 -> 140106722784768 [dir=none]
	140106722784768 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854701840 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854702176 -> 140106854701840
	140106854702176 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854702176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854702320 -> 140106854702176
	140106854702320 -> 140107086972384 [dir=none]
	140107086972384 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106854702320 -> 140107077225344 [dir=none]
	140107077225344 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106854702320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854702608 -> 140106854702320
	140106854702608 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854704912 -> 140106854702608
	140106854704912 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854704912 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854708752 -> 140106854704912
	140106854708752 -> 140106722786048 [dir=none]
	140106722786048 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106854708752 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854710960 -> 140106854708752
	140106854710960 -> 140106722785968 [dir=none]
	140106722785968 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106854710960 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106854703520 -> 140106854710960
	140106854703520 -> 140107086974144 [dir=none]
	140107086974144 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106854703520 -> 140107077224784 [dir=none]
	140107077224784 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106854703520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106813162640 -> 140106854703520
	140106813162640 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106813162736 -> 140106813162640
	140106813162736 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106813162736 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854701792 -> 140106813162736
	140106813162400 -> 140106854703520
	140107077224784 [label="t_enc.encoder.ffn_layers.1.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107077224784 -> 140106813162400
	140106813162400 [label=AccumulateGrad]
	140106813162016 -> 140106854703520
	140107077225264 [label="t_enc.encoder.ffn_layers.1.conv_1.bias
 (768)" fillcolor=lightblue]
	140107077225264 -> 140106813162016
	140106813162016 [label=AccumulateGrad]
	140106854702224 -> 140106854702320
	140107077225344 [label="t_enc.encoder.ffn_layers.1.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107077225344 -> 140106854702224
	140106854702224 [label=AccumulateGrad]
	140106854701936 -> 140106854702320
	140107077225504 [label="t_enc.encoder.ffn_layers.1.conv_2.bias
 (192)" fillcolor=lightblue]
	140107077225504 -> 140106854701936
	140106854701936 [label=AccumulateGrad]
	140106854701360 -> 140106854701264
	140107077225584 [label="t_enc.encoder.norm_layers_2.1.gamma
 (192)" fillcolor=lightblue]
	140107077225584 -> 140106854701360
	140106854701360 [label=AccumulateGrad]
	140106854701120 -> 140106854701264
	140107077225664 [label="t_enc.encoder.norm_layers_2.1.beta
 (192)" fillcolor=lightblue]
	140107077225664 -> 140106854701120
	140106854701120 [label=AccumulateGrad]
	140106854701072 -> 140106854700976
	140106854701072 -> 140106722786128 [dir=none]
	140106722786128 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854701072 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854701552 -> 140106854701072
	140106854701552 -> 140107082859840 [dir=none]
	140107082859840 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854701552 -> 140107077226624 [dir=none]
	140107077226624 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854701552 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854701696 -> 140106854701552
	140106854701696 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854702848 -> 140106854701696
	140106854702848 [label=CloneBackward0]
	140106854703232 -> 140106854702848
	140106854703232 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106854708704 -> 140106854703232
	140106854708704 [label="AddBackward0
------------
alpha: 1"]
	140106813161776 -> 140106854708704
	140106813161776 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813161536 -> 140106813161776
	140106813161536 -> 140106722782528 [dir=none]
	140106722782528 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106813161536 -> 140106722781488 [dir=none]
	140106722781488 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106813161536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813162928 -> 140106813161536
	140106813162928 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813163072 -> 140106813162928
	140106813163072 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813163168 -> 140106813163072
	140106813163168 -> 140106722786208 [dir=none]
	140106722786208 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106813163168 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813163264 -> 140106813163168
	140106813163264 -> 140106722786368 [dir=none]
	140106722786368 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106813163264 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106813163360 -> 140106813163264
	140106813163360 -> 140107086977344 [dir=none]
	140107086977344 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106813163360 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106813163456 -> 140106813163360
	140106813163456 [label="AddBackward0
------------
alpha: 1"]
	140106813163552 -> 140106813163456
	140106813163552 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106813163696 -> 140106813163552
	140106813163696 -> 140106722786288 [dir=none]
	140106722786288 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106813163696 -> 140106722786448 [dir=none]
	140106722786448 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813163696 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813163792 -> 140106813163696
	140106813163792 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813163936 -> 140106813163792
	140106813163936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813164032 -> 140106813163936
	140106813164032 -> 140106722786688 [dir=none]
	140106722786688 [label="other
 ()" fillcolor=orange]
	140106813164032 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813164128 -> 140106813164032
	140106813164128 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813164224 -> 140106813164128
	140106813164224 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813164320 -> 140106813164224
	140106813164320 -> 140107086971344 [dir=none]
	140107086971344 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813164320 -> 140107077225744 [dir=none]
	140107077225744 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813164320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854701024 -> 140106813164320
	140106813164416 -> 140106813164320
	140107077225744 [label="t_enc.encoder.attn_layers.2.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077225744 -> 140106813164416
	140106813164416 [label=AccumulateGrad]
	140106813164368 -> 140106813164320
	140107077225824 [label="t_enc.encoder.attn_layers.2.conv_q.bias
 (192)" fillcolor=lightblue]
	140107077225824 -> 140106813164368
	140106813164368 [label=AccumulateGrad]
	140106813163744 -> 140106813163696
	140106813163744 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813164080 -> 140106813163744
	140106813164080 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813164272 -> 140106813164080
	140106813164272 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813164512 -> 140106813164272
	140106813164512 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813164560 -> 140106813164512
	140106813164560 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813164704 -> 140106813164560
	140106813164704 -> 140107086971344 [dir=none]
	140107086971344 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813164704 -> 140107077225904 [dir=none]
	140107077225904 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813164704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854701024 -> 140106813164704
	140106813164800 -> 140106813164704
	140107077225904 [label="t_enc.encoder.attn_layers.2.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077225904 -> 140106813164800
	140106813164800 [label=AccumulateGrad]
	140106813164752 -> 140106813164704
	140107077226064 [label="t_enc.encoder.attn_layers.2.conv_k.bias
 (192)" fillcolor=lightblue]
	140107077226064 -> 140106813164752
	140106813164752 [label=AccumulateGrad]
	140106813163504 -> 140106813163456
	140106813163504 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106813163984 -> 140106813163504
	140106813163984 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106813164464 -> 140106813163984
	140106813164464 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813164656 -> 140106813164464
	140106813164656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813163888 -> 140106813164656
	140106813163888 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106813164896 -> 140106813163888
	140106813164896 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106813164992 -> 140106813164896
	140106813164992 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106813165088 -> 140106813164992
	140106813165088 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106854711104 -> 140106813165088
	140106854711104 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106813165280 -> 140106854711104
	140106813165280 -> 140106722785808 [dir=none]
	140106722785808 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106813165280 -> 140106722787008 [dir=none]
	140106722787008 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813165280 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813165376 -> 140106813165280
	140106813165376 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813165520 -> 140106813165376
	140106813165520 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813165616 -> 140106813165520
	140106813165616 -> 140106722787248 [dir=none]
	140106722787248 [label="other
 ()" fillcolor=orange]
	140106813165616 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813164128 -> 140106813165616
	140106813165328 -> 140106813165280
	140106813165328 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106813165712 -> 140106813165328
	140106813165712 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106813165424 -> 140106813165712
	140106813165424 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813165856 -> 140106813165424
	140106813165856 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813165904 -> 140106813165856
	140106813165904 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813166048 -> 140106813165904
	140106813166048 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813166144 -> 140106813166048
	140107077224304 [label="t_enc.encoder.attn_layers.2.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107077224304 -> 140106813166144
	140106813166144 [label=AccumulateGrad]
	140106813162880 -> 140106813161536
	140106813162880 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813163216 -> 140106813162880
	140106813163216 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813163408 -> 140106813163216
	140106813163408 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813163600 -> 140106813163408
	140106813163600 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813163840 -> 140106813163600
	140106813163840 -> 140107086971344 [dir=none]
	140107086971344 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813163840 -> 140107077226144 [dir=none]
	140107077226144 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813163840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854701024 -> 140106813163840
	140106813164848 -> 140106813163840
	140107077226144 [label="t_enc.encoder.attn_layers.2.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077226144 -> 140106813164848
	140106813164848 [label=AccumulateGrad]
	140106813164608 -> 140106813163840
	140107077226224 [label="t_enc.encoder.attn_layers.2.conv_v.bias
 (192)" fillcolor=lightblue]
	140107077226224 -> 140106813164608
	140106813164608 [label=AccumulateGrad]
	140106813162688 -> 140106854708704
	140106813162688 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813163120 -> 140106813162688
	140106813163120 -> 140106722786928 [dir=none]
	140106722786928 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106813163120 -> 140106722771008 [dir=none]
	140106722771008 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106813163120 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813162976 -> 140106813163120
	140106813162976 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813164944 -> 140106813162976
	140106813164944 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813165184 -> 140106813164944
	140106813165184 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106813165136 -> 140106813165184
	140106813165136 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813165664 -> 140106813165136
	140106813165664 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813165760 -> 140106813165664
	140106813165760 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813166000 -> 140106813165760
	140106813166000 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106813166192 -> 140106813166000
	140106813166192 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106813166288 -> 140106813166192
	140106813166288 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813166384 -> 140106813166288
	140106813166384 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106813163168 -> 140106813166384
	140106813163312 -> 140106813163120
	140106813163312 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106813165232 -> 140106813163312
	140106813165232 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106813165808 -> 140106813165232
	140106813165808 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813166240 -> 140106813165808
	140106813166240 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813166336 -> 140106813166240
	140106813166336 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813166432 -> 140106813166336
	140107077226864 [label="t_enc.encoder.attn_layers.2.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107077226864 -> 140106813166432
	140106813166432 [label=AccumulateGrad]
	140106854701648 -> 140106854701552
	140107077226624 [label="t_enc.encoder.attn_layers.2.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077226624 -> 140106854701648
	140106854701648 [label=AccumulateGrad]
	140106854701168 -> 140106854701552
	140107077226704 [label="t_enc.encoder.attn_layers.2.conv_o.bias
 (192)" fillcolor=lightblue]
	140107077226704 -> 140106854701168
	140106854701168 [label=AccumulateGrad]
	140106854700736 -> 140106854700640
	140107077227984 [label="t_enc.encoder.norm_layers_1.2.gamma
 (192)" fillcolor=lightblue]
	140107077227984 -> 140106854700736
	140106854700736 [label=AccumulateGrad]
	140106854700496 -> 140106854700640
	140107077228064 [label="t_enc.encoder.norm_layers_1.2.beta
 (192)" fillcolor=lightblue]
	140107077228064 -> 140106854700496
	140106854700496 [label=AccumulateGrad]
	140106854700448 -> 140106854700352
	140106854700448 -> 140106722787168 [dir=none]
	140106722787168 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854700448 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854700784 -> 140106854700448
	140106854700784 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854700784 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854700928 -> 140106854700784
	140106854700928 -> 140107086978384 [dir=none]
	140107086978384 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106854700928 -> 140107077228704 [dir=none]
	140107077228704 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106854700928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854701216 -> 140106854700928
	140106854701216 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854701984 -> 140106854701216
	140106854701984 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854701984 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854702272 -> 140106854701984
	140106854702272 -> 140106722783888 [dir=none]
	140106722783888 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106854702272 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813165040 -> 140106854702272
	140106813165040 -> 140106722786848 [dir=none]
	140106722786848 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106813165040 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106813164176 -> 140106813165040
	140106813164176 -> 140107086986704 [dir=none]
	140107086986704 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106813164176 -> 140107077228144 [dir=none]
	140107077228144 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106813164176 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106813166480 -> 140106813164176
	140106813166480 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106813166576 -> 140106813166480
	140106813166576 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106813166576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854700400 -> 140106813166576
	140106813165472 -> 140106813164176
	140107077228144 [label="t_enc.encoder.ffn_layers.2.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107077228144 -> 140106813165472
	140106813165472 [label=AccumulateGrad]
	140106813162208 -> 140106813164176
	140107077228384 [label="t_enc.encoder.ffn_layers.2.conv_1.bias
 (768)" fillcolor=lightblue]
	140107077228384 -> 140106813162208
	140106813162208 [label=AccumulateGrad]
	140106854700832 -> 140106854700928
	140107077228704 [label="t_enc.encoder.ffn_layers.2.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107077228704 -> 140106854700832
	140106854700832 [label=AccumulateGrad]
	140106854700544 -> 140106854700928
	140107077229104 [label="t_enc.encoder.ffn_layers.2.conv_2.bias
 (192)" fillcolor=lightblue]
	140107077229104 -> 140106854700544
	140106854700544 [label=AccumulateGrad]
	140106854700112 -> 140106854700016
	140107077229184 [label="t_enc.encoder.norm_layers_2.2.gamma
 (192)" fillcolor=lightblue]
	140107077229184 -> 140106854700112
	140106854700112 [label=AccumulateGrad]
	140106854699872 -> 140106854700016
	140107077229504 [label="t_enc.encoder.norm_layers_2.2.beta
 (192)" fillcolor=lightblue]
	140107077229504 -> 140106854699872
	140106854699872 [label=AccumulateGrad]
	140106854699824 -> 140106854699728
	140106854699824 -> 140106722775888 [dir=none]
	140106722775888 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854699824 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854700160 -> 140106854699824
	140106854700160 -> 140107082864240 [dir=none]
	140107082864240 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854700160 -> 140107077230384 [dir=none]
	140107077230384 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854700160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854700304 -> 140106854700160
	140106854700304 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854701600 -> 140106854700304
	140106854701600 [label=CloneBackward0]
	140106854704576 -> 140106854701600
	140106854704576 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813165568 -> 140106854704576
	140106813165568 [label="AddBackward0
------------
alpha: 1"]
	140106813163024 -> 140106813165568
	140106813163024 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813166096 -> 140106813163024
	140106813166096 -> 140106722784608 [dir=none]
	140106722784608 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106813166096 -> 140106722785488 [dir=none]
	140106722785488 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106813166096 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813166768 -> 140106813166096
	140106813166768 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813166912 -> 140106813166768
	140106813166912 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813167008 -> 140106813166912
	140106813167008 -> 140106722777728 [dir=none]
	140106722777728 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106813167008 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813167104 -> 140106813167008
	140106813167104 -> 140106722785168 [dir=none]
	140106722785168 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106813167104 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106813167200 -> 140106813167104
	140106813167200 -> 140107082860560 [dir=none]
	140107082860560 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106813167200 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106813167296 -> 140106813167200
	140106813167296 [label="AddBackward0
------------
alpha: 1"]
	140106813167392 -> 140106813167296
	140106813167392 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106813167536 -> 140106813167392
	140106813167536 -> 140106722782208 [dir=none]
	140106722782208 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106813167536 -> 140106722786528 [dir=none]
	140106722786528 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813167536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813167632 -> 140106813167536
	140106813167632 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813167776 -> 140106813167632
	140106813167776 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813167872 -> 140106813167776
	140106813167872 -> 140106722784048 [dir=none]
	140106722784048 [label="other
 ()" fillcolor=orange]
	140106813167872 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813167968 -> 140106813167872
	140106813167968 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813168064 -> 140106813167968
	140106813168064 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813168160 -> 140106813168064
	140106813168160 -> 140107082859360 [dir=none]
	140107082859360 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813168160 -> 140107077229584 [dir=none]
	140107077229584 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813168160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854699776 -> 140106813168160
	140106813168256 -> 140106813168160
	140107077229584 [label="t_enc.encoder.attn_layers.3.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077229584 -> 140106813168256
	140106813168256 [label=AccumulateGrad]
	140106813168208 -> 140106813168160
	140107077229664 [label="t_enc.encoder.attn_layers.3.conv_q.bias
 (192)" fillcolor=lightblue]
	140107077229664 -> 140106813168208
	140106813168208 [label=AccumulateGrad]
	140106813167584 -> 140106813167536
	140106813167584 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813167920 -> 140106813167584
	140106813167920 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813168112 -> 140106813167920
	140106813168112 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813168352 -> 140106813168112
	140106813168352 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813168400 -> 140106813168352
	140106813168400 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813168544 -> 140106813168400
	140106813168544 -> 140107082859360 [dir=none]
	140107082859360 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813168544 -> 140107077229984 [dir=none]
	140107077229984 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813168544 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854699776 -> 140106813168544
	140106813168640 -> 140106813168544
	140107077229984 [label="t_enc.encoder.attn_layers.3.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077229984 -> 140106813168640
	140106813168640 [label=AccumulateGrad]
	140106813168592 -> 140106813168544
	140107077230064 [label="t_enc.encoder.attn_layers.3.conv_k.bias
 (192)" fillcolor=lightblue]
	140107077230064 -> 140106813168592
	140106813168592 [label=AccumulateGrad]
	140106813167344 -> 140106813167296
	140106813167344 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106813167824 -> 140106813167344
	140106813167824 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106813168304 -> 140106813167824
	140106813168304 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813168496 -> 140106813168304
	140106813168496 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813167728 -> 140106813168496
	140106813167728 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106813168736 -> 140106813167728
	140106813168736 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106813168832 -> 140106813168736
	140106813168832 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106813168928 -> 140106813168832
	140106813168928 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106813169024 -> 140106813168928
	140106813169024 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106813169120 -> 140106813169024
	140106813169120 -> 140106722786768 [dir=none]
	140106722786768 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106813169120 -> 140106722786608 [dir=none]
	140106722786608 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813169120 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813169216 -> 140106813169120
	140106813169216 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813169360 -> 140106813169216
	140106813169360 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813169456 -> 140106813169360
	140106813169456 -> 140106814112048 [dir=none]
	140106814112048 [label="other
 ()" fillcolor=orange]
	140106813169456 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813167968 -> 140106813169456
	140106813169168 -> 140106813169120
	140106813169168 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106813169552 -> 140106813169168
	140106813169552 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106813169264 -> 140106813169552
	140106813169264 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813169696 -> 140106813169264
	140106813169696 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813169744 -> 140106813169696
	140106813169744 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813169888 -> 140106813169744
	140106813169888 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813169984 -> 140106813169888
	140107077227824 [label="t_enc.encoder.attn_layers.3.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107077227824 -> 140106813169984
	140106813169984 [label=AccumulateGrad]
	140106813166720 -> 140106813166096
	140106813166720 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813167056 -> 140106813166720
	140106813167056 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813167248 -> 140106813167056
	140106813167248 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813167440 -> 140106813167248
	140106813167440 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813167680 -> 140106813167440
	140106813167680 -> 140107082859360 [dir=none]
	140107082859360 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813167680 -> 140107077230144 [dir=none]
	140107077230144 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813167680 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854699776 -> 140106813167680
	140106813168688 -> 140106813167680
	140107077230144 [label="t_enc.encoder.attn_layers.3.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077230144 -> 140106813168688
	140106813168688 [label=AccumulateGrad]
	140106813168448 -> 140106813167680
	140107077230224 [label="t_enc.encoder.attn_layers.3.conv_v.bias
 (192)" fillcolor=lightblue]
	140107077230224 -> 140106813168448
	140106813168448 [label=AccumulateGrad]
	140106813166528 -> 140106813165568
	140106813166528 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813166960 -> 140106813166528
	140106813166960 -> 140106814111968 [dir=none]
	140106814111968 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106813166960 -> 140106814112368 [dir=none]
	140106814112368 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106813166960 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813166816 -> 140106813166960
	140106813166816 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813168784 -> 140106813166816
	140106813168784 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813168976 -> 140106813168784
	140106813168976 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106813167488 -> 140106813168976
	140106813167488 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813169504 -> 140106813167488
	140106813169504 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813169600 -> 140106813169504
	140106813169600 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813163648 -> 140106813169600
	140106813163648 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106813168016 -> 140106813163648
	140106813168016 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106813170080 -> 140106813168016
	140106813170080 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813169312 -> 140106813170080
	140106813169312 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106813167008 -> 140106813169312
	140106813167152 -> 140106813166960
	140106813167152 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106813169072 -> 140106813167152
	140106813169072 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106813169648 -> 140106813169072
	140106813169648 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813169840 -> 140106813169648
	140106813169840 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813170032 -> 140106813169840
	140106813170032 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813170128 -> 140106813170032
	140107077230544 [label="t_enc.encoder.attn_layers.3.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107077230544 -> 140106813170128
	140106813170128 [label=AccumulateGrad]
	140106854700256 -> 140106854700160
	140107077230384 [label="t_enc.encoder.attn_layers.3.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077230384 -> 140106854700256
	140106854700256 [label=AccumulateGrad]
	140106854699920 -> 140106854700160
	140107077230464 [label="t_enc.encoder.attn_layers.3.conv_o.bias
 (192)" fillcolor=lightblue]
	140107077230464 -> 140106854699920
	140106854699920 [label=AccumulateGrad]
	140106854699488 -> 140106854699392
	140107077230704 [label="t_enc.encoder.norm_layers_1.3.gamma
 (192)" fillcolor=lightblue]
	140107077230704 -> 140106854699488
	140106854699488 [label=AccumulateGrad]
	140106854699248 -> 140106854699392
	140107077230784 [label="t_enc.encoder.norm_layers_1.3.beta
 (192)" fillcolor=lightblue]
	140107077230784 -> 140106854699248
	140106854699248 [label=AccumulateGrad]
	140106854699200 -> 140106854699104
	140106854699200 -> 140106722787088 [dir=none]
	140106722787088 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854699200 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854699536 -> 140106854699200
	140106854699536 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854699536 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854699680 -> 140106854699536
	140106854699680 -> 140107082864560 [dir=none]
	140107082864560 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106854699680 -> 140107077231504 [dir=none]
	140107077231504 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106854699680 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854699968 -> 140106854699680
	140106854699968 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854700592 -> 140106854699968
	140106854700592 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854700592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813166672 -> 140106854700592
	140106813166672 -> 140106814112448 [dir=none]
	140106814112448 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106813166672 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813168880 -> 140106813166672
	140106813168880 -> 140106814112208 [dir=none]
	140106814112208 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106813168880 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106813165952 -> 140106813168880
	140106813165952 -> 140107082863520 [dir=none]
	140107082863520 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106813165952 -> 140107077230864 [dir=none]
	140107077230864 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106813165952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106813170176 -> 140106813165952
	140106813170176 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106813170272 -> 140106813170176
	140106813170272 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106813170272 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854699152 -> 140106813170272
	140106813169936 -> 140106813165952
	140107077230864 [label="t_enc.encoder.ffn_layers.3.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107077230864 -> 140106813169936
	140106813169936 [label=AccumulateGrad]
	140106813162784 -> 140106813165952
	140107077231344 [label="t_enc.encoder.ffn_layers.3.conv_1.bias
 (768)" fillcolor=lightblue]
	140107077231344 -> 140106813162784
	140106813162784 [label=AccumulateGrad]
	140106854699584 -> 140106854699680
	140107077231504 [label="t_enc.encoder.ffn_layers.3.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107077231504 -> 140106854699584
	140106854699584 [label=AccumulateGrad]
	140106854699296 -> 140106854699680
	140107077231664 [label="t_enc.encoder.ffn_layers.3.conv_2.bias
 (192)" fillcolor=lightblue]
	140107077231664 -> 140106854699296
	140106854699296 [label=AccumulateGrad]
	140106854698864 -> 140106854698768
	140107077231744 [label="t_enc.encoder.norm_layers_2.3.gamma
 (192)" fillcolor=lightblue]
	140107077231744 -> 140106854698864
	140106854698864 [label=AccumulateGrad]
	140106854698624 -> 140106854698768
	140107077232304 [label="t_enc.encoder.norm_layers_2.3.beta
 (192)" fillcolor=lightblue]
	140107077232304 -> 140106854698624
	140106854698624 [label=AccumulateGrad]
	140106854698576 -> 140106854698480
	140106854698576 -> 140106814112528 [dir=none]
	140106814112528 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854698576 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854698912 -> 140106854698576
	140106854698912 -> 140107077224464 [dir=none]
	140107077224464 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854698912 -> 140107077233184 [dir=none]
	140107077233184 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854698912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854699056 -> 140106854698912
	140106854699056 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854700208 -> 140106854699056
	140106854700208 [label=CloneBackward0]
	140106854700880 -> 140106854700208
	140106854700880 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813169408 -> 140106854700880
	140106813169408 [label="AddBackward0
------------
alpha: 1"]
	140106813166864 -> 140106813169408
	140106813166864 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813169792 -> 140106813166864
	140106813169792 -> 140106814112128 [dir=none]
	140106814112128 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106813169792 -> 140106814111808 [dir=none]
	140106814111808 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106813169792 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813170464 -> 140106813169792
	140106813170464 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813170608 -> 140106813170464
	140106813170608 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813170704 -> 140106813170608
	140106813170704 -> 140106814112608 [dir=none]
	140106814112608 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106813170704 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813170800 -> 140106813170704
	140106813170800 -> 140106814112768 [dir=none]
	140106814112768 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106813170800 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106813170896 -> 140106813170800
	140106813170896 -> 140107082866000 [dir=none]
	140107082866000 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106813170896 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106813170992 -> 140106813170896
	140106813170992 [label="AddBackward0
------------
alpha: 1"]
	140106813171088 -> 140106813170992
	140106813171088 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106813171232 -> 140106813171088
	140106813171232 -> 140106814112688 [dir=none]
	140106814112688 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106813171232 -> 140106814112848 [dir=none]
	140106814112848 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813171232 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813171328 -> 140106813171232
	140106813171328 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813171472 -> 140106813171328
	140106813171472 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813171568 -> 140106813171472
	140106813171568 -> 140106814112928 [dir=none]
	140106814112928 [label="other
 ()" fillcolor=orange]
	140106813171568 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813171664 -> 140106813171568
	140106813171664 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813171760 -> 140106813171664
	140106813171760 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813171856 -> 140106813171760
	140106813171856 -> 140107082861200 [dir=none]
	140107082861200 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813171856 -> 140107077232464 [dir=none]
	140107077232464 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813171856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854698528 -> 140106813171856
	140106813171952 -> 140106813171856
	140107077232464 [label="t_enc.encoder.attn_layers.4.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077232464 -> 140106813171952
	140106813171952 [label=AccumulateGrad]
	140106813171904 -> 140106813171856
	140107077232544 [label="t_enc.encoder.attn_layers.4.conv_q.bias
 (192)" fillcolor=lightblue]
	140107077232544 -> 140106813171904
	140106813171904 [label=AccumulateGrad]
	140106813171280 -> 140106813171232
	140106813171280 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813171616 -> 140106813171280
	140106813171616 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813171808 -> 140106813171616
	140106813171808 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813172048 -> 140106813171808
	140106813172048 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813172096 -> 140106813172048
	140106813172096 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813172240 -> 140106813172096
	140106813172240 -> 140107082861200 [dir=none]
	140107082861200 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813172240 -> 140107077232624 [dir=none]
	140107077232624 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813172240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854698528 -> 140106813172240
	140106813172336 -> 140106813172240
	140107077232624 [label="t_enc.encoder.attn_layers.4.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077232624 -> 140106813172336
	140106813172336 [label=AccumulateGrad]
	140106813172288 -> 140106813172240
	140107077232784 [label="t_enc.encoder.attn_layers.4.conv_k.bias
 (192)" fillcolor=lightblue]
	140107077232784 -> 140106813172288
	140106813172288 [label=AccumulateGrad]
	140106813171040 -> 140106813170992
	140106813171040 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106813171520 -> 140106813171040
	140106813171520 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106813172000 -> 140106813171520
	140106813172000 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813172192 -> 140106813172000
	140106813172192 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813171424 -> 140106813172192
	140106813171424 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106813172432 -> 140106813171424
	140106813172432 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106813172528 -> 140106813172432
	140106813172528 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106813172624 -> 140106813172528
	140106813172624 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106813172720 -> 140106813172624
	140106813172720 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106813172816 -> 140106813172720
	140106813172816 -> 140106814113168 [dir=none]
	140106814113168 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106813172816 -> 140106814113488 [dir=none]
	140106814113488 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813172816 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813172960 -> 140106813172816
	140106813172960 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813173104 -> 140106813172960
	140106813173104 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813173200 -> 140106813173104
	140106813173200 -> 140106814113248 [dir=none]
	140106814113248 [label="other
 ()" fillcolor=orange]
	140106813173200 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813171664 -> 140106813173200
	140106813172864 -> 140106813172816
	140106813172864 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106813173296 -> 140106813172864
	140106813173296 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106813173008 -> 140106813173296
	140106813173008 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813173440 -> 140106813173008
	140106813173440 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813173488 -> 140106813173440
	140106813173488 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813173632 -> 140106813173488
	140106813173632 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813173728 -> 140106813173632
	140107077230624 [label="t_enc.encoder.attn_layers.4.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107077230624 -> 140106813173728
	140106813173728 [label=AccumulateGrad]
	140106813170416 -> 140106813169792
	140106813170416 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813170752 -> 140106813170416
	140106813170752 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813170944 -> 140106813170752
	140106813170944 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813171136 -> 140106813170944
	140106813171136 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813171376 -> 140106813171136
	140106813171376 -> 140107082861200 [dir=none]
	140107082861200 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813171376 -> 140107077232864 [dir=none]
	140107077232864 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813171376 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854698528 -> 140106813171376
	140106813172384 -> 140106813171376
	140107077232864 [label="t_enc.encoder.attn_layers.4.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077232864 -> 140106813172384
	140106813172384 [label=AccumulateGrad]
	140106813172144 -> 140106813171376
	140107077233104 [label="t_enc.encoder.attn_layers.4.conv_v.bias
 (192)" fillcolor=lightblue]
	140107077233104 -> 140106813172144
	140106813172144 [label=AccumulateGrad]
	140106813170224 -> 140106813169408
	140106813170224 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813170656 -> 140106813170224
	140106813170656 -> 140106814112288 [dir=none]
	140106814112288 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106813170656 -> 140106814113728 [dir=none]
	140106814113728 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106813170656 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813170512 -> 140106813170656
	140106813170512 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813172480 -> 140106813170512
	140106813172480 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813172672 -> 140106813172480
	140106813172672 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106813171184 -> 140106813172672
	140106813171184 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813173248 -> 140106813171184
	140106813173248 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813173344 -> 140106813173248
	140106813173344 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813173536 -> 140106813173344
	140106813173536 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106813173824 -> 140106813173536
	140106813173824 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106813173056 -> 140106813173824
	140106813173056 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813173920 -> 140106813173056
	140106813173920 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106813170704 -> 140106813173920
	140106813170848 -> 140106813170656
	140106813170848 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106813172768 -> 140106813170848
	140106813172768 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106813173392 -> 140106813172768
	140106813173392 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813173680 -> 140106813173392
	140106813173680 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813173872 -> 140106813173680
	140106813173872 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813173968 -> 140106813173872
	140107077233664 [label="t_enc.encoder.attn_layers.4.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107077233664 -> 140106813173968
	140106813173968 [label=AccumulateGrad]
	140106854699008 -> 140106854698912
	140107077233184 [label="t_enc.encoder.attn_layers.4.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077233184 -> 140106854699008
	140106854699008 [label=AccumulateGrad]
	140106854698672 -> 140106854698912
	140107077233344 [label="t_enc.encoder.attn_layers.4.conv_o.bias
 (192)" fillcolor=lightblue]
	140107077233344 -> 140106854698672
	140106854698672 [label=AccumulateGrad]
	140106854698240 -> 140106854698144
	140107077233984 [label="t_enc.encoder.norm_layers_1.4.gamma
 (192)" fillcolor=lightblue]
	140107077233984 -> 140106854698240
	140106854698240 [label=AccumulateGrad]
	140106854698000 -> 140106854698144
	140107077234144 [label="t_enc.encoder.norm_layers_1.4.beta
 (192)" fillcolor=lightblue]
	140107077234144 -> 140106854698000
	140106854698000 [label=AccumulateGrad]
	140106854697952 -> 140106854697856
	140106854697952 -> 140106814113008 [dir=none]
	140106814113008 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854697952 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854698288 -> 140106854697952
	140106854698288 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854698288 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854698432 -> 140106854698288
	140106854698432 -> 140107082869360 [dir=none]
	140107082869360 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106854698432 -> 140107077234784 [dir=none]
	140107077234784 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106854698432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854698720 -> 140106854698432
	140106854698720 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854699344 -> 140106854698720
	140106854699344 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854699344 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813170368 -> 140106854699344
	140106813170368 -> 140106814113568 [dir=none]
	140106814113568 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106813170368 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813172576 -> 140106813170368
	140106813172576 -> 140106814113328 [dir=none]
	140106814113328 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106813172576 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106813173584 -> 140106813172576
	140106813173584 -> 140107082874720 [dir=none]
	140107082874720 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106813173584 -> 140107077234304 [dir=none]
	140107077234304 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106813173584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106813172912 -> 140106813173584
	140106813172912 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106813174064 -> 140106813172912
	140106813174064 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106813174064 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854697904 -> 140106813174064
	140106813173776 -> 140106813173584
	140107077234304 [label="t_enc.encoder.ffn_layers.4.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107077234304 -> 140106813173776
	140106813173776 [label=AccumulateGrad]
	140106813166624 -> 140106813173584
	140107077234464 [label="t_enc.encoder.ffn_layers.4.conv_1.bias
 (768)" fillcolor=lightblue]
	140107077234464 -> 140106813166624
	140106813166624 [label=AccumulateGrad]
	140106854698336 -> 140106854698432
	140107077234784 [label="t_enc.encoder.ffn_layers.4.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107077234784 -> 140106854698336
	140106854698336 [label=AccumulateGrad]
	140106854698048 -> 140106854698432
	140107077234864 [label="t_enc.encoder.ffn_layers.4.conv_2.bias
 (192)" fillcolor=lightblue]
	140107077234864 -> 140106854698048
	140106854698048 [label=AccumulateGrad]
	140106854697616 -> 140106854697520
	140107077234944 [label="t_enc.encoder.norm_layers_2.4.gamma
 (192)" fillcolor=lightblue]
	140107077234944 -> 140106854697616
	140106854697616 [label=AccumulateGrad]
	140106854697376 -> 140106854697520
	140107077235104 [label="t_enc.encoder.norm_layers_2.4.beta
 (192)" fillcolor=lightblue]
	140107077235104 -> 140106854697376
	140106854697376 [label=AccumulateGrad]
	140106854697328 -> 140106854697232
	140106854697328 -> 140106814113808 [dir=none]
	140106814113808 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854697328 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854697664 -> 140106854697328
	140106854697664 -> 140107077233584 [dir=none]
	140107077233584 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106854697664 -> 140107077236064 [dir=none]
	140107077236064 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854697664 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854697808 -> 140106854697664
	140106854697808 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106854698960 -> 140106854697808
	140106854698960 [label=CloneBackward0]
	140106854699632 -> 140106854698960
	140106854699632 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813173152 -> 140106854699632
	140106813173152 [label="AddBackward0
------------
alpha: 1"]
	140106813170560 -> 140106813173152
	140106813170560 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813171712 -> 140106813170560
	140106813171712 -> 140106814111888 [dir=none]
	140106814111888 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106813171712 -> 140106814113648 [dir=none]
	140106814113648 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106813171712 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813174256 -> 140106813171712
	140106813174256 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813174400 -> 140106813174256
	140106813174400 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106813174496 -> 140106813174400
	140106813174496 -> 140106814113888 [dir=none]
	140106814113888 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106813174496 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813174592 -> 140106813174496
	140106813174592 -> 140106814114048 [dir=none]
	140106814114048 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106813174592 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106813174688 -> 140106813174592
	140106813174688 -> 140107077228464 [dir=none]
	140107077228464 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106813174688 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106813174784 -> 140106813174688
	140106813174784 [label="AddBackward0
------------
alpha: 1"]
	140106813174880 -> 140106813174784
	140106813174880 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106813175024 -> 140106813174880
	140106813175024 -> 140106814113968 [dir=none]
	140106814113968 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106813175024 -> 140106814114128 [dir=none]
	140106814114128 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813175024 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813175120 -> 140106813175024
	140106813175120 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813175264 -> 140106813175120
	140106813175264 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813175360 -> 140106813175264
	140106813175360 -> 140106814114208 [dir=none]
	140106814114208 [label="other
 ()" fillcolor=orange]
	140106813175360 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813175456 -> 140106813175360
	140106813175456 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813175552 -> 140106813175456
	140106813175552 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813175648 -> 140106813175552
	140106813175648 -> 140107077226464 [dir=none]
	140107077226464 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813175648 -> 140107077235184 [dir=none]
	140107077235184 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813175648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854697280 -> 140106813175648
	140106813175744 -> 140106813175648
	140107077235184 [label="t_enc.encoder.attn_layers.5.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077235184 -> 140106813175744
	140106813175744 [label=AccumulateGrad]
	140106813175696 -> 140106813175648
	140107077235344 [label="t_enc.encoder.attn_layers.5.conv_q.bias
 (192)" fillcolor=lightblue]
	140107077235344 -> 140106813175696
	140106813175696 [label=AccumulateGrad]
	140106813175072 -> 140106813175024
	140106813175072 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813175408 -> 140106813175072
	140106813175408 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106813175600 -> 140106813175408
	140106813175600 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813175840 -> 140106813175600
	140106813175840 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813175888 -> 140106813175840
	140106813175888 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813176032 -> 140106813175888
	140106813176032 -> 140107077226464 [dir=none]
	140107077226464 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813176032 -> 140107077235504 [dir=none]
	140107077235504 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813176032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854697280 -> 140106813176032
	140106813176128 -> 140106813176032
	140107077235504 [label="t_enc.encoder.attn_layers.5.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077235504 -> 140106813176128
	140106813176128 [label=AccumulateGrad]
	140106813176080 -> 140106813176032
	140107077235584 [label="t_enc.encoder.attn_layers.5.conv_k.bias
 (192)" fillcolor=lightblue]
	140107077235584 -> 140106813176080
	140106813176080 [label=AccumulateGrad]
	140106813174832 -> 140106813174784
	140106813174832 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106813175312 -> 140106813174832
	140106813175312 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106813175792 -> 140106813175312
	140106813175792 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813175984 -> 140106813175792
	140106813175984 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106813175216 -> 140106813175984
	140106813175216 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106813176224 -> 140106813175216
	140106813176224 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106813176320 -> 140106813176224
	140106813176320 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106813176416 -> 140106813176320
	140106813176416 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106854701408 -> 140106813176416
	140106854701408 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106813176512 -> 140106854701408
	140106813176512 -> 140106814114368 [dir=none]
	140106814114368 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106813176512 -> 140106814113408 [dir=none]
	140106814113408 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106813176512 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813176608 -> 140106813176512
	140106813176608 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813176752 -> 140106813176608
	140106813176752 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813176848 -> 140106813176752
	140106813176848 -> 140106814114288 [dir=none]
	140106814114288 [label="other
 ()" fillcolor=orange]
	140106813176848 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813175456 -> 140106813176848
	140106813176560 -> 140106813176512
	140106813176560 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106813176944 -> 140106813176560
	140106813176944 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106813176656 -> 140106813176944
	140106813176656 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106813177088 -> 140106813176656
	140106813177088 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813177136 -> 140106813177088
	140106813177136 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813177280 -> 140106813177136
	140106813177280 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813177376 -> 140106813177280
	140107077233904 [label="t_enc.encoder.attn_layers.5.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107077233904 -> 140106813177376
	140106813177376 [label=AccumulateGrad]
	140106813174208 -> 140106813171712
	140106813174208 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106854701312 -> 140106813174208
	140106854701312 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106813174640 -> 140106854701312
	140106813174640 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106813174304 -> 140106813174640
	140106813174304 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106813175504 -> 140106813174304
	140106813175504 -> 140107077226464 [dir=none]
	140107077226464 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106813175504 -> 140107077235744 [dir=none]
	140107077235744 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106813175504 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854697280 -> 140106813175504
	140106813175936 -> 140106813175504
	140107077235744 [label="t_enc.encoder.attn_layers.5.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077235744 -> 140106813175936
	140106813175936 [label=AccumulateGrad]
	140106813175168 -> 140106813175504
	140107077235904 [label="t_enc.encoder.attn_layers.5.conv_v.bias
 (192)" fillcolor=lightblue]
	140107077235904 -> 140106813175168
	140106813175168 [label=AccumulateGrad]
	140106813174016 -> 140106813173152
	140106813174016 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106813174448 -> 140106813174016
	140106813174448 -> 140106814114528 [dir=none]
	140106814114528 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106813174448 -> 140106814114848 [dir=none]
	140106814114848 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106813174448 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106813174736 -> 140106813174448
	140106813174736 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813176176 -> 140106813174736
	140106813176176 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813176368 -> 140106813176176
	140106813176368 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106813174976 -> 140106813176368
	140106813174976 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813176896 -> 140106813174976
	140106813176896 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813176992 -> 140106813176896
	140106813176992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106813177184 -> 140106813176992
	140106813177184 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106813177424 -> 140106813177184
	140106813177424 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106813177616 -> 140106813177424
	140106813177616 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106813177712 -> 140106813177616
	140106813177712 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106813174496 -> 140106813177712
	140106813174544 -> 140106813174448
	140106813174544 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106813176464 -> 140106813174544
	140106813176464 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106813177040 -> 140106813176464
	140106813177040 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106813177520 -> 140106813177040
	140106813177520 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106813177664 -> 140106813177520
	140106813177664 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106813177760 -> 140106813177664
	140107077236384 [label="t_enc.encoder.attn_layers.5.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107077236384 -> 140106813177760
	140106813177760 [label=AccumulateGrad]
	140106854697760 -> 140106854697664
	140107077236064 [label="t_enc.encoder.attn_layers.5.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107077236064 -> 140106854697760
	140106854697760 [label=AccumulateGrad]
	140106854697424 -> 140106854697664
	140107077236144 [label="t_enc.encoder.attn_layers.5.conv_o.bias
 (192)" fillcolor=lightblue]
	140107077236144 -> 140106854697424
	140106854697424 [label=AccumulateGrad]
	140106854696992 -> 140106854696896
	140107077236624 [label="t_enc.encoder.norm_layers_1.5.gamma
 (192)" fillcolor=lightblue]
	140107077236624 -> 140106854696992
	140106854696992 [label=AccumulateGrad]
	140106854696752 -> 140106854696896
	140107077236864 [label="t_enc.encoder.norm_layers_1.5.beta
 (192)" fillcolor=lightblue]
	140107077236864 -> 140106854696752
	140106854696752 [label=AccumulateGrad]
	140106854696704 -> 140106854696608
	140106854696704 -> 140106814114608 [dir=none]
	140106814114608 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106854696704 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854697040 -> 140106854696704
	140106854697040 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854697040 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854697184 -> 140106854697040
	140106854697184 -> 140107077233744 [dir=none]
	140107077233744 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106854697184 -> 140107077237424 [dir=none]
	140107077237424 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106854697184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854697472 -> 140106854697184
	140106854697472 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106854698096 -> 140106854697472
	140106854698096 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106854698096 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813174160 -> 140106854698096
	140106813174160 -> 140106814115248 [dir=none]
	140106814115248 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106813174160 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813176272 -> 140106813174160
	140106813176272 -> 140106814115168 [dir=none]
	140106814115168 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106813176272 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106813177232 -> 140106813176272
	140106813177232 -> 140107077231824 [dir=none]
	140107077231824 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106813177232 -> 140107077237024 [dir=none]
	140107077237024 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106813177232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106813177808 -> 140106813177232
	140106813177808 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106813174352 -> 140106813177808
	140106813174352 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106813174352 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854696656 -> 140106813174352
	140106813177568 -> 140106813177232
	140107077237024 [label="t_enc.encoder.ffn_layers.5.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107077237024 -> 140106813177568
	140106813177568 [label=AccumulateGrad]
	140106813170320 -> 140106813177232
	140107077237184 [label="t_enc.encoder.ffn_layers.5.conv_1.bias
 (768)" fillcolor=lightblue]
	140107077237184 -> 140106813170320
	140106813170320 [label=AccumulateGrad]
	140106854697088 -> 140106854697184
	140107077237424 [label="t_enc.encoder.ffn_layers.5.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107077237424 -> 140106854697088
	140106854697088 [label=AccumulateGrad]
	140106854696800 -> 140106854697184
	140107077237504 [label="t_enc.encoder.ffn_layers.5.conv_2.bias
 (192)" fillcolor=lightblue]
	140107077237504 -> 140106854696800
	140106854696800 [label=AccumulateGrad]
	140106854696368 -> 140106854696272
	140107071062080 [label="t_enc.encoder.norm_layers_2.5.gamma
 (192)" fillcolor=lightblue]
	140107071062080 -> 140106854696368
	140106854696368 [label=AccumulateGrad]
	140106854695456 -> 140106854696272
	140107071062240 [label="t_enc.encoder.norm_layers_2.5.beta
 (192)" fillcolor=lightblue]
	140107071062240 -> 140106854695456
	140106854695456 [label=AccumulateGrad]
	140106854695360 -> 140106854695264
	140106854695360 -> 140107011976032 [dir=none]
	140107011976032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106854695360 -> 140107053618640 [dir=none]
	140107053618640 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854695360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854695696 -> 140106854695360
	140106854695696 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106854695696 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854696080 -> 140106854695696
	140106854696080 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854696416 -> 140106854696080
	140106854696416 -> 140107053617440 [dir=none]
	140107053617440 [label="bias
 (192)" fillcolor=orange]
	140106854696416 -> 140107011975472 [dir=none]
	140107011975472 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106854696416 -> 140106814115328 [dir=none]
	140106814115328 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106854696416 -> 140106814115408 [dir=none]
	140106814115408 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106854696416 -> 140107053616960 [dir=none]
	140107053616960 [label="weight
 (192)" fillcolor=orange]
	140106854696416 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106854696512 -> 140106854696416
	140106854696512 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106854698384 -> 140106854696512
	140106854698384 [label="AddBackward0
------------
alpha: 1"]
	140106854696848 -> 140106854698384
	140106854696848 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106813176800 -> 140106854696848
	140106813176800 -> 140107053615840 [dir=none]
	140107053615840 [label="bias
 (192)" fillcolor=orange]
	140106813176800 -> 140107011977552 [dir=none]
	140107011977552 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106813176800 -> 140106814115488 [dir=none]
	140106814115488 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106813176800 -> 140106814114928 [dir=none]
	140106814114928 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106813176800 -> 140107053615760 [dir=none]
	140107053615760 [label="weight
 (192)" fillcolor=orange]
	140106813176800 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106809802912 -> 140106813176800
	140106809802912 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809803200 -> 140106809802912
	140106809803200 [label="AddBackward0
------------
alpha: 1"]
	140106809803248 -> 140106809803200
	140106809803248 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809803488 -> 140106809803248
	140106809803488 -> 140107053613920 [dir=none]
	140107053613920 [label="bias
 (192)" fillcolor=orange]
	140106809803488 -> 140107011967152 [dir=none]
	140107011967152 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106809803488 -> 140106814114768 [dir=none]
	140106814114768 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106809803488 -> 140106814115648 [dir=none]
	140106814115648 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106809803488 -> 140107053613840 [dir=none]
	140107053613840 [label="weight
 (192)" fillcolor=orange]
	140106809803488 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106809803536 -> 140106809803488
	140106809803536 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809803824 -> 140106809803536
	140106809803824 [label="AddBackward0
------------
alpha: 1"]
	140106809803872 -> 140106809803824
	140106809803872 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809804208 -> 140106809803872
	140106809804208 -> 140107058265472 [dir=none]
	140107058265472 [label="bias
 (192)" fillcolor=orange]
	140106809804208 -> 140107011973392 [dir=none]
	140107011973392 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106809804208 -> 140106814115568 [dir=none]
	140106814115568 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106809804208 -> 140106814115808 [dir=none]
	140106814115808 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106809804208 -> 140107058264992 [dir=none]
	140107058264992 [label="weight
 (192)" fillcolor=orange]
	140106809804208 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106809804256 -> 140106809804208
	140106809804256 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809804544 -> 140106809804256
	140106809804544 [label="AddBackward0
------------
alpha: 1"]
	140106809804592 -> 140106809804544
	140106809804592 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809804832 -> 140106809804592
	140106809804832 -> 140107058261872 [dir=none]
	140107058261872 [label="bias
 (192)" fillcolor=orange]
	140106809804832 -> 140107019762832 [dir=none]
	140107019762832 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106809804832 -> 140106814115008 [dir=none]
	140106814115008 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106809804832 -> 140106814116048 [dir=none]
	140106814116048 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106809804832 -> 140107058261792 [dir=none]
	140107058261792 [label="weight
 (192)" fillcolor=orange]
	140106809804832 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106809804880 -> 140106809804832
	140106809804880 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809805168 -> 140106809804880
	140106809805168 [label="AddBackward0
------------
alpha: 1"]
	140106809805216 -> 140106809805168
	140106809805216 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809805456 -> 140106809805216
	140106809805456 -> 140107058261152 [dir=none]
	140107058261152 [label="bias
 (192)" fillcolor=orange]
	140106809805456 -> 140107019759072 [dir=none]
	140107019759072 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106809805456 -> 140106814115968 [dir=none]
	140106814115968 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106809805456 -> 140106814116208 [dir=none]
	140106814116208 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106809805456 -> 140107058260992 [dir=none]
	140107058260992 [label="weight
 (192)" fillcolor=orange]
	140106809805456 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106809805504 -> 140106809805456
	140106809805504 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809805792 -> 140106809805504
	140106809805792 [label="AddBackward0
------------
alpha: 1"]
	140106809805840 -> 140106809805792
	140106809805840 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809805840 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809806032 -> 140106809805840
	140106809806032 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809806032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809806128 -> 140106809806032
	140106809806128 [label="AddBackward0
------------
alpha: 1"]
	140106809806224 -> 140106809806128
	140106809806224 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809806224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809806368 -> 140106809806224
	140106809806368 [label="AddBackward0
------------
alpha: 1"]
	140106809806464 -> 140106809806368
	140106809806464 [label="AddBackward0
------------
alpha: 1"]
	140106809806608 -> 140106809806464
	140106809806608 [label="AddBackward0
------------
alpha: 1"]
	140106809806752 -> 140106809806608
	140106809806752 [label="AddBackward0
------------
alpha: 1"]
	140106809806896 -> 140106809806752
	140106809806896 [label="AddBackward0
------------
alpha: 1"]
	140106809807040 -> 140106809806896
	140106809807040 [label="AddBackward0
------------
alpha: 1"]
	140106809807184 -> 140106809807040
	140106809807184 [label="AddBackward0
------------
alpha: 1"]
	140106809807328 -> 140106809807184
	140106809807328 [label="AddBackward0
------------
alpha: 1"]
	140106809807472 -> 140106809807328
	140106809807472 [label="AddBackward0
------------
alpha: 1"]
	140106809807616 -> 140106809807472
	140106809807616 [label="AddBackward0
------------
alpha: 1"]
	140106809807760 -> 140106809807616
	140106809807760 [label="AddBackward0
------------
alpha: 1"]
	140106809807904 -> 140106809807760
	140106809807904 [label="AddBackward0
------------
alpha: 1"]
	140106809808048 -> 140106809807904
	140106809808048 [label="AddBackward0
------------
alpha: 1"]
	140106809808192 -> 140106809808048
	140106809808192 [label="AddBackward0
------------
alpha: 1"]
	140106809808336 -> 140106809808192
	140106809808336 [label="AddBackward0
------------
alpha: 1"]
	140106809808480 -> 140106809808336
	140106809808480 [label="AddBackward0
------------
alpha: 1"]
	140106809808624 -> 140106809808480
	140106809808624 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809808720 -> 140106809808624
	140106809808720 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809808816 -> 140106809808720
	140106809808816 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809808912 -> 140106809808816
	140106809808912 -> 140107031834320 [dir=none]
	140107031834320 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809808912 -> 140107031834800 [dir=none]
	140107031834800 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809808912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809809008 -> 140106809808912
	140106809809008 [label=CppFunction]
	140106809809200 -> 140106809809008
	140106809809200 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809809344 -> 140106809809200
	140106809809344 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809809440 -> 140106809809344
	140106809809440 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809809536 -> 140106809809440
	140106809809536 [label="AddBackward0
------------
alpha: 1"]
	140106809809632 -> 140106809809536
	140106809809632 -> 140107031832880 [dir=none]
	140107031832880 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809809632 -> 140107031833680 [dir=none]
	140107031833680 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809809632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809809776 -> 140106809809632
	140106809809776 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809809776 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809809968 -> 140106809809776
	140106809809968 -> 140107110300416 [dir=none]
	140107110300416 [label="input
 (7, 1025, 286)" fillcolor=orange]
	140106809809968 -> 140107065184544 [dir=none]
	140107065184544 [label="weight
 (192, 1025, 1)" fillcolor=orange]
	140106809809968 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809810064 -> 140106809809968
	140107065184544 [label="enc_p.0.pre.weight
 (192, 1025, 1)" fillcolor=lightblue]
	140107065184544 -> 140106809810064
	140106809810064 [label=AccumulateGrad]
	140106809810016 -> 140106809809968
	140107065184624 [label="enc_p.0.pre.bias
 (192)" fillcolor=lightblue]
	140107065184624 -> 140106809810016
	140106809810016 [label=AccumulateGrad]
	140106809809728 -> 140106809809632
	140106809809728 -> 140107065181824 [dir=none]
	140107065181824 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809809728 -> 140106814116608 [dir=none]
	140106814116608 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809809728 -> 140107065186304 [dir=none]
	140107065186304 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809809728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809810112 -> 140106809809728
	140107065186304 [label="enc_p.0.enc.in_layers.0.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065186304 -> 140106809810112
	140106809810112 [label=AccumulateGrad]
	140106809809872 -> 140106809809728
	140107065181824 [label="enc_p.0.enc.in_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065181824 -> 140106809809872
	140106809809872 [label=AccumulateGrad]
	140106809809680 -> 140106809809632
	140107065185904 [label="enc_p.0.enc.in_layers.0.bias
 (384)" fillcolor=lightblue]
	140107065185904 -> 140106809809680
	140106809809680 [label=AccumulateGrad]
	140106809809584 -> 140106809809536
	140106809809584 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809810160 -> 140106809809584
	140106809810160 [label="SliceBackward0
----------------------------
dim           :            1
end           :          384
self_sym_sizes: (7, 6144, 1)
start         :            0
step          :            1"]
	140106809810400 -> 140106809810160
	140106809810400 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809810400
	140106809810496 -> 140107031832400 [dir=none]
	140107031832400 [label="input
 (7, 512, 1)" fillcolor=orange]
	140106809810496 -> 140107031833600 [dir=none]
	140107031833600 [label="weight
 (6144, 512, 1)" fillcolor=orange]
	140106809810496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (6144,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809810592 -> 140106809810496
	140106809810592 -> 140107938940384 [dir=none]
	140107938940384 [label="g
 (6144, 1, 1)" fillcolor=orange]
	140106809810592 -> 140106814116528 [dir=none]
	140106814116528 [label="result1
 (6144, 1, 1)" fillcolor=orange]
	140106809810592 -> 140107065185104 [dir=none]
	140107065185104 [label="v
 (6144, 512, 1)" fillcolor=orange]
	140106809810592 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809810736 -> 140106809810592
	140107065185104 [label="enc_p.0.enc.cond_layer.weight_v
 (6144, 512, 1)" fillcolor=lightblue]
	140107065185104 -> 140106809810736
	140106809810736 [label=AccumulateGrad]
	140106809810688 -> 140106809810592
	140107938940384 [label="enc_p.0.enc.cond_layer.weight_g
 (6144, 1, 1)" fillcolor=lightblue]
	140107938940384 -> 140106809810688
	140106809810688 [label=AccumulateGrad]
	140106809810544 -> 140106809810496
	140107065185024 [label="enc_p.0.enc.cond_layer.bias
 (6144)" fillcolor=lightblue]
	140107065185024 -> 140106809810544
	140106809810544 [label=AccumulateGrad]
	140106809809152 -> 140106809809008
	140106809809152 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809809488 -> 140106809809152
	140106809809488 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809809440 -> 140106809809488
	140106809808960 -> 140106809808912
	140106809808960 -> 140107065185984 [dir=none]
	140107065185984 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809808960 -> 140106814115888 [dir=none]
	140106814115888 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809808960 -> 140107065187104 [dir=none]
	140107065187104 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809808960 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809810208 -> 140106809808960
	140107065187104 [label="enc_p.0.enc.res_skip_layers.0.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065187104 -> 140106809810208
	140106809810208 [label=AccumulateGrad]
	140106809809392 -> 140106809808960
	140107065185984 [label="enc_p.0.enc.res_skip_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065185984 -> 140106809809392
	140106809809392 [label=AccumulateGrad]
	140106809808528 -> 140106809808912
	140107065186464 [label="enc_p.0.enc.res_skip_layers.0.bias
 (384)" fillcolor=lightblue]
	140107065186464 -> 140106809808528
	140106809808528 [label=AccumulateGrad]
	140106809808432 -> 140106809808336
	140106809808432 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809808768 -> 140106809808432
	140106809808768 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809809056 -> 140106809808768
	140106809809056 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809809248 -> 140106809809056
	140106809809248 -> 140107031833840 [dir=none]
	140107031833840 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809809248 -> 140107031835680 [dir=none]
	140107031835680 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809809248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809810352 -> 140106809809248
	140106809810352 [label=CppFunction]
	140106809810784 -> 140106809810352
	140106809810784 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809810928 -> 140106809810784
	140106809810928 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809811024 -> 140106809810928
	140106809811024 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809811120 -> 140106809811024
	140106809811120 [label="AddBackward0
------------
alpha: 1"]
	140106809811216 -> 140106809811120
	140106809811216 -> 140107031836080 [dir=none]
	140107031836080 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809811216 -> 140107099116768 [dir=none]
	140107099116768 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809811216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809810304 -> 140106809811216
	140106809810304 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809810304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809811552 -> 140106809810304
	140106809811552 [label="AddBackward0
------------
alpha: 1"]
	140106809809776 -> 140106809811552
	140106809811648 -> 140106809811552
	140106809811648 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809811744 -> 140106809811648
	140106809811744 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106809811840 -> 140106809811744
	140106809811840 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809808912 -> 140106809811840
	140106809811312 -> 140106809811216
	140106809811312 -> 140107065186624 [dir=none]
	140107065186624 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809811312 -> 140106814116688 [dir=none]
	140106814116688 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809811312 -> 140107065187744 [dir=none]
	140107065187744 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809811312 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809811696 -> 140106809811312
	140107065187744 [label="enc_p.0.enc.in_layers.1.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065187744 -> 140106809811696
	140106809811696 [label=AccumulateGrad]
	140106809811600 -> 140106809811312
	140107065186624 [label="enc_p.0.enc.in_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065186624 -> 140106809811600
	140106809811600 [label=AccumulateGrad]
	140106809811264 -> 140106809811216
	140107065187424 [label="enc_p.0.enc.in_layers.1.bias
 (384)" fillcolor=lightblue]
	140107065187424 -> 140106809811264
	140106809811264 [label=AccumulateGrad]
	140106809811168 -> 140106809811120
	140106809811168 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809811792 -> 140106809811168
	140106809811792 [label="SliceBackward0
----------------------------
dim           :            1
end           :          768
self_sym_sizes: (7, 6144, 1)
start         :          384
step          :            1"]
	140106809811888 -> 140106809811792
	140106809811888 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809811888
	140106809810832 -> 140106809810352
	140106809810832 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809811072 -> 140106809810832
	140106809811072 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809811024 -> 140106809811072
	140106809809104 -> 140106809809248
	140106809809104 -> 140107065187504 [dir=none]
	140107065187504 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809809104 -> 140106814116768 [dir=none]
	140106814116768 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809809104 -> 140107065188784 [dir=none]
	140107065188784 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809809104 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809811936 -> 140106809809104
	140107065188784 [label="enc_p.0.enc.res_skip_layers.1.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065188784 -> 140106809811936
	140106809811936 [label=AccumulateGrad]
	140106809810976 -> 140106809809104
	140107065187504 [label="enc_p.0.enc.res_skip_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065187504 -> 140106809810976
	140106809810976 [label=AccumulateGrad]
	140106809808576 -> 140106809809248
	140107065187904 [label="enc_p.0.enc.res_skip_layers.1.bias
 (384)" fillcolor=lightblue]
	140107065187904 -> 140106809808576
	140106809808576 [label=AccumulateGrad]
	140106809808288 -> 140106809808192
	140106809808288 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809808864 -> 140106809808288
	140106809808864 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809810448 -> 140106809808864
	140106809810448 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809810640 -> 140106809810448
	140106809810640 -> 140107065186144 [dir=none]
	140107065186144 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809810640 -> 140107065187024 [dir=none]
	140107065187024 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809810640 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809811504 -> 140106809810640
	140106809811504 [label=CppFunction]
	140106809812032 -> 140106809811504
	140106809812032 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809812176 -> 140106809812032
	140106809812176 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809812272 -> 140106809812176
	140106809812272 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809812368 -> 140106809812272
	140106809812368 [label="AddBackward0
------------
alpha: 1"]
	140106809812464 -> 140106809812368
	140106809812464 -> 140107031834880 [dir=none]
	140107031834880 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809812464 -> 140107031838400 [dir=none]
	140107031838400 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809812464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809812608 -> 140106809812464
	140106809812608 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809812608 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809812800 -> 140106809812608
	140106809812800 [label="AddBackward0
------------
alpha: 1"]
	140106809810304 -> 140106809812800
	140106809812896 -> 140106809812800
	140106809812896 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809812992 -> 140106809812896
	140106809812992 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106809813088 -> 140106809812992
	140106809813088 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809809248 -> 140106809813088
	140106809812560 -> 140106809812464
	140106809812560 -> 140107065187984 [dir=none]
	140107065187984 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809812560 -> 140106814115088 [dir=none]
	140106814115088 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809812560 -> 140107065189664 [dir=none]
	140107065189664 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809812560 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809812944 -> 140106809812560
	140107065189664 [label="enc_p.0.enc.in_layers.2.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065189664 -> 140106809812944
	140106809812944 [label=AccumulateGrad]
	140106809812848 -> 140106809812560
	140107065187984 [label="enc_p.0.enc.in_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065187984 -> 140106809812848
	140106809812848 [label=AccumulateGrad]
	140106809812512 -> 140106809812464
	140107065189184 [label="enc_p.0.enc.in_layers.2.bias
 (384)" fillcolor=lightblue]
	140107065189184 -> 140106809812512
	140106809812512 [label=AccumulateGrad]
	140106809812416 -> 140106809812368
	140106809812416 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809813040 -> 140106809812416
	140106809813040 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1152
self_sym_sizes: (7, 6144, 1)
start         :          768
step          :            1"]
	140106809813136 -> 140106809813040
	140106809813136 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809813136
	140106809811408 -> 140106809811504
	140106809811408 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809812320 -> 140106809811408
	140106809812320 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809812272 -> 140106809812320
	140106809809824 -> 140106809810640
	140106809809824 -> 140107065189504 [dir=none]
	140107065189504 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809809824 -> 140106814116368 [dir=none]
	140106814116368 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809809824 -> 140107065190144 [dir=none]
	140107065190144 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809809824 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809813184 -> 140106809809824
	140107065190144 [label="enc_p.0.enc.res_skip_layers.2.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065190144 -> 140106809813184
	140106809813184 [label=AccumulateGrad]
	140106809812224 -> 140106809809824
	140107065189504 [label="enc_p.0.enc.res_skip_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065189504 -> 140106809812224
	140106809812224 [label=AccumulateGrad]
	140106809808384 -> 140106809810640
	140107065189904 [label="enc_p.0.enc.res_skip_layers.2.bias
 (384)" fillcolor=lightblue]
	140107065189904 -> 140106809808384
	140106809808384 [label=AccumulateGrad]
	140106809808144 -> 140106809808048
	140106809808144 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809809296 -> 140106809808144
	140106809809296 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809811984 -> 140106809809296
	140106809811984 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809812080 -> 140106809811984
	140106809812080 -> 140107065187664 [dir=none]
	140107065187664 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809812080 -> 140107031838480 [dir=none]
	140107031838480 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809812080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809812752 -> 140106809812080
	140106809812752 [label=CppFunction]
	140106809813280 -> 140106809812752
	140106809813280 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809813424 -> 140106809813280
	140106809813424 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809813520 -> 140106809813424
	140106809813520 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809813616 -> 140106809813520
	140106809813616 [label="AddBackward0
------------
alpha: 1"]
	140106809813712 -> 140106809813616
	140106809813712 -> 140107065188304 [dir=none]
	140107065188304 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809813712 -> 140107031838560 [dir=none]
	140107031838560 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809813712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809813856 -> 140106809813712
	140106809813856 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809813856 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809814048 -> 140106809813856
	140106809814048 [label="AddBackward0
------------
alpha: 1"]
	140106809812608 -> 140106809814048
	140106809814144 -> 140106809814048
	140106809814144 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809814240 -> 140106809814144
	140106809814240 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106809814336 -> 140106809814240
	140106809814336 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809810640 -> 140106809814336
	140106809813808 -> 140106809813712
	140106809813808 -> 140107065189984 [dir=none]
	140107065189984 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809813808 -> 140106814116288 [dir=none]
	140106814116288 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809813808 -> 140107065191024 [dir=none]
	140107065191024 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809813808 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809814192 -> 140106809813808
	140107065191024 [label="enc_p.0.enc.in_layers.3.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065191024 -> 140106809814192
	140106809814192 [label=AccumulateGrad]
	140106809814096 -> 140106809813808
	140107065189984 [label="enc_p.0.enc.in_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065189984 -> 140106809814096
	140106809814096 [label=AccumulateGrad]
	140106809813760 -> 140106809813712
	140107065190224 [label="enc_p.0.enc.in_layers.3.bias
 (384)" fillcolor=lightblue]
	140107065190224 -> 140106809813760
	140106809813760 [label=AccumulateGrad]
	140106809813664 -> 140106809813616
	140106809813664 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809814288 -> 140106809813664
	140106809814288 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1536
self_sym_sizes: (7, 6144, 1)
start         :         1152
step          :            1"]
	140106809814384 -> 140106809814288
	140106809814384 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809814384
	140106809812656 -> 140106809812752
	140106809812656 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809813328 -> 140106809812656
	140106809813328 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809813520 -> 140106809813328
	140106809811456 -> 140106809812080
	140106809811456 -> 140107065190464 [dir=none]
	140107065190464 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809811456 -> 140106814116848 [dir=none]
	140106814116848 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809811456 -> 140107065191824 [dir=none]
	140107065191824 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809811456 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809814000 -> 140106809811456
	140107065191824 [label="enc_p.0.enc.res_skip_layers.3.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065191824 -> 140106809814000
	140106809814000 [label=AccumulateGrad]
	140106809813568 -> 140106809811456
	140107065190464 [label="enc_p.0.enc.res_skip_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065190464 -> 140106809813568
	140106809813568 [label=AccumulateGrad]
	140106809808240 -> 140106809812080
	140107065191184 [label="enc_p.0.enc.res_skip_layers.3.bias
 (384)" fillcolor=lightblue]
	140107065191184 -> 140106809808240
	140106809808240 [label=AccumulateGrad]
	140106809808000 -> 140106809807904
	140106809808000 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809810880 -> 140106809808000
	140106809810880 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809813232 -> 140106809810880
	140106809813232 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809814432 -> 140106809813232
	140106809814432 -> 140107065189584 [dir=none]
	140107065189584 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809814432 -> 140107019748752 [dir=none]
	140107019748752 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809814432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809814480 -> 140106809814432
	140106809814480 [label=CppFunction]
	140106809814576 -> 140106809814480
	140106809814576 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809814720 -> 140106809814576
	140106809814720 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809814816 -> 140106809814720
	140106809814816 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809814912 -> 140106809814816
	140106809814912 [label="AddBackward0
------------
alpha: 1"]
	140106809815008 -> 140106809814912
	140106809815008 -> 140107019749392 [dir=none]
	140107019749392 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809815008 -> 140107065190064 [dir=none]
	140107065190064 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809815008 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809815152 -> 140106809815008
	140106809815152 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809815152 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809815344 -> 140106809815152
	140106809815344 [label="AddBackward0
------------
alpha: 1"]
	140106809813856 -> 140106809815344
	140106809815440 -> 140106809815344
	140106809815440 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809815536 -> 140106809815440
	140106809815536 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106809815632 -> 140106809815536
	140106809815632 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809812080 -> 140106809815632
	140106809815104 -> 140106809815008
	140106809815104 -> 140107065191264 [dir=none]
	140107065191264 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809815104 -> 140106814117248 [dir=none]
	140106814117248 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809815104 -> 140107065192784 [dir=none]
	140107065192784 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809815104 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809815488 -> 140106809815104
	140107065192784 [label="enc_p.0.enc.in_layers.4.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065192784 -> 140106809815488
	140106809815488 [label=AccumulateGrad]
	140106809813472 -> 140106809815104
	140107065191264 [label="enc_p.0.enc.in_layers.4.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065191264 -> 140106809813472
	140106809813472 [label=AccumulateGrad]
	140106809815056 -> 140106809815008
	140107065191904 [label="enc_p.0.enc.in_layers.4.bias
 (384)" fillcolor=lightblue]
	140107065191904 -> 140106809815056
	140106809815056 [label=AccumulateGrad]
	140106809814960 -> 140106809814912
	140106809814960 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809815392 -> 140106809814960
	140106809815392 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1920
self_sym_sizes: (7, 6144, 1)
start         :         1536
step          :            1"]
	140106809815584 -> 140106809815392
	140106809815584 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809815584
	140106809814528 -> 140106809814480
	140106809814528 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809814864 -> 140106809814528
	140106809814864 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809814816 -> 140106809814864
	140106809812704 -> 140106809814432
	140106809812704 -> 140107065192064 [dir=none]
	140107065192064 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809812704 -> 140106814117168 [dir=none]
	140106814117168 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809812704 -> 140107065193424 [dir=none]
	140107065193424 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809812704 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809815296 -> 140106809812704
	140107065193424 [label="enc_p.0.enc.res_skip_layers.4.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065193424 -> 140106809815296
	140106809815296 [label=AccumulateGrad]
	140106809814768 -> 140106809812704
	140107065192064 [label="enc_p.0.enc.res_skip_layers.4.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065192064 -> 140106809814768
	140106809814768 [label=AccumulateGrad]
	140106809808096 -> 140106809814432
	140107065192944 [label="enc_p.0.enc.res_skip_layers.4.bias
 (384)" fillcolor=lightblue]
	140107065192944 -> 140106809808096
	140106809808096 [label=AccumulateGrad]
	140106809807856 -> 140106809807760
	140106809807856 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809812128 -> 140106809807856
	140106809812128 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809813952 -> 140106809812128
	140106809813952 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809814624 -> 140106809813952
	140106809814624 -> 140107065190944 [dir=none]
	140107065190944 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809814624 -> 140107019747872 [dir=none]
	140107019747872 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809814624 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809811360 -> 140106809814624
	140106809811360 [label=CppFunction]
	140106809815248 -> 140106809811360
	140106809815248 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809815872 -> 140106809815248
	140106809815872 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809815968 -> 140106809815872
	140106809815968 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809816064 -> 140106809815968
	140106809816064 [label="AddBackward0
------------
alpha: 1"]
	140106809816160 -> 140106809816064
	140106809816160 -> 140107019748112 [dir=none]
	140107019748112 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809816160 -> 140107065191424 [dir=none]
	140107065191424 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809816160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809816304 -> 140106809816160
	140106809816304 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809816304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809816496 -> 140106809816304
	140106809816496 [label="AddBackward0
------------
alpha: 1"]
	140106809815152 -> 140106809816496
	140106809816544 -> 140106809816496
	140106809816544 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809816736 -> 140106809816544
	140106809816736 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106809816832 -> 140106809816736
	140106809816832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809814432 -> 140106809816832
	140106809816256 -> 140106809816160
	140106809816256 -> 140107065193184 [dir=none]
	140107065193184 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809816256 -> 140106814117008 [dir=none]
	140106814117008 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809816256 -> 140107065193824 [dir=none]
	140107065193824 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809816256 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809816688 -> 140106809816256
	140107065193824 [label="enc_p.0.enc.in_layers.5.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065193824 -> 140106809816688
	140106809816688 [label=AccumulateGrad]
	140106809816400 -> 140106809816256
	140107065193184 [label="enc_p.0.enc.in_layers.5.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065193184 -> 140106809816400
	140106809816400 [label=AccumulateGrad]
	140106809816208 -> 140106809816160
	140107065193504 [label="enc_p.0.enc.in_layers.5.bias
 (384)" fillcolor=lightblue]
	140107065193504 -> 140106809816208
	140106809816208 [label=AccumulateGrad]
	140106809816112 -> 140106809816064
	140106809816112 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809816784 -> 140106809816112
	140106809816784 [label="SliceBackward0
----------------------------
dim           :            1
end           :         2304
self_sym_sizes: (7, 6144, 1)
start         :         1920
step          :            1"]
	140106809816880 -> 140106809816784
	140106809816880 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809816880
	140106809815200 -> 140106809811360
	140106809815200 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809816016 -> 140106809815200
	140106809816016 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809815968 -> 140106809816016
	140106809813904 -> 140106809814624
	140106809813904 -> 140107065193584 [dir=none]
	140107065193584 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809813904 -> 140106814116928 [dir=none]
	140106814116928 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809813904 -> 140107065194144 [dir=none]
	140107065194144 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809813904 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809816928 -> 140106809813904
	140107065194144 [label="enc_p.0.enc.res_skip_layers.5.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065194144 -> 140106809816928
	140106809816928 [label=AccumulateGrad]
	140106809815920 -> 140106809813904
	140107065193584 [label="enc_p.0.enc.res_skip_layers.5.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065193584 -> 140106809815920
	140106809815920 [label=AccumulateGrad]
	140106809807952 -> 140106809814624
	140107065193904 [label="enc_p.0.enc.res_skip_layers.5.bias
 (384)" fillcolor=lightblue]
	140107065193904 -> 140106809807952
	140106809807952 [label=AccumulateGrad]
	140106809807712 -> 140106809807616
	140106809807712 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809813376 -> 140106809807712
	140106809813376 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809815680 -> 140106809813376
	140106809815680 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809815776 -> 140106809815680
	140106809815776 -> 140107065192624 [dir=none]
	140107065192624 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809815776 -> 140107019748832 [dir=none]
	140107019748832 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809815776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809816448 -> 140106809815776
	140106809816448 [label=CppFunction]
	140106809817024 -> 140106809816448
	140106809817024 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809817168 -> 140106809817024
	140106809817168 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809817264 -> 140106809817168
	140106809817264 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809817360 -> 140106809817264
	140106809817360 [label="AddBackward0
------------
alpha: 1"]
	140106809817456 -> 140106809817360
	140106809817456 -> 140107019748272 [dir=none]
	140107019748272 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809817456 -> 140107065193264 [dir=none]
	140107065193264 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809817456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809817600 -> 140106809817456
	140106809817600 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809817600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809817792 -> 140106809817600
	140106809817792 [label="AddBackward0
------------
alpha: 1"]
	140106809816304 -> 140106809817792
	140106809817888 -> 140106809817792
	140106809817888 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809817984 -> 140106809817888
	140106809817984 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106809818080 -> 140106809817984
	140106809818080 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809814624 -> 140106809818080
	140106809817552 -> 140106809817456
	140106809817552 -> 140107934891312 [dir=none]
	140107934891312 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809817552 -> 140106814116448 [dir=none]
	140106814116448 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809817552 -> 140107065194384 [dir=none]
	140107065194384 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809817552 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809817936 -> 140106809817552
	140107065194384 [label="enc_p.0.enc.in_layers.6.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065194384 -> 140106809817936
	140106809817936 [label=AccumulateGrad]
	140106809817840 -> 140106809817552
	140107934891312 [label="enc_p.0.enc.in_layers.6.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107934891312 -> 140106809817840
	140106809817840 [label=AccumulateGrad]
	140106809817504 -> 140106809817456
	140107065194304 [label="enc_p.0.enc.in_layers.6.bias
 (384)" fillcolor=lightblue]
	140107065194304 -> 140106809817504
	140106809817504 [label=AccumulateGrad]
	140106809817408 -> 140106809817360
	140106809817408 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809818032 -> 140106809817408
	140106809818032 [label="SliceBackward0
----------------------------
dim           :            1
end           :         2688
self_sym_sizes: (7, 6144, 1)
start         :         2304
step          :            1"]
	140106809818128 -> 140106809818032
	140106809818128 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106809818128
	140106809816352 -> 140106809816448
	140106809816352 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809817312 -> 140106809816352
	140106809817312 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809817264 -> 140106809817312
	140106809815728 -> 140106809815776
	140106809815728 -> 140107065193984 [dir=none]
	140107065193984 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809815728 -> 140106814117728 [dir=none]
	140106814117728 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809815728 -> 140107065194864 [dir=none]
	140107065194864 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809815728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809818176 -> 140106809815728
	140107065194864 [label="enc_p.0.enc.res_skip_layers.6.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065194864 -> 140106809818176
	140106809818176 [label=AccumulateGrad]
	140106809817216 -> 140106809815728
	140107065193984 [label="enc_p.0.enc.res_skip_layers.6.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065193984 -> 140106809817216
	140106809817216 [label=AccumulateGrad]
	140106809807808 -> 140106809815776
	140107065194544 [label="enc_p.0.enc.res_skip_layers.6.bias
 (384)" fillcolor=lightblue]
	140107065194544 -> 140106809807808
	140106809807808 [label=AccumulateGrad]
	140106809807568 -> 140106809807472
	140106809807568 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809814672 -> 140106809807568
	140106809814672 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809816976 -> 140106809814672
	140106809816976 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809816592 -> 140106809816976
	140106809816592 -> 140107065193664 [dir=none]
	140107065193664 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809816592 -> 140107019749232 [dir=none]
	140107019749232 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809816592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809817744 -> 140106809816592
	140106809817744 [label=CppFunction]
	140106809818272 -> 140106809817744
	140106809818272 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809818416 -> 140106809818272
	140106809818416 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106809818512 -> 140106809818416
	140106809818512 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809818608 -> 140106809818512
	140106809818608 [label="AddBackward0
------------
alpha: 1"]
	140106809818704 -> 140106809818608
	140106809818704 -> 140107019749072 [dir=none]
	140107019749072 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809818704 -> 140107065194064 [dir=none]
	140107065194064 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106809818704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809818848 -> 140106809818704
	140106809818848 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809818848 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809819040 -> 140106809818848
	140106809819040 [label="AddBackward0
------------
alpha: 1"]
	140106809817600 -> 140106809819040
	140106809819088 -> 140106809819040
	140106809819088 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807853216 -> 140106809819088
	140106807853216 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807853312 -> 140106807853216
	140106807853312 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809815776 -> 140106807853312
	140106809818800 -> 140106809818704
	140106809818800 -> 140107934927344 [dir=none]
	140107934927344 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809818800 -> 140106814117408 [dir=none]
	140106814117408 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809818800 -> 140107065195104 [dir=none]
	140107065195104 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106809818800 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809818944 -> 140106809818800
	140107065195104 [label="enc_p.0.enc.in_layers.7.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065195104 -> 140106809818944
	140106809818944 [label=AccumulateGrad]
	140106809818992 -> 140106809818800
	140107934927344 [label="enc_p.0.enc.in_layers.7.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107934927344 -> 140106809818992
	140106809818992 [label=AccumulateGrad]
	140106809818752 -> 140106809818704
	140107065194944 [label="enc_p.0.enc.in_layers.7.bias
 (384)" fillcolor=lightblue]
	140107065194944 -> 140106809818752
	140106809818752 [label=AccumulateGrad]
	140106809818656 -> 140106809818608
	140106809818656 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106809818896 -> 140106809818656
	140106809818896 [label="SliceBackward0
----------------------------
dim           :            1
end           :         3072
self_sym_sizes: (7, 6144, 1)
start         :         2688
step          :            1"]
	140106807853360 -> 140106809818896
	140106807853360 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807853360
	140106809817648 -> 140106809817744
	140106809817648 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106809818560 -> 140106809817648
	140106809818560 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106809818512 -> 140106809818560
	140106809816640 -> 140106809816592
	140106809816640 -> 140107065194624 [dir=none]
	140107065194624 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106809816640 -> 140106814117088 [dir=none]
	140106814117088 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106809816640 -> 140107065195504 [dir=none]
	140107065195504 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106809816640 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809818464 -> 140106809816640
	140107065195504 [label="enc_p.0.enc.res_skip_layers.7.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107065195504 -> 140106809818464
	140106809818464 [label=AccumulateGrad]
	140106809817696 -> 140106809816640
	140107065194624 [label="enc_p.0.enc.res_skip_layers.7.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065194624 -> 140106809817696
	140106809817696 [label=AccumulateGrad]
	140106809807664 -> 140106809816592
	140107065195264 [label="enc_p.0.enc.res_skip_layers.7.bias
 (384)" fillcolor=lightblue]
	140107065195264 -> 140106809807664
	140106809807664 [label=AccumulateGrad]
	140106809807424 -> 140106809807328
	140106809807424 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809815824 -> 140106809807424
	140106809815824 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809818224 -> 140106809815824
	140106809818224 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809818368 -> 140106809818224
	140106809818368 -> 140107065194224 [dir=none]
	140107065194224 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809818368 -> 140107019749712 [dir=none]
	140107019749712 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106809818368 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807853456 -> 140106809818368
	140106807853456 [label=CppFunction]
	140106807853552 -> 140106807853456
	140106807853552 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807853696 -> 140106807853552
	140106807853696 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807853792 -> 140106807853696
	140106807853792 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807853888 -> 140106807853792
	140106807853888 [label="AddBackward0
------------
alpha: 1"]
	140106807853984 -> 140106807853888
	140106807853984 -> 140107019749312 [dir=none]
	140107019749312 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807853984 -> 140107065194784 [dir=none]
	140107065194784 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807853984 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807854128 -> 140106807853984
	140106807854128 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807854128 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807854320 -> 140106807854128
	140106807854320 [label="AddBackward0
------------
alpha: 1"]
	140106809818848 -> 140106807854320
	140106807854416 -> 140106807854320
	140106807854416 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807854512 -> 140106807854416
	140106807854512 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807854608 -> 140106807854512
	140106807854608 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809816592 -> 140106807854608
	140106807854080 -> 140106807853984
	140106807854080 -> 140107065195344 [dir=none]
	140107065195344 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807854080 -> 140106814117888 [dir=none]
	140106814117888 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807854080 -> 140107065196224 [dir=none]
	140107065196224 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807854080 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807854464 -> 140106807854080
	140107065196224 [label="enc_p.0.enc.in_layers.8.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107065196224 -> 140106807854464
	140106807854464 [label=AccumulateGrad]
	140106807854368 -> 140106807854080
	140107065195344 [label="enc_p.0.enc.in_layers.8.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065195344 -> 140106807854368
	140106807854368 [label=AccumulateGrad]
	140106807854032 -> 140106807853984
	140107065195584 [label="enc_p.0.enc.in_layers.8.bias
 (384)" fillcolor=lightblue]
	140107065195584 -> 140106807854032
	140106807854032 [label=AccumulateGrad]
	140106807853936 -> 140106807853888
	140106807853936 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807854560 -> 140106807853936
	140106807854560 [label="SliceBackward0
----------------------------
dim           :            1
end           :         3456
self_sym_sizes: (7, 6144, 1)
start         :         3072
step          :            1"]
	140106807854656 -> 140106807854560
	140106807854656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807854656
	140106807853504 -> 140106807853456
	140106807853504 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807853840 -> 140106807853504
	140106807853840 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807853792 -> 140106807853840
	140106807853264 -> 140106809818368
	140106807853264 -> 140107065195744 [dir=none]
	140107065195744 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807853264 -> 140106814117968 [dir=none]
	140106814117968 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807853264 -> 140107058250192 [dir=none]
	140107058250192 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807853264 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807854704 -> 140106807853264
	140107058250192 [label="enc_p.0.enc.res_skip_layers.8.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058250192 -> 140106807854704
	140106807854704 [label=AccumulateGrad]
	140106807853744 -> 140106807853264
	140107065195744 [label="enc_p.0.enc.res_skip_layers.8.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065195744 -> 140106807853744
	140106807853744 [label=AccumulateGrad]
	140106807853168 -> 140106809818368
	140107058249792 [label="enc_p.0.enc.res_skip_layers.8.bias
 (384)" fillcolor=lightblue]
	140107058249792 -> 140106807853168
	140106807853168 [label=AccumulateGrad]
	140106809807280 -> 140106809807184
	140106809807280 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809817120 -> 140106809807280
	140106809817120 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809807520 -> 140106809817120
	140106809807520 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807853600 -> 140106809807520
	140106807853600 -> 140107065195024 [dir=none]
	140107065195024 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807853600 -> 140107019750112 [dir=none]
	140107019750112 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106807853600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807854272 -> 140106807853600
	140106807854272 [label=CppFunction]
	140106807854800 -> 140106807854272
	140106807854800 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807854944 -> 140106807854800
	140106807854944 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807855040 -> 140106807854944
	140106807855040 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807855136 -> 140106807855040
	140106807855136 [label="AddBackward0
------------
alpha: 1"]
	140106807855232 -> 140106807855136
	140106807855232 -> 140107019750032 [dir=none]
	140107019750032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807855232 -> 140107065195424 [dir=none]
	140107065195424 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807855232 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807855376 -> 140106807855232
	140106807855376 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807855376 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807855568 -> 140106807855376
	140106807855568 [label="AddBackward0
------------
alpha: 1"]
	140106807854128 -> 140106807855568
	140106807855664 -> 140106807855568
	140106807855664 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807855760 -> 140106807855664
	140106807855760 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807855856 -> 140106807855760
	140106807855856 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106809818368 -> 140106807855856
	140106807855328 -> 140106807855232
	140106807855328 -> 140107065196384 [dir=none]
	140107065196384 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807855328 -> 140106814116128 [dir=none]
	140106814116128 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807855328 -> 140107058250672 [dir=none]
	140107058250672 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807855328 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106809817072 -> 140106807855328
	140107058250672 [label="enc_p.0.enc.in_layers.9.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058250672 -> 140106809817072
	140106809817072 [label=AccumulateGrad]
	140106809818320 -> 140106807855328
	140107065196384 [label="enc_p.0.enc.in_layers.9.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107065196384 -> 140106809818320
	140106809818320 [label=AccumulateGrad]
	140106807855280 -> 140106807855232
	140107058250272 [label="enc_p.0.enc.in_layers.9.bias
 (384)" fillcolor=lightblue]
	140107058250272 -> 140106807855280
	140106807855280 [label=AccumulateGrad]
	140106807855184 -> 140106807855136
	140106807855184 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807855616 -> 140106807855184
	140106807855616 [label="SliceBackward0
----------------------------
dim           :            1
end           :         3840
self_sym_sizes: (7, 6144, 1)
start         :         3456
step          :            1"]
	140106807855808 -> 140106807855616
	140106807855808 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807855808
	140106807854176 -> 140106807854272
	140106807854176 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807855088 -> 140106807854176
	140106807855088 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807855040 -> 140106807855088
	140106807853408 -> 140106807853600
	140106807853408 -> 140107934852224 [dir=none]
	140107934852224 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807853408 -> 140106814118208 [dir=none]
	140106814118208 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807853408 -> 140107058251152 [dir=none]
	140107058251152 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807853408 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807855520 -> 140106807853408
	140107058251152 [label="enc_p.0.enc.res_skip_layers.9.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058251152 -> 140106807855520
	140106807855520 [label=AccumulateGrad]
	140106807854992 -> 140106807853408
	140107934852224 [label="enc_p.0.enc.res_skip_layers.9.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107934852224 -> 140106807854992
	140106807854992 [label=AccumulateGrad]
	140106807853120 -> 140106807853600
	140107058251072 [label="enc_p.0.enc.res_skip_layers.9.bias
 (384)" fillcolor=lightblue]
	140107058251072 -> 140106807853120
	140106807853120 [label=AccumulateGrad]
	140106809807136 -> 140106809807040
	140106809807136 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809807376 -> 140106809807136
	140106809807376 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809807232 -> 140106809807376
	140106809807232 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807854848 -> 140106809807232
	140106807854848 -> 140107065195824 [dir=none]
	140107065195824 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807854848 -> 140107019750432 [dir=none]
	140107019750432 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106807854848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807855712 -> 140106807854848
	140106807855712 [label=CppFunction]
	140106807855472 -> 140106807855712
	140106807855472 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807856096 -> 140106807855472
	140106807856096 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807856192 -> 140106807856096
	140106807856192 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807856288 -> 140106807856192
	140106807856288 [label="AddBackward0
------------
alpha: 1"]
	140106807856384 -> 140106807856288
	140106807856384 -> 140107019750192 [dir=none]
	140107019750192 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807856384 -> 140107058249872 [dir=none]
	140107058249872 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807856384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807856528 -> 140106807856384
	140106807856528 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807856528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807856720 -> 140106807856528
	140106807856720 [label="AddBackward0
------------
alpha: 1"]
	140106807855376 -> 140106807856720
	140106807856816 -> 140106807856720
	140106807856816 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807856912 -> 140106807856816
	140106807856912 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807857056 -> 140106807856912
	140106807857056 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807853600 -> 140106807857056
	140106807856480 -> 140106807856384
	140106807856480 -> 140107058250512 [dir=none]
	140107058250512 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807856480 -> 140106814117568 [dir=none]
	140106814117568 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807856480 -> 140107058251552 [dir=none]
	140107058251552 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807856480 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807856864 -> 140106807856480
	140107058251552 [label="enc_p.0.enc.in_layers.10.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058251552 -> 140106807856864
	140106807856864 [label=AccumulateGrad]
	140106807856768 -> 140106807856480
	140107058250512 [label="enc_p.0.enc.in_layers.10.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058250512 -> 140106807856768
	140106807856768 [label=AccumulateGrad]
	140106807856432 -> 140106807856384
	140107058251312 [label="enc_p.0.enc.in_layers.10.bias
 (384)" fillcolor=lightblue]
	140107058251312 -> 140106807856432
	140106807856432 [label=AccumulateGrad]
	140106807856336 -> 140106807856288
	140106807856336 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807857008 -> 140106807856336
	140106807857008 [label="SliceBackward0
----------------------------
dim           :            1
end           :         4224
self_sym_sizes: (7, 6144, 1)
start         :         3840
step          :            1"]
	140106807857104 -> 140106807857008
	140106807857104 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807857104
	140106807855424 -> 140106807855712
	140106807855424 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807856240 -> 140106807855424
	140106807856240 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807856192 -> 140106807856240
	140106807854224 -> 140106807854848
	140106807854224 -> 140107058251392 [dir=none]
	140107058251392 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807854224 -> 140106814118048 [dir=none]
	140106814118048 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807854224 -> 140107058252112 [dir=none]
	140107058252112 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807854224 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807857152 -> 140106807854224
	140107058252112 [label="enc_p.0.enc.res_skip_layers.10.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058252112 -> 140106807857152
	140106807857152 [label=AccumulateGrad]
	140106807856144 -> 140106807854224
	140107058251392 [label="enc_p.0.enc.res_skip_layers.10.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058251392 -> 140106807856144
	140106807856144 [label=AccumulateGrad]
	140106807853648 -> 140106807854848
	140107058251632 [label="enc_p.0.enc.res_skip_layers.10.bias
 (384)" fillcolor=lightblue]
	140107058251632 -> 140106807853648
	140106807853648 [label=AccumulateGrad]
	140106809806992 -> 140106809806896
	140106809806992 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809808672 -> 140106809806992
	140106809808672 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106807855904 -> 140106809808672
	140106807855904 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807856000 -> 140106807855904
	140106807856000 -> 140107058250592 [dir=none]
	140107058250592 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807856000 -> 140107019752832 [dir=none]
	140107019752832 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106807856000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807856672 -> 140106807856000
	140106807856672 [label=CppFunction]
	140106807857248 -> 140106807856672
	140106807857248 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807857392 -> 140106807857248
	140106807857392 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807857488 -> 140106807857392
	140106807857488 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807857584 -> 140106807857488
	140106807857584 [label="AddBackward0
------------
alpha: 1"]
	140106807857680 -> 140106807857584
	140106807857680 -> 140107019751072 [dir=none]
	140107019751072 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807857680 -> 140107058250832 [dir=none]
	140107058250832 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807857680 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807857776 -> 140106807857680
	140106807857776 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807857776 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807857968 -> 140106807857776
	140106807857968 [label="AddBackward0
------------
alpha: 1"]
	140106807856528 -> 140106807857968
	140106807858064 -> 140106807857968
	140106807858064 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807858160 -> 140106807858064
	140106807858160 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807858256 -> 140106807858160
	140106807858256 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807854848 -> 140106807858256
	140106807857728 -> 140106807857680
	140106807857728 -> 140107058251792 [dir=none]
	140107058251792 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807857728 -> 140106814117328 [dir=none]
	140106814117328 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807857728 -> 140107058253552 [dir=none]
	140107058253552 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807857728 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807858112 -> 140106807857728
	140107058253552 [label="enc_p.0.enc.in_layers.11.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058253552 -> 140106807858112
	140106807858112 [label=AccumulateGrad]
	140106807858016 -> 140106807857728
	140107058251792 [label="enc_p.0.enc.in_layers.11.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058251792 -> 140106807858016
	140106807858016 [label=AccumulateGrad]
	140106807856960 -> 140106807857680
	140107058252832 [label="enc_p.0.enc.in_layers.11.bias
 (384)" fillcolor=lightblue]
	140107058252832 -> 140106807856960
	140106807856960 [label=AccumulateGrad]
	140106807857632 -> 140106807857584
	140106807857632 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807858208 -> 140106807857632
	140106807858208 [label="SliceBackward0
----------------------------
dim           :            1
end           :         4608
self_sym_sizes: (7, 6144, 1)
start         :         4224
step          :            1"]
	140106807858304 -> 140106807858208
	140106807858304 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807858304
	140106807856576 -> 140106807856672
	140106807856576 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807857536 -> 140106807856576
	140106807857536 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807857488 -> 140106807857536
	140106807855952 -> 140106807856000
	140106807855952 -> 140107058252992 [dir=none]
	140107058252992 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807855952 -> 140106814117808 [dir=none]
	140106814117808 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807855952 -> 140107058254672 [dir=none]
	140107058254672 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807855952 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807858352 -> 140106807855952
	140107058254672 [label="enc_p.0.enc.res_skip_layers.11.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058254672 -> 140106807858352
	140106807858352 [label=AccumulateGrad]
	140106807857440 -> 140106807855952
	140107058252992 [label="enc_p.0.enc.res_skip_layers.11.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058252992 -> 140106807857440
	140106807857440 [label=AccumulateGrad]
	140106807854752 -> 140106807856000
	140107058253632 [label="enc_p.0.enc.res_skip_layers.11.bias
 (384)" fillcolor=lightblue]
	140107058253632 -> 140106807854752
	140106807854752 [label=AccumulateGrad]
	140106809806848 -> 140106809806752
	140106809806848 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809807088 -> 140106809806848
	140106809807088 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106807857200 -> 140106809807088
	140106807857200 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807857296 -> 140106807857200
	140106807857296 -> 140107058251472 [dir=none]
	140107058251472 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807857296 -> 140107019753712 [dir=none]
	140107019753712 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106807857296 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807857920 -> 140106807857296
	140106807857920 [label=CppFunction]
	140106807858448 -> 140106807857920
	140106807858448 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807858592 -> 140106807858448
	140106807858592 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807858688 -> 140106807858592
	140106807858688 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807858784 -> 140106807858688
	140106807858784 [label="AddBackward0
------------
alpha: 1"]
	140106807858880 -> 140106807858784
	140106807858880 -> 140107019753552 [dir=none]
	140107019753552 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807858880 -> 140107058252032 [dir=none]
	140107058252032 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807858880 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807859024 -> 140106807858880
	140106807859024 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807859024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807859216 -> 140106807859024
	140106807859216 [label="AddBackward0
------------
alpha: 1"]
	140106807857776 -> 140106807859216
	140106807859312 -> 140106807859216
	140106807859312 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807859408 -> 140106807859312
	140106807859408 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807859504 -> 140106807859408
	140106807859504 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807856000 -> 140106807859504
	140106807858976 -> 140106807858880
	140106807858976 -> 140107058254032 [dir=none]
	140107058254032 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807858976 -> 140106814118288 [dir=none]
	140106814118288 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807858976 -> 140107058255232 [dir=none]
	140107058255232 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807858976 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807859360 -> 140106807858976
	140107058255232 [label="enc_p.0.enc.in_layers.12.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058255232 -> 140106807859360
	140106807859360 [label=AccumulateGrad]
	140106807859264 -> 140106807858976
	140107058254032 [label="enc_p.0.enc.in_layers.12.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058254032 -> 140106807859264
	140106807859264 [label=AccumulateGrad]
	140106807858928 -> 140106807858880
	140107058254752 [label="enc_p.0.enc.in_layers.12.bias
 (384)" fillcolor=lightblue]
	140107058254752 -> 140106807858928
	140106807858928 [label=AccumulateGrad]
	140106807858832 -> 140106807858784
	140106807858832 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807859456 -> 140106807858832
	140106807859456 [label="SliceBackward0
----------------------------
dim           :            1
end           :         4992
self_sym_sizes: (7, 6144, 1)
start         :         4608
step          :            1"]
	140106807859552 -> 140106807859456
	140106807859552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807859552
	140106807857824 -> 140106807857920
	140106807857824 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807858736 -> 140106807857824
	140106807858736 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807858688 -> 140106807858736
	140106807856624 -> 140106807857296
	140106807856624 -> 140107058254912 [dir=none]
	140107058254912 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807856624 -> 140106814118608 [dir=none]
	140106814118608 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807856624 -> 140107058256512 [dir=none]
	140107058256512 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807856624 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807859600 -> 140106807856624
	140107058256512 [label="enc_p.0.enc.res_skip_layers.12.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058256512 -> 140106807859600
	140106807859600 [label=AccumulateGrad]
	140106807858640 -> 140106807856624
	140107058254912 [label="enc_p.0.enc.res_skip_layers.12.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058254912 -> 140106807858640
	140106807858640 [label=AccumulateGrad]
	140106807854896 -> 140106807857296
	140107058255392 [label="enc_p.0.enc.res_skip_layers.12.bias
 (384)" fillcolor=lightblue]
	140107058255392 -> 140106807854896
	140106807854896 [label=AccumulateGrad]
	140106809806704 -> 140106809806608
	140106809806704 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809810256 -> 140106809806704
	140106809810256 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809809920 -> 140106809810256
	140106809809920 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807858400 -> 140106809809920
	140106807858400 -> 140107058253392 [dir=none]
	140107058253392 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807858400 -> 140107019753952 [dir=none]
	140107019753952 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106807858400 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807858496 -> 140106807858400
	140106807858496 [label=CppFunction]
	140106807859120 -> 140106807858496
	140106807859120 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807859744 -> 140106807859120
	140106807859744 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807859840 -> 140106807859744
	140106807859840 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807859936 -> 140106807859840
	140106807859936 [label="AddBackward0
------------
alpha: 1"]
	140106807860032 -> 140106807859936
	140106807860032 -> 140107019753872 [dir=none]
	140107019753872 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807860032 -> 140107058254272 [dir=none]
	140107058254272 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807860032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807860176 -> 140106807860032
	140106807860176 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807860176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807860368 -> 140106807860176
	140106807860368 [label="AddBackward0
------------
alpha: 1"]
	140106807859024 -> 140106807860368
	140106807860464 -> 140106807860368
	140106807860464 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807860560 -> 140106807860464
	140106807860560 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807860656 -> 140106807860560
	140106807860656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807857296 -> 140106807860656
	140106807860128 -> 140106807860032
	140106807860128 -> 140107058255472 [dir=none]
	140107058255472 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807860128 -> 140106814118448 [dir=none]
	140106814118448 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807860128 -> 140107058256912 [dir=none]
	140107058256912 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807860128 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807860512 -> 140106807860128
	140107058256912 [label="enc_p.0.enc.in_layers.13.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058256912 -> 140106807860512
	140106807860512 [label=AccumulateGrad]
	140106807860416 -> 140106807860128
	140107058255472 [label="enc_p.0.enc.in_layers.13.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058255472 -> 140106807860416
	140106807860416 [label=AccumulateGrad]
	140106807860080 -> 140106807860032
	140107058256672 [label="enc_p.0.enc.in_layers.13.bias
 (384)" fillcolor=lightblue]
	140107058256672 -> 140106807860080
	140106807860080 [label=AccumulateGrad]
	140106807859984 -> 140106807859936
	140106807859984 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807860608 -> 140106807859984
	140106807860608 [label="SliceBackward0
----------------------------
dim           :            1
end           :         5376
self_sym_sizes: (7, 6144, 1)
start         :         4992
step          :            1"]
	140106807860704 -> 140106807860608
	140106807860704 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807860704
	140106807859648 -> 140106807858496
	140106807859648 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807859888 -> 140106807859648
	140106807859888 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807859840 -> 140106807859888
	140106807857872 -> 140106807858400
	140106807857872 -> 140107058256752 [dir=none]
	140107058256752 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807857872 -> 140106814118768 [dir=none]
	140106814118768 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807857872 -> 140107058257312 [dir=none]
	140107058257312 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807857872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807860752 -> 140106807857872
	140107058257312 [label="enc_p.0.enc.res_skip_layers.13.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058257312 -> 140106807860752
	140106807860752 [label=AccumulateGrad]
	140106807859792 -> 140106807857872
	140107058256752 [label="enc_p.0.enc.res_skip_layers.13.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058256752 -> 140106807859792
	140106807859792 [label=AccumulateGrad]
	140106807856048 -> 140106807858400
	140107058256992 [label="enc_p.0.enc.res_skip_layers.13.bias
 (384)" fillcolor=lightblue]
	140107058256992 -> 140106807856048
	140106807856048 [label=AccumulateGrad]
	140106809806560 -> 140106809806464
	140106809806560 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106809806800 -> 140106809806560
	140106809806800 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106809806656 -> 140106809806800
	140106809806656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807859072 -> 140106809806656
	140106807859072 -> 140107031828240 [dir=none]
	140107031828240 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807859072 -> 140107019754672 [dir=none]
	140107019754672 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106807859072 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807860320 -> 140106807859072
	140106807860320 [label=CppFunction]
	140106807860992 -> 140106807860320
	140106807860992 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807861136 -> 140106807860992
	140106807861136 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807861232 -> 140106807861136
	140106807861232 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807861328 -> 140106807861232
	140106807861328 [label="AddBackward0
------------
alpha: 1"]
	140106807861424 -> 140106807861328
	140106807861424 -> 140107019754272 [dir=none]
	140107019754272 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807861424 -> 140107058255632 [dir=none]
	140107058255632 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807861424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807861568 -> 140106807861424
	140106807861568 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807861568 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807861760 -> 140106807861568
	140106807861760 [label="AddBackward0
------------
alpha: 1"]
	140106807860176 -> 140106807861760
	140106807861856 -> 140106807861760
	140106807861856 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807861952 -> 140106807861856
	140106807861952 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807862048 -> 140106807861952
	140106807862048 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807858400 -> 140106807862048
	140106807861520 -> 140106807861424
	140106807861520 -> 140107058257072 [dir=none]
	140107058257072 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807861520 -> 140106814118128 [dir=none]
	140106814118128 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807861520 -> 140107058258272 [dir=none]
	140107058258272 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807861520 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807861904 -> 140106807861520
	140107058258272 [label="enc_p.0.enc.in_layers.14.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058258272 -> 140106807861904
	140106807861904 [label=AccumulateGrad]
	140106807861808 -> 140106807861520
	140107058257072 [label="enc_p.0.enc.in_layers.14.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058257072 -> 140106807861808
	140106807861808 [label=AccumulateGrad]
	140106807861472 -> 140106807861424
	140107058257472 [label="enc_p.0.enc.in_layers.14.bias
 (384)" fillcolor=lightblue]
	140107058257472 -> 140106807861472
	140106807861472 [label=AccumulateGrad]
	140106807861376 -> 140106807861328
	140106807861376 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807862000 -> 140106807861376
	140106807862000 [label="SliceBackward0
----------------------------
dim           :            1
end           :         5760
self_sym_sizes: (7, 6144, 1)
start         :         5376
step          :            1"]
	140106807862096 -> 140106807862000
	140106807862096 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807862096
	140106807860896 -> 140106807860320
	140106807860896 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807861280 -> 140106807860896
	140106807861280 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807861232 -> 140106807861280
	140106807859168 -> 140106807859072
	140106807859168 -> 140107058257632 [dir=none]
	140107058257632 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807859168 -> 140106814117488 [dir=none]
	140106814117488 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807859168 -> 140107058258592 [dir=none]
	140107058258592 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106807859168 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807862144 -> 140106807859168
	140107058258592 [label="enc_p.0.enc.res_skip_layers.14.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107058258592 -> 140106807862144
	140106807862144 [label=AccumulateGrad]
	140106807861184 -> 140106807859168
	140107058257632 [label="enc_p.0.enc.res_skip_layers.14.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058257632 -> 140106807861184
	140106807861184 [label=AccumulateGrad]
	140106807857344 -> 140106807859072
	140107058258352 [label="enc_p.0.enc.res_skip_layers.14.bias
 (384)" fillcolor=lightblue]
	140107058258352 -> 140106807857344
	140106807857344 [label=AccumulateGrad]
	140106809806416 -> 140106809806368
	140106809806416 -> 140107058256832 [dir=none]
	140107058256832 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809806416 -> 140107058255152 [dir=none]
	140107058255152 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106809806416 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809806944 -> 140106809806416
	140106809806944 [label=CppFunction]
	140106807861040 -> 140106809806944
	140106807861040 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807862192 -> 140106807861040
	140106807862192 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106807861616 -> 140106807862192
	140106807861616 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807862288 -> 140106807861616
	140106807862288 [label="AddBackward0
------------
alpha: 1"]
	140106807862384 -> 140106807862288
	140106807862384 -> 140107019754512 [dir=none]
	140107019754512 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807862384 -> 140107019754592 [dir=none]
	140107019754592 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106807862384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807862528 -> 140106807862384
	140106807862528 -> 140107031832960 [dir=none]
	140107031832960 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807862528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807862720 -> 140106807862528
	140106807862720 [label="AddBackward0
------------
alpha: 1"]
	140106807861568 -> 140106807862720
	140106807862816 -> 140106807862720
	140106807862816 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106807862912 -> 140106807862816
	140106807862912 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106807863008 -> 140106807862912
	140106807863008 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106807859072 -> 140106807863008
	140106807862480 -> 140106807862384
	140106807862480 -> 140107058258912 [dir=none]
	140107058258912 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106807862480 -> 140106814118848 [dir=none]
	140106814118848 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106807862480 -> 140107058258432 [dir=none]
	140107058258432 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106807862480 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807862864 -> 140106807862480
	140107058258432 [label="enc_p.0.enc.in_layers.15.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107058258432 -> 140106807862864
	140106807862864 [label=AccumulateGrad]
	140106807862768 -> 140106807862480
	140107058258912 [label="enc_p.0.enc.in_layers.15.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107058258912 -> 140106807862768
	140106807862768 [label=AccumulateGrad]
	140106807862432 -> 140106807862384
	140107058258672 [label="enc_p.0.enc.in_layers.15.bias
 (384)" fillcolor=lightblue]
	140107058258672 -> 140106807862432
	140106807862432 [label=AccumulateGrad]
	140106807862336 -> 140106807862288
	140106807862336 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106807862960 -> 140106807862336
	140106807862960 [label="SliceBackward0
----------------------------
dim           :            1
end           :         6144
self_sym_sizes: (7, 6144, 1)
start         :         5760
step          :            1"]
	140106807863056 -> 140106807862960
	140106807863056 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 6144, 1)
start         :                   0
step          :                   1"]
	140106809810496 -> 140106807863056
	140106807861088 -> 140106809806944
	140106807861088 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807862240 -> 140106807861088
	140106807862240 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807861616 -> 140106807862240
	140106809806512 -> 140106809806416
	140106809806512 -> 140107058258752 [dir=none]
	140107058258752 [label="g
 (192, 1, 1)" fillcolor=orange]
	140106809806512 -> 140106814119248 [dir=none]
	140106814119248 [label="result1
 (192, 1, 1)" fillcolor=orange]
	140106809806512 -> 140107058259232 [dir=none]
	140107058259232 [label="v
 (192, 192, 1)" fillcolor=orange]
	140106809806512 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106807863104 -> 140106809806512
	140107058259232 [label="enc_p.0.enc.res_skip_layers.15.weight_v
 (192, 192, 1)" fillcolor=lightblue]
	140107058259232 -> 140106807863104
	140106807863104 [label=AccumulateGrad]
	140106807861664 -> 140106809806512
	140107058258752 [label="enc_p.0.enc.res_skip_layers.15.weight_g
 (192, 1, 1)" fillcolor=lightblue]
	140107058258752 -> 140106807861664
	140106807861664 [label=AccumulateGrad]
	140106807858544 -> 140106809806416
	140107058258992 [label="enc_p.0.enc.res_skip_layers.15.bias
 (192)" fillcolor=lightblue]
	140107058258992 -> 140106807858544
	140106807858544 [label=AccumulateGrad]
	140106809806176 -> 140106809806128
	140106809806176 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551615"]
	140106809806320 -> 140106809806176
	140106809806320 -> 140107058258512 [dir=none]
	140107058258512 [label="mat1
 (7, 512)" fillcolor=orange]
	140106809806320 -> 140106814119408 [dir=none]
	140106814119408 [label="mat2
 (512, 192)" fillcolor=orange]
	140106809806320 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (7, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (512, 192)
mat2_sym_strides:       (1, 512)"]
	140106807860800 -> 140106809806320
	140107053619440 [label="enc_p.1.ge_proj.bias
 (192)" fillcolor=lightblue]
	140107053619440 -> 140106807860800
	140106807860800 [label=AccumulateGrad]
	140106807860944 -> 140106809806320
	140106807860944 [label="SqueezeBackward1
------------------------------------
dim           : 18446744073709551615
self_sym_sizes:          (7, 512, 1)"]
	140107604685488 -> 140106807860944
	140106807859696 -> 140106809806320
	140106807859696 [label=TBackward0]
	140106807862576 -> 140106807859696
	140107053618960 [label="enc_p.1.ge_proj.weight
 (192, 512)" fillcolor=lightblue]
	140107053618960 -> 140106807862576
	140106807862576 [label=AccumulateGrad]
	140106809805888 -> 140106809805792
	140106809805888 -> 140106814118928 [dir=none]
	140106814118928 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106809805888 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106809805936 -> 140106809805888
	140106809805936 -> 140107019760432 [dir=none]
	140107019760432 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809805936 -> 140107058260192 [dir=none]
	140107058260192 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106809805936 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809806272 -> 140106809805936
	140106809806272 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106807863200 -> 140106809806272
	140106807863200 [label=CloneBackward0]
	140106807863296 -> 140106807863200
	140106807863296 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807863488 -> 140106807863296
	140106807863488 [label="AddBackward0
------------
alpha: 1"]
	140106807863584 -> 140106807863488
	140106807863584 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106807863728 -> 140106807863584
	140106807863728 -> 140106814118688 [dir=none]
	140106814118688 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106807863728 -> 140106814119168 [dir=none]
	140106814119168 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106807863728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807863824 -> 140106807863728
	140106807863824 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106807863968 -> 140106807863824
	140106807863968 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106807864064 -> 140106807863968
	140106807864064 -> 140106814119088 [dir=none]
	140106814119088 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106807864064 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106807864160 -> 140106807864064
	140106807864160 -> 140106814119488 [dir=none]
	140106814119488 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106807864160 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106807864256 -> 140106807864160
	140106807864256 -> 140107019755232 [dir=none]
	140107019755232 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106807864256 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106807864352 -> 140106807864256
	140106807864352 [label="AddBackward0
------------
alpha: 1"]
	140106807864448 -> 140106807864352
	140106807864448 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106807864592 -> 140106807864448
	140106807864592 -> 140106814119968 [dir=none]
	140106814119968 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106807864592 -> 140106814119888 [dir=none]
	140106814119888 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106807864592 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807864688 -> 140106807864592
	140106807864688 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807864832 -> 140106807864688
	140106807864832 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807864928 -> 140106807864832
	140106807864928 -> 140106814120048 [dir=none]
	140106814120048 [label="other
 ()" fillcolor=orange]
	140106807864928 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807865024 -> 140106807864928
	140106807865024 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807865120 -> 140106807865024
	140106807865120 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106807865216 -> 140106807865120
	140106807865216 -> 140107031833280 [dir=none]
	140107031833280 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807865216 -> 140107058259392 [dir=none]
	140107058259392 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106807865216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809805840 -> 140106807865216
	140106807865312 -> 140106807865216
	140107058259392 [label="enc_p.1.encoder.attn_layers.0.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058259392 -> 140106807865312
	140106807865312 [label=AccumulateGrad]
	140106807865264 -> 140106807865216
	140107058259312 [label="enc_p.1.encoder.attn_layers.0.conv_q.bias
 (192)" fillcolor=lightblue]
	140107058259312 -> 140106807865264
	140106807865264 [label=AccumulateGrad]
	140106807864640 -> 140106807864592
	140106807864640 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106807864976 -> 140106807864640
	140106807864976 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106807865168 -> 140106807864976
	140106807865168 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106807865408 -> 140106807865168
	140106807865408 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807865456 -> 140106807865408
	140106807865456 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106807865600 -> 140106807865456
	140106807865600 -> 140107031833280 [dir=none]
	140107031833280 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807865600 -> 140107058259472 [dir=none]
	140107058259472 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106807865600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809805840 -> 140106807865600
	140106807865696 -> 140106807865600
	140107058259472 [label="enc_p.1.encoder.attn_layers.0.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058259472 -> 140106807865696
	140106807865696 [label=AccumulateGrad]
	140106807865648 -> 140106807865600
	140107058259552 [label="enc_p.1.encoder.attn_layers.0.conv_k.bias
 (192)" fillcolor=lightblue]
	140107058259552 -> 140106807865648
	140106807865648 [label=AccumulateGrad]
	140106807864400 -> 140106807864352
	140106807864400 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106807864880 -> 140106807864400
	140106807864880 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106807865360 -> 140106807864880
	140106807865360 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106807865552 -> 140106807865360
	140106807865552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106807864784 -> 140106807865552
	140106807864784 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106807865792 -> 140106807864784
	140106807865792 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106807865888 -> 140106807865792
	140106807865888 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106807865984 -> 140106807865888
	140106807865984 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106807866080 -> 140106807865984
	140106807866080 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106807866176 -> 140106807866080
	140106807866176 -> 140106814120208 [dir=none]
	140106814120208 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106807866176 -> 140106814119728 [dir=none]
	140106814119728 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106807866176 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807866272 -> 140106807866176
	140106807866272 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807866416 -> 140106807866272
	140106807866416 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807866512 -> 140106807866416
	140106807866512 -> 140106814119568 [dir=none]
	140106814119568 [label="other
 ()" fillcolor=orange]
	140106807866512 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807865024 -> 140106807866512
	140106807866224 -> 140106807866176
	140106807866224 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106807866608 -> 140106807866224
	140106807866608 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106807866320 -> 140106807866608
	140106807866320 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106807866752 -> 140106807866320
	140106807866752 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106807866800 -> 140106807866752
	140106807866800 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106807866944 -> 140106807866800
	140106807866944 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106807867040 -> 140106807866944
	140107058259072 [label="enc_p.1.encoder.attn_layers.0.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107058259072 -> 140106807867040
	140106807867040 [label=AccumulateGrad]
	140106807863776 -> 140106807863728
	140106807863776 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807864112 -> 140106807863776
	140106807864112 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807864304 -> 140106807864112
	140106807864304 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807864496 -> 140106807864304
	140106807864496 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106807864736 -> 140106807864496
	140106807864736 -> 140107031833280 [dir=none]
	140107031833280 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807864736 -> 140107058259632 [dir=none]
	140107058259632 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106807864736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809805840 -> 140106807864736
	140106807865744 -> 140106807864736
	140107058259632 [label="enc_p.1.encoder.attn_layers.0.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058259632 -> 140106807865744
	140106807865744 [label=AccumulateGrad]
	140106807865504 -> 140106807864736
	140107058260112 [label="enc_p.1.encoder.attn_layers.0.conv_v.bias
 (192)" fillcolor=lightblue]
	140107058260112 -> 140106807865504
	140106807865504 [label=AccumulateGrad]
	140106807863536 -> 140106807863488
	140106807863536 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106807864016 -> 140106807863536
	140106807864016 -> 140106814120848 [dir=none]
	140106814120848 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106807864016 -> 140106814120688 [dir=none]
	140106814120688 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106807864016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807863872 -> 140106807864016
	140106807863872 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106807865840 -> 140106807863872
	140106807865840 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106807866032 -> 140106807865840
	140106807866032 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106807864544 -> 140106807866032
	140106807864544 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106807866560 -> 140106807864544
	140106807866560 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106807866992 -> 140106807866560
	140106807866992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106807867088 -> 140106807866992
	140106807867088 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106807867184 -> 140106807867088
	140106807867184 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106807867280 -> 140106807867184
	140106807867280 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106807867376 -> 140106807867280
	140106807867376 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106807864064 -> 140106807867376
	140106807864208 -> 140106807864016
	140106807864208 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106807866128 -> 140106807864208
	140106807866128 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106807866848 -> 140106807866128
	140106807866848 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106807866368 -> 140106807866848
	140106807866368 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106807867328 -> 140106807866368
	140106807867328 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106807867424 -> 140106807867328
	140107058260672 [label="enc_p.1.encoder.attn_layers.0.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107058260672 -> 140106807867424
	140106807867424 [label=AccumulateGrad]
	140106809805984 -> 140106809805936
	140107058260192 [label="enc_p.1.encoder.attn_layers.0.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058260192 -> 140106809805984
	140106809805984 [label=AccumulateGrad]
	140106807861712 -> 140106809805936
	140107058260592 [label="enc_p.1.encoder.attn_layers.0.conv_o.bias
 (192)" fillcolor=lightblue]
	140107058260592 -> 140106807861712
	140106807861712 [label=AccumulateGrad]
	140106809805552 -> 140106809805456
	140107058260992 [label="enc_p.1.encoder.norm_layers_1.0.gamma
 (192)" fillcolor=lightblue]
	140107058260992 -> 140106809805552
	140106809805552 [label=AccumulateGrad]
	140106809805312 -> 140106809805456
	140107058261152 [label="enc_p.1.encoder.norm_layers_1.0.beta
 (192)" fillcolor=lightblue]
	140107058261152 -> 140106809805312
	140106809805312 [label=AccumulateGrad]
	140106809805264 -> 140106809805168
	140106809805264 -> 140106814120928 [dir=none]
	140106814120928 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106809805264 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106809805600 -> 140106809805264
	140106809805600 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809805600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809805744 -> 140106809805600
	140106809805744 -> 140107019756432 [dir=none]
	140107019756432 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106809805744 -> 140107058261632 [dir=none]
	140107058261632 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106809805744 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809806080 -> 140106809805744
	140106809806080 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106807863440 -> 140106809806080
	140106807863440 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807863440 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807863632 -> 140106807863440
	140106807863632 -> 140106814121168 [dir=none]
	140106814121168 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106807863632 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106807865936 -> 140106807863632
	140106807865936 -> 140106814120528 [dir=none]
	140106814120528 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106807865936 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106807867136 -> 140106807865936
	140106807867136 -> 140107019757072 [dir=none]
	140107019757072 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106807867136 -> 140107058261472 [dir=none]
	140107058261472 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106807867136 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106807867472 -> 140106807867136
	140106807867472 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106807867568 -> 140106807867472
	140106807867568 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807867568 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809805216 -> 140106807867568
	140106807867232 -> 140106807867136
	140107058261472 [label="enc_p.1.encoder.ffn_layers.0.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107058261472 -> 140106807867232
	140106807867232 [label=AccumulateGrad]
	140106807863248 -> 140106807867136
	140107058261552 [label="enc_p.1.encoder.ffn_layers.0.conv_1.bias
 (768)" fillcolor=lightblue]
	140107058261552 -> 140106807863248
	140106807863248 [label=AccumulateGrad]
	140106809805648 -> 140106809805744
	140107058261632 [label="enc_p.1.encoder.ffn_layers.0.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107058261632 -> 140106809805648
	140106809805648 [label=AccumulateGrad]
	140106809805360 -> 140106809805744
	140107058261712 [label="enc_p.1.encoder.ffn_layers.0.conv_2.bias
 (192)" fillcolor=lightblue]
	140107058261712 -> 140106809805360
	140106809805360 [label=AccumulateGrad]
	140106809804928 -> 140106809804832
	140107058261792 [label="enc_p.1.encoder.norm_layers_2.0.gamma
 (192)" fillcolor=lightblue]
	140107058261792 -> 140106809804928
	140106809804928 [label=AccumulateGrad]
	140106809804688 -> 140106809804832
	140107058261872 [label="enc_p.1.encoder.norm_layers_2.0.beta
 (192)" fillcolor=lightblue]
	140107058261872 -> 140106809804688
	140106809804688 [label=AccumulateGrad]
	140106809804640 -> 140106809804544
	140106809804640 -> 140106814121248 [dir=none]
	140106814121248 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106809804640 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106809804976 -> 140106809804640
	140106809804976 -> 140107011972272 [dir=none]
	140107011972272 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809804976 -> 140107058264192 [dir=none]
	140107058264192 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106809804976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809805120 -> 140106809804976
	140106809805120 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106809805696 -> 140106809805120
	140106809805696 [label=CloneBackward0]
	140106807862624 -> 140106809805696
	140106807862624 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807866464 -> 140106807862624
	140106807866464 [label="AddBackward0
------------
alpha: 1"]
	140106807863920 -> 140106807866464
	140106807863920 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106807865072 -> 140106807863920
	140106807865072 -> 140106814120768 [dir=none]
	140106814120768 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106807865072 -> 140106814119808 [dir=none]
	140106814119808 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106807865072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807867760 -> 140106807865072
	140106807867760 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106807867904 -> 140106807867760
	140106807867904 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106807868000 -> 140106807867904
	140106807868000 -> 140106814121488 [dir=none]
	140106814121488 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106807868000 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106807868096 -> 140106807868000
	140106807868096 -> 140106814120128 [dir=none]
	140106814120128 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106807868096 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106807868192 -> 140106807868096
	140106807868192 -> 140107011965872 [dir=none]
	140107011965872 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106807868192 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106807868288 -> 140106807868192
	140106807868288 [label="AddBackward0
------------
alpha: 1"]
	140106807868384 -> 140106807868288
	140106807868384 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106807868528 -> 140106807868384
	140106807868528 -> 140106814121968 [dir=none]
	140106814121968 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106807868528 -> 140106814121888 [dir=none]
	140106814121888 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106807868528 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807868624 -> 140106807868528
	140106807868624 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807868768 -> 140106807868624
	140106807868768 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807868864 -> 140106807868768
	140106807868864 -> 140106814121808 [dir=none]
	140106814121808 [label="other
 ()" fillcolor=orange]
	140106807868864 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807868960 -> 140106807868864
	140106807868960 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807869056 -> 140106807868960
	140106807869056 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106807869152 -> 140106807869056
	140106807869152 -> 140107019763232 [dir=none]
	140107019763232 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807869152 -> 140107058261952 [dir=none]
	140107058261952 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106807869152 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809804592 -> 140106807869152
	140106807869248 -> 140106807869152
	140107058261952 [label="enc_p.1.encoder.attn_layers.1.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058261952 -> 140106807869248
	140106807869248 [label=AccumulateGrad]
	140106807869200 -> 140106807869152
	140107058262672 [label="enc_p.1.encoder.attn_layers.1.conv_q.bias
 (192)" fillcolor=lightblue]
	140107058262672 -> 140106807869200
	140106807869200 [label=AccumulateGrad]
	140106807868576 -> 140106807868528
	140106807868576 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106807868912 -> 140106807868576
	140106807868912 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106807869104 -> 140106807868912
	140106807869104 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106807869344 -> 140106807869104
	140106807869344 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807869392 -> 140106807869344
	140106807869392 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868424864 -> 140106807869392
	140106868424864 -> 140107019763232 [dir=none]
	140107019763232 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868424864 -> 140107058263152 [dir=none]
	140107058263152 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868424864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809804592 -> 140106868424864
	140106868424960 -> 140106868424864
	140107058263152 [label="enc_p.1.encoder.attn_layers.1.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058263152 -> 140106868424960
	140106868424960 [label=AccumulateGrad]
	140106868424912 -> 140106868424864
	140107058263312 [label="enc_p.1.encoder.attn_layers.1.conv_k.bias
 (192)" fillcolor=lightblue]
	140107058263312 -> 140106868424912
	140106868424912 [label=AccumulateGrad]
	140106807868336 -> 140106807868288
	140106807868336 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106807868816 -> 140106807868336
	140106807868816 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106807869296 -> 140106807868816
	140106807869296 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106807868720 -> 140106807869296
	140106807868720 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106807868480 -> 140106807868720
	140106807868480 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106868425056 -> 140106807868480
	140106868425056 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106868425152 -> 140106868425056
	140106868425152 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106868425248 -> 140106868425152
	140106868425248 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106807866896 -> 140106868425248
	140106807866896 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106868425392 -> 140106807866896
	140106868425392 -> 140106814121648 [dir=none]
	140106814121648 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106868425392 -> 140106814122288 [dir=none]
	140106814122288 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868425392 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868425536 -> 140106868425392
	140106868425536 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868425680 -> 140106868425536
	140106868425680 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868425776 -> 140106868425680
	140106868425776 -> 140106814122208 [dir=none]
	140106814122208 [label="other
 ()" fillcolor=orange]
	140106868425776 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807868960 -> 140106868425776
	140106868425440 -> 140106868425392
	140106868425440 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106868425872 -> 140106868425440
	140106868425872 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106868425584 -> 140106868425872
	140106868425584 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868426016 -> 140106868425584
	140106868426016 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868426064 -> 140106868426016
	140106868426064 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868426208 -> 140106868426064
	140106868426208 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868426304 -> 140106868426208
	140107058260752 [label="enc_p.1.encoder.attn_layers.1.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107058260752 -> 140106868426304
	140106868426304 [label=AccumulateGrad]
	140106807867712 -> 140106807865072
	140106807867712 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807868048 -> 140106807867712
	140106807868048 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106807868240 -> 140106807868048
	140106807868240 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807868432 -> 140106807868240
	140106807868432 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106807868672 -> 140106807868432
	140106807868672 -> 140107019763232 [dir=none]
	140107019763232 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106807868672 -> 140107058263392 [dir=none]
	140107058263392 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106807868672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809804592 -> 140106807868672
	140106807867856 -> 140106807868672
	140107058263392 [label="enc_p.1.encoder.attn_layers.1.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058263392 -> 140106807867856
	140106807867856 [label=AccumulateGrad]
	140106868425008 -> 140106807868672
	140107058263632 [label="enc_p.1.encoder.attn_layers.1.conv_v.bias
 (192)" fillcolor=lightblue]
	140107058263632 -> 140106868425008
	140106868425008 [label=AccumulateGrad]
	140106807867520 -> 140106807866464
	140106807867520 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106807867952 -> 140106807867520
	140106807867952 -> 140106814122448 [dir=none]
	140106814122448 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106807867952 -> 140106814122768 [dir=none]
	140106814122768 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106807867952 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106807867808 -> 140106807867952
	140106807867808 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106807869008 -> 140106807867808
	140106807869008 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868425344 -> 140106807869008
	140106868425344 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106868425296 -> 140106868425344
	140106868425296 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868425824 -> 140106868425296
	140106868425824 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868425920 -> 140106868425824
	140106868425920 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868426112 -> 140106868425920
	140106868426112 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106868426400 -> 140106868426112
	140106868426400 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106868425632 -> 140106868426400
	140106868425632 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868426496 -> 140106868425632
	140106868426496 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106807868000 -> 140106868426496
	140106807868144 -> 140106807867952
	140106807868144 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106868424768 -> 140106807868144
	140106868424768 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106868425968 -> 140106868424768
	140106868425968 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868426256 -> 140106868425968
	140106868426256 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868426448 -> 140106868426256
	140106868426448 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868426544 -> 140106868426448
	140107058264832 [label="enc_p.1.encoder.attn_layers.1.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107058264832 -> 140106868426544
	140106868426544 [label=AccumulateGrad]
	140106809805072 -> 140106809804976
	140107058264192 [label="enc_p.1.encoder.attn_layers.1.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107058264192 -> 140106809805072
	140106809805072 [label=AccumulateGrad]
	140106809804736 -> 140106809804976
	140107058264672 [label="enc_p.1.encoder.attn_layers.1.conv_o.bias
 (192)" fillcolor=lightblue]
	140107058264672 -> 140106809804736
	140106809804736 [label=AccumulateGrad]
	140106809804304 -> 140106809804208
	140107058264992 [label="enc_p.1.encoder.norm_layers_1.1.gamma
 (192)" fillcolor=lightblue]
	140107058264992 -> 140106809804304
	140106809804304 [label=AccumulateGrad]
	140106809803968 -> 140106809804208
	140107058265472 [label="enc_p.1.encoder.norm_layers_1.1.beta
 (192)" fillcolor=lightblue]
	140107058265472 -> 140106809803968
	140106809803968 [label=AccumulateGrad]
	140106809803920 -> 140106809803824
	140106809803920 -> 140106814122368 [dir=none]
	140106814122368 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106809803920 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106809804352 -> 140106809803920
	140106809804352 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809804352 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809804496 -> 140106809804352
	140106809804496 -> 140107011972432 [dir=none]
	140107011972432 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106809804496 -> 140107053613680 [dir=none]
	140107053613680 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106809804496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809804784 -> 140106809804496
	140106809804784 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106809805408 -> 140106809804784
	140106809805408 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809805408 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807867664 -> 140106809805408
	140106807867664 -> 140106814121728 [dir=none]
	140106814121728 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106807867664 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106807867616 -> 140106807867664
	140106807867616 -> 140106814119008 [dir=none]
	140106814119008 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106807867616 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106868426160 -> 140106807867616
	140106868426160 -> 140107011971152 [dir=none]
	140107011971152 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106868426160 -> 140107053613200 [dir=none]
	140107053613200 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106868426160 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106868426592 -> 140106868426160
	140106868426592 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106868426688 -> 140106868426592
	140106868426688 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106868426688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809803872 -> 140106868426688
	140106868426352 -> 140106868426160
	140107053613200 [label="enc_p.1.encoder.ffn_layers.1.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107053613200 -> 140106868426352
	140106868426352 [label=AccumulateGrad]
	140106868425200 -> 140106868426160
	140107053613600 [label="enc_p.1.encoder.ffn_layers.1.conv_1.bias
 (768)" fillcolor=lightblue]
	140107053613600 -> 140106868425200
	140106868425200 [label=AccumulateGrad]
	140106809804400 -> 140106809804496
	140107053613680 [label="enc_p.1.encoder.ffn_layers.1.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107053613680 -> 140106809804400
	140106809804400 [label=AccumulateGrad]
	140106809804112 -> 140106809804496
	140107053613760 [label="enc_p.1.encoder.ffn_layers.1.conv_2.bias
 (192)" fillcolor=lightblue]
	140107053613760 -> 140106809804112
	140106809804112 [label=AccumulateGrad]
	140106809803584 -> 140106809803488
	140107053613840 [label="enc_p.1.encoder.norm_layers_2.1.gamma
 (192)" fillcolor=lightblue]
	140107053613840 -> 140106809803584
	140106809803584 [label=AccumulateGrad]
	140106809803344 -> 140106809803488
	140107053613920 [label="enc_p.1.encoder.norm_layers_2.1.beta
 (192)" fillcolor=lightblue]
	140107053613920 -> 140106809803344
	140106809803344 [label=AccumulateGrad]
	140106809803296 -> 140106809803200
	140106809803296 -> 140106725091872 [dir=none]
	140106725091872 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106809803296 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106809803632 -> 140106809803296
	140106809803632 -> 140107011976832 [dir=none]
	140107011976832 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106809803632 -> 140107053614880 [dir=none]
	140107053614880 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106809803632 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809803776 -> 140106809803632
	140106809803776 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106809805024 -> 140106809803776
	140106809805024 [label=CloneBackward0]
	140106809804160 -> 140106809805024
	140106809804160 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106807863152 -> 140106809804160
	140106807863152 [label="AddBackward0
------------
alpha: 1"]
	140106868425104 -> 140106807863152
	140106868425104 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868424816 -> 140106868425104
	140106868424816 -> 140106814122048 [dir=none]
	140106814122048 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106868424816 -> 140106814122848 [dir=none]
	140106814122848 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106868424816 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868426880 -> 140106868424816
	140106868426880 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868427024 -> 140106868426880
	140106868427024 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868427120 -> 140106868427024
	140106868427120 -> 140106814122928 [dir=none]
	140106814122928 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106868427120 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868427216 -> 140106868427120
	140106868427216 -> 140106814121568 [dir=none]
	140106814121568 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106868427216 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106868427312 -> 140106868427216
	140106868427312 -> 140107011973872 [dir=none]
	140107011973872 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106868427312 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106868427408 -> 140106868427312
	140106868427408 [label="AddBackward0
------------
alpha: 1"]
	140106868427504 -> 140106868427408
	140106868427504 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106868427648 -> 140106868427504
	140106868427648 -> 140106814123328 [dir=none]
	140106814123328 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106868427648 -> 140106814123248 [dir=none]
	140106814123248 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868427648 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868427744 -> 140106868427648
	140106868427744 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868427888 -> 140106868427744
	140106868427888 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868427984 -> 140106868427888
	140106868427984 -> 140106814123168 [dir=none]
	140106814123168 [label="other
 ()" fillcolor=orange]
	140106868427984 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868428080 -> 140106868427984
	140106868428080 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868428176 -> 140106868428080
	140106868428176 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868428272 -> 140106868428176
	140106868428272 -> 140107011966672 [dir=none]
	140107011966672 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868428272 -> 140107053614080 [dir=none]
	140107053614080 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868428272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809803248 -> 140106868428272
	140106868428368 -> 140106868428272
	140107053614080 [label="enc_p.1.encoder.attn_layers.2.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053614080 -> 140106868428368
	140106868428368 [label=AccumulateGrad]
	140106868428320 -> 140106868428272
	140107053614160 [label="enc_p.1.encoder.attn_layers.2.conv_q.bias
 (192)" fillcolor=lightblue]
	140107053614160 -> 140106868428320
	140106868428320 [label=AccumulateGrad]
	140106868427696 -> 140106868427648
	140106868427696 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868428032 -> 140106868427696
	140106868428032 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868428224 -> 140106868428032
	140106868428224 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868428464 -> 140106868428224
	140106868428464 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868428512 -> 140106868428464
	140106868428512 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868428656 -> 140106868428512
	140106868428656 -> 140107011966672 [dir=none]
	140107011966672 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868428656 -> 140107053614240 [dir=none]
	140107053614240 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868428656 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809803248 -> 140106868428656
	140106868428752 -> 140106868428656
	140107053614240 [label="enc_p.1.encoder.attn_layers.2.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053614240 -> 140106868428752
	140106868428752 [label=AccumulateGrad]
	140106868428704 -> 140106868428656
	140107053614320 [label="enc_p.1.encoder.attn_layers.2.conv_k.bias
 (192)" fillcolor=lightblue]
	140107053614320 -> 140106868428704
	140106868428704 [label=AccumulateGrad]
	140106868427456 -> 140106868427408
	140106868427456 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106868427936 -> 140106868427456
	140106868427936 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106868428416 -> 140106868427936
	140106868428416 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868428608 -> 140106868428416
	140106868428608 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868427840 -> 140106868428608
	140106868427840 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106868428848 -> 140106868427840
	140106868428848 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106868428944 -> 140106868428848
	140106868428944 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106868429040 -> 140106868428944
	140106868429040 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106868429136 -> 140106868429040
	140106868429136 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106868429232 -> 140106868429136
	140106868429232 -> 140106814123648 [dir=none]
	140106814123648 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106868429232 -> 140106814123008 [dir=none]
	140106814123008 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868429232 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868429328 -> 140106868429232
	140106868429328 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868429472 -> 140106868429328
	140106868429472 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868429568 -> 140106868429472
	140106868429568 -> 140106814123488 [dir=none]
	140106814123488 [label="other
 ()" fillcolor=orange]
	140106868429568 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868428080 -> 140106868429568
	140106868429280 -> 140106868429232
	140106868429280 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106868429664 -> 140106868429280
	140106868429664 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106868429376 -> 140106868429664
	140106868429376 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868429808 -> 140106868429376
	140106868429808 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868429856 -> 140106868429808
	140106868429856 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868430000 -> 140106868429856
	140106868430000 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868430096 -> 140106868430000
	140107058264912 [label="enc_p.1.encoder.attn_layers.2.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107058264912 -> 140106868430096
	140106868430096 [label=AccumulateGrad]
	140106868426832 -> 140106868424816
	140106868426832 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868427168 -> 140106868426832
	140106868427168 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868427360 -> 140106868427168
	140106868427360 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868427552 -> 140106868427360
	140106868427552 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868427792 -> 140106868427552
	140106868427792 -> 140107011966672 [dir=none]
	140107011966672 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868427792 -> 140107053614400 [dir=none]
	140107053614400 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868427792 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809803248 -> 140106868427792
	140106868428800 -> 140106868427792
	140107053614400 [label="enc_p.1.encoder.attn_layers.2.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053614400 -> 140106868428800
	140106868428800 [label=AccumulateGrad]
	140106868428560 -> 140106868427792
	140107053614560 [label="enc_p.1.encoder.attn_layers.2.conv_v.bias
 (192)" fillcolor=lightblue]
	140107053614560 -> 140106868428560
	140106868428560 [label=AccumulateGrad]
	140106868426640 -> 140106807863152
	140106868426640 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868427072 -> 140106868426640
	140106868427072 -> 140106814122688 [dir=none]
	140106814122688 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106868427072 -> 140106814123888 [dir=none]
	140106814123888 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106868427072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868426928 -> 140106868427072
	140106868426928 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868428896 -> 140106868426928
	140106868428896 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868429088 -> 140106868428896
	140106868429088 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106868427600 -> 140106868429088
	140106868427600 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868429616 -> 140106868427600
	140106868429616 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868429712 -> 140106868429616
	140106868429712 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868429904 -> 140106868429712
	140106868429904 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106868430048 -> 140106868429904
	140106868430048 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106868430240 -> 140106868430048
	140106868430240 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868430336 -> 140106868430240
	140106868430336 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106868427120 -> 140106868430336
	140106868427264 -> 140106868427072
	140106868427264 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106868429184 -> 140106868427264
	140106868429184 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106868429760 -> 140106868429184
	140106868429760 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868428128 -> 140106868429760
	140106868428128 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868430288 -> 140106868428128
	140106868430288 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868430384 -> 140106868430288
	140107053615440 [label="enc_p.1.encoder.attn_layers.2.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107053615440 -> 140106868430384
	140106868430384 [label=AccumulateGrad]
	140106809803728 -> 140106809803632
	140107053614880 [label="enc_p.1.encoder.attn_layers.2.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053614880 -> 140106809803728
	140106809803728 [label=AccumulateGrad]
	140106809803392 -> 140106809803632
	140107053615200 [label="enc_p.1.encoder.attn_layers.2.conv_o.bias
 (192)" fillcolor=lightblue]
	140107053615200 -> 140106809803392
	140106809803392 [label=AccumulateGrad]
	140106809802816 -> 140106813176800
	140107053615760 [label="enc_p.1.encoder.norm_layers_1.2.gamma
 (192)" fillcolor=lightblue]
	140107053615760 -> 140106809802816
	140106809802816 [label=AccumulateGrad]
	140106809802864 -> 140106813176800
	140107053615840 [label="enc_p.1.encoder.norm_layers_1.2.beta
 (192)" fillcolor=lightblue]
	140107053615840 -> 140106809802864
	140106809802864 [label=AccumulateGrad]
	140106813162832 -> 140106854698384
	140106813162832 -> 140106814123568 [dir=none]
	140106814123568 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106813162832 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106813174928 -> 140106813162832
	140106813174928 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106813174928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809803152 -> 140106813174928
	140106809803152 -> 140107011976912 [dir=none]
	140107011976912 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106809803152 -> 140107053616320 [dir=none]
	140107053616320 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106809803152 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106809803440 -> 140106809803152
	140106809803440 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106807863680 -> 140106809803440
	140106807863680 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106807863680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106809804448 -> 140106807863680
	140106809804448 -> 140106814124128 [dir=none]
	140106814124128 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106809804448 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868428992 -> 140106809804448
	140106868428992 -> 140106814123808 [dir=none]
	140106814123808 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106868428992 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106868429952 -> 140106868428992
	140106868429952 -> 140107011976512 [dir=none]
	140107011976512 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106868429952 -> 140107053615920 [dir=none]
	140107053615920 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106868429952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106868430432 -> 140106868429952
	140106868430432 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106868430528 -> 140106868430432
	140106868430528 -> 140107058259152 [dir=none]
	140107058259152 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106868430528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106854696848 -> 140106868430528
	140106868429424 -> 140106868429952
	140107053615920 [label="enc_p.1.encoder.ffn_layers.2.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107053615920 -> 140106868429424
	140106868429424 [label=AccumulateGrad]
	140106868425728 -> 140106868429952
	140107053616240 [label="enc_p.1.encoder.ffn_layers.2.conv_1.bias
 (768)" fillcolor=lightblue]
	140107053616240 -> 140106868425728
	140106868425728 [label=AccumulateGrad]
	140106809803056 -> 140106809803152
	140107053616320 [label="enc_p.1.encoder.ffn_layers.2.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107053616320 -> 140106809803056
	140106809803056 [label=AccumulateGrad]
	140106809802960 -> 140106809803152
	140107053616880 [label="enc_p.1.encoder.ffn_layers.2.conv_2.bias
 (192)" fillcolor=lightblue]
	140107053616880 -> 140106809802960
	140106809802960 [label=AccumulateGrad]
	140106854696560 -> 140106854696416
	140107053616960 [label="enc_p.1.encoder.norm_layers_2.2.gamma
 (192)" fillcolor=lightblue]
	140107053616960 -> 140106854696560
	140106854696560 [label=AccumulateGrad]
	140106854695888 -> 140106854696416
	140107053617440 [label="enc_p.1.encoder.norm_layers_2.2.beta
 (192)" fillcolor=lightblue]
	140107053617440 -> 140106854695888
	140106854695888 [label=AccumulateGrad]
	140106854695600 -> 140106854695360
	140107053618640 [label="enc_p.1.out_proj.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053618640 -> 140106854695600
	140106854695600 [label=AccumulateGrad]
	140106854695504 -> 140106854695360
	140107053618880 [label="enc_p.1.out_proj.bias
 (192)" fillcolor=lightblue]
	140107053618880 -> 140106854695504
	140106854695504 [label=AccumulateGrad]
	140106854695216 -> 140106854695168
	140106854695216 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551615"]
	140106854696176 -> 140106854695216
	140106854696176 -> 140106850823488 [dir=none]
	140106850823488 [label="mat1
 (7, 512)" fillcolor=orange]
	140106854696176 -> 140106814123728 [dir=none]
	140106814123728 [label="mat2
 (512, 192)" fillcolor=orange]
	140106854696176 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (7, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (512, 192)
mat2_sym_strides:       (1, 512)"]
	140106854696224 -> 140106854696176
	140107053629200 [label="enc_p.2.ge_proj.bias
 (192)" fillcolor=lightblue]
	140107053629200 -> 140106854696224
	140106854696224 [label=AccumulateGrad]
	140106854696464 -> 140106854696176
	140106854696464 [label="SqueezeBackward1
------------------------------------
dim           : 18446744073709551615
self_sym_sizes:          (7, 512, 1)"]
	140107604685488 -> 140106854696464
	140106854695312 -> 140106854696176
	140106854695312 [label=TBackward0]
	140106813174112 -> 140106854695312
	140107053629120 [label="enc_p.2.ge_proj.weight
 (192, 512)" fillcolor=lightblue]
	140107053629120 -> 140106813174112
	140106813174112 [label=AccumulateGrad]
	140106857856976 -> 140106857856880
	140106857856976 -> 140106814124208 [dir=none]
	140106814124208 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106857856976 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106854694976 -> 140106857856976
	140106854694976 -> 140106840747488 [dir=none]
	140106840747488 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106854694976 -> 140107053621200 [dir=none]
	140107053621200 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106854694976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854695984 -> 140106854694976
	140106854695984 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106854697712 -> 140106854695984
	140106854697712 [label=CloneBackward0]
	140106807862672 -> 140106854697712
	140106807862672 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106809803680 -> 140106807862672
	140106809803680 [label="AddBackward0
------------
alpha: 1"]
	140106868430480 -> 140106809803680
	140106868430480 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868430576 -> 140106868430480
	140106868430576 -> 140106814124368 [dir=none]
	140106814124368 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106868430576 -> 140106814122608 [dir=none]
	140106814122608 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106868430576 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868430672 -> 140106868430576
	140106868430672 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868430816 -> 140106868430672
	140106868430816 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868430912 -> 140106868430816
	140106868430912 -> 140106814124448 [dir=none]
	140106814124448 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106868430912 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868431008 -> 140106868430912
	140106868431008 -> 140106814123968 [dir=none]
	140106814123968 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106868431008 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106868431104 -> 140106868431008
	140106868431104 -> 140106847140128 [dir=none]
	140106847140128 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106868431104 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106868425488 -> 140106868431104
	140106868425488 [label="AddBackward0
------------
alpha: 1"]
	140106868431200 -> 140106868425488
	140106868431200 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106868431344 -> 140106868431200
	140106868431344 -> 140106814124528 [dir=none]
	140106814124528 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106868431344 -> 140106814124688 [dir=none]
	140106814124688 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868431344 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868431440 -> 140106868431344
	140106868431440 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868431584 -> 140106868431440
	140106868431584 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868431680 -> 140106868431584
	140106868431680 -> 140106814124288 [dir=none]
	140106814124288 [label="other
 ()" fillcolor=orange]
	140106868431680 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868431776 -> 140106868431680
	140106868431776 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868431872 -> 140106868431776
	140106868431872 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868431968 -> 140106868431872
	140106868431968 -> 140106847941984 [dir=none]
	140106847941984 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868431968 -> 140107053619520 [dir=none]
	140107053619520 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868431968 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857856928 -> 140106868431968
	140106868432064 -> 140106868431968
	140107053619520 [label="enc_p.2.encoder.attn_layers.0.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053619520 -> 140106868432064
	140106868432064 [label=AccumulateGrad]
	140106868432016 -> 140106868431968
	140107053619680 [label="enc_p.2.encoder.attn_layers.0.conv_q.bias
 (192)" fillcolor=lightblue]
	140107053619680 -> 140106868432016
	140106868432016 [label=AccumulateGrad]
	140106868431392 -> 140106868431344
	140106868431392 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868431728 -> 140106868431392
	140106868431728 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868431920 -> 140106868431728
	140106868431920 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868432160 -> 140106868431920
	140106868432160 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868432208 -> 140106868432160
	140106868432208 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868432352 -> 140106868432208
	140106868432352 -> 140106847941984 [dir=none]
	140106847941984 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868432352 -> 140107053620560 [dir=none]
	140107053620560 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868432352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857856928 -> 140106868432352
	140106868432448 -> 140106868432352
	140107053620560 [label="enc_p.2.encoder.attn_layers.0.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053620560 -> 140106868432448
	140106868432448 [label=AccumulateGrad]
	140106868432400 -> 140106868432352
	140107053620800 [label="enc_p.2.encoder.attn_layers.0.conv_k.bias
 (192)" fillcolor=lightblue]
	140107053620800 -> 140106868432400
	140106868432400 [label=AccumulateGrad]
	140106868431152 -> 140106868425488
	140106868431152 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106868431632 -> 140106868431152
	140106868431632 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106868432112 -> 140106868431632
	140106868432112 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868432304 -> 140106868432112
	140106868432304 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868431536 -> 140106868432304
	140106868431536 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106868432544 -> 140106868431536
	140106868432544 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106868432640 -> 140106868432544
	140106868432640 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106868432736 -> 140106868432640
	140106868432736 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106868432832 -> 140106868432736
	140106868432832 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106868432928 -> 140106868432832
	140106868432928 -> 140106814124848 [dir=none]
	140106814124848 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106868432928 -> 140106814123408 [dir=none]
	140106814123408 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868432928 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868433024 -> 140106868432928
	140106868433024 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868433168 -> 140106868433024
	140106868433168 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868433264 -> 140106868433168
	140106868433264 -> 140106814124048 [dir=none]
	140106814124048 [label="other
 ()" fillcolor=orange]
	140106868433264 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868431776 -> 140106868433264
	140106868432976 -> 140106868432928
	140106868432976 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106868433360 -> 140106868432976
	140106868433360 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106868433072 -> 140106868433360
	140106868433072 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868433504 -> 140106868433072
	140106868433504 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868433552 -> 140106868433504
	140106868433552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868433696 -> 140106868433552
	140106868433696 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868433792 -> 140106868433696
	140107053615520 [label="enc_p.2.encoder.attn_layers.0.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107053615520 -> 140106868433792
	140106868433792 [label=AccumulateGrad]
	140106868430144 -> 140106868430576
	140106868430144 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868430960 -> 140106868430144
	140106868430960 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868430192 -> 140106868430960
	140106868430192 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868431248 -> 140106868430192
	140106868431248 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868431488 -> 140106868431248
	140106868431488 -> 140106847941984 [dir=none]
	140106847941984 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868431488 -> 140107053620960 [dir=none]
	140107053620960 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868431488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857856928 -> 140106868431488
	140106868432496 -> 140106868431488
	140107053620960 [label="enc_p.2.encoder.attn_layers.0.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053620960 -> 140106868432496
	140106868432496 [label=AccumulateGrad]
	140106868432256 -> 140106868431488
	140107053621120 [label="enc_p.2.encoder.attn_layers.0.conv_v.bias
 (192)" fillcolor=lightblue]
	140107053621120 -> 140106868432256
	140106868432256 [label=AccumulateGrad]
	140106868429520 -> 140106809803680
	140106868429520 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868430864 -> 140106868429520
	140106868430864 -> 140106814124928 [dir=none]
	140106814124928 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106868430864 -> 140106814125168 [dir=none]
	140106814125168 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106868430864 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868430720 -> 140106868430864
	140106868430720 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868432592 -> 140106868430720
	140106868432592 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868432784 -> 140106868432592
	140106868432784 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106868431296 -> 140106868432784
	140106868431296 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868433312 -> 140106868431296
	140106868433312 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868433408 -> 140106868433312
	140106868433408 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868433600 -> 140106868433408
	140106868433600 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106868433888 -> 140106868433600
	140106868433888 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106868433936 -> 140106868433888
	140106868433936 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868434032 -> 140106868433936
	140106868434032 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106868430912 -> 140106868434032
	140106868431056 -> 140106868430864
	140106868431056 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106868432880 -> 140106868431056
	140106868432880 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106868433456 -> 140106868432880
	140106868433456 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868433744 -> 140106868433456
	140106868433744 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868433984 -> 140106868433744
	140106868433984 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868434080 -> 140106868433984
	140107053621360 [label="enc_p.2.encoder.attn_layers.0.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107053621360 -> 140106868434080
	140106868434080 [label=AccumulateGrad]
	140106854695792 -> 140106854694976
	140107053621200 [label="enc_p.2.encoder.attn_layers.0.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053621200 -> 140106854695792
	140106854695792 [label=AccumulateGrad]
	140106854695024 -> 140106854694976
	140107053621280 [label="enc_p.2.encoder.attn_layers.0.conv_o.bias
 (192)" fillcolor=lightblue]
	140107053621280 -> 140106854695024
	140106854695024 [label=AccumulateGrad]
	140106857856352 -> 140106857856496
	140107053621520 [label="enc_p.2.encoder.norm_layers_1.0.gamma
 (192)" fillcolor=lightblue]
	140107053621520 -> 140106857856352
	140106857856352 [label=AccumulateGrad]
	140106857856640 -> 140106857856496
	140107053621680 [label="enc_p.2.encoder.norm_layers_1.0.beta
 (192)" fillcolor=lightblue]
	140107053621680 -> 140106857856640
	140106857856640 [label=AccumulateGrad]
	140106857856304 -> 140106857856208
	140106857856304 -> 140106814125088 [dir=none]
	140106814125088 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106857856304 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106857856688 -> 140106857856304
	140106857856688 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857856688 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857856832 -> 140106857856688
	140106857856832 -> 140106841513296 [dir=none]
	140106841513296 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106857856832 -> 140107053622080 [dir=none]
	140107053622080 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106857856832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857856736 -> 140106857856832
	140106857856736 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106809803008 -> 140106857856736
	140106809803008 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106809803008 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868426976 -> 140106809803008
	140106868426976 -> 140106814125408 [dir=none]
	140106814125408 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106868426976 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868432688 -> 140106868426976
	140106868432688 -> 140106814124768 [dir=none]
	140106814124768 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106868432688 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106868433648 -> 140106868432688
	140106868433648 -> 140106840737168 [dir=none]
	140106840737168 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106868433648 -> 140107053621840 [dir=none]
	140107053621840 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106868433648 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106868434128 -> 140106868433648
	140106868434128 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106868434224 -> 140106868434128
	140106868434224 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106868434224 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857856256 -> 140106868434224
	140106868433120 -> 140106868433648
	140107053621840 [label="enc_p.2.encoder.ffn_layers.0.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107053621840 -> 140106868433120
	140106868433120 [label=AccumulateGrad]
	140106868426736 -> 140106868433648
	140107053622000 [label="enc_p.2.encoder.ffn_layers.0.conv_1.bias
 (768)" fillcolor=lightblue]
	140107053622000 -> 140106868426736
	140106868426736 [label=AccumulateGrad]
	140106857856400 -> 140106857856832
	140107053622080 [label="enc_p.2.encoder.ffn_layers.0.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107053622080 -> 140106857856400
	140106857856400 [label=AccumulateGrad]
	140106854695120 -> 140106857856832
	140107053622160 [label="enc_p.2.encoder.ffn_layers.0.conv_2.bias
 (192)" fillcolor=lightblue]
	140107053622160 -> 140106854695120
	140106854695120 [label=AccumulateGrad]
	140106857855968 -> 140106857855872
	140107053622400 [label="enc_p.2.encoder.norm_layers_2.0.gamma
 (192)" fillcolor=lightblue]
	140107053622400 -> 140106857855968
	140106857855968 [label=AccumulateGrad]
	140106857855728 -> 140106857855872
	140107053622560 [label="enc_p.2.encoder.norm_layers_2.0.beta
 (192)" fillcolor=lightblue]
	140107053622560 -> 140106857855728
	140106857855728 [label=AccumulateGrad]
	140106857855680 -> 140106857855584
	140106857855680 -> 140106814125488 [dir=none]
	140106814125488 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106857855680 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106857856016 -> 140106857855680
	140106857856016 -> 140106809591424 [dir=none]
	140106809591424 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106857856016 -> 140107053624320 [dir=none]
	140107053624320 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106857856016 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857856160 -> 140106857856016
	140106857856160 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106854697136 -> 140106857856160
	140106854697136 [label=CloneBackward0]
	140106809803104 -> 140106854697136
	140106809803104 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868433216 -> 140106809803104
	140106868433216 [label="AddBackward0
------------
alpha: 1"]
	140106868430768 -> 140106868433216
	140106868430768 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868431824 -> 140106868430768
	140106868431824 -> 140106814125248 [dir=none]
	140106814125248 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106868431824 -> 140106814124608 [dir=none]
	140106814124608 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106868431824 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868434416 -> 140106868431824
	140106868434416 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868434560 -> 140106868434416
	140106868434560 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868434656 -> 140106868434560
	140106868434656 -> 140106814125648 [dir=none]
	140106814125648 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106868434656 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868434752 -> 140106868434656
	140106868434752 -> 140106814125008 [dir=none]
	140106814125008 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106868434752 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106868434848 -> 140106868434752
	140106868434848 -> 140106835514672 [dir=none]
	140106835514672 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106868434848 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106868434944 -> 140106868434848
	140106868434944 [label="AddBackward0
------------
alpha: 1"]
	140106868435040 -> 140106868434944
	140106868435040 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106868435184 -> 140106868435040
	140106868435184 -> 140106814126048 [dir=none]
	140106814126048 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106868435184 -> 140106814125968 [dir=none]
	140106814125968 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868435184 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868435280 -> 140106868435184
	140106868435280 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868435424 -> 140106868435280
	140106868435424 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868435520 -> 140106868435424
	140106868435520 -> 140106814125888 [dir=none]
	140106814125888 [label="other
 ()" fillcolor=orange]
	140106868435520 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868435616 -> 140106868435520
	140106868435616 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868435712 -> 140106868435616
	140106868435712 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868435808 -> 140106868435712
	140106868435808 -> 140106838051392 [dir=none]
	140106838051392 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868435808 -> 140107053622800 [dir=none]
	140107053622800 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868435808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857855632 -> 140106868435808
	140106868435904 -> 140106868435808
	140107053622800 [label="enc_p.2.encoder.attn_layers.1.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053622800 -> 140106868435904
	140106868435904 [label=AccumulateGrad]
	140106868435856 -> 140106868435808
	140107053622880 [label="enc_p.2.encoder.attn_layers.1.conv_q.bias
 (192)" fillcolor=lightblue]
	140107053622880 -> 140106868435856
	140106868435856 [label=AccumulateGrad]
	140106868435232 -> 140106868435184
	140106868435232 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868435568 -> 140106868435232
	140106868435568 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868435760 -> 140106868435568
	140106868435760 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868436000 -> 140106868435760
	140106868436000 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868436048 -> 140106868436000
	140106868436048 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868436192 -> 140106868436048
	140106868436192 -> 140106838051392 [dir=none]
	140106838051392 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868436192 -> 140107053623600 [dir=none]
	140107053623600 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868436192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857855632 -> 140106868436192
	140106868436288 -> 140106868436192
	140107053623600 [label="enc_p.2.encoder.attn_layers.1.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053623600 -> 140106868436288
	140106868436288 [label=AccumulateGrad]
	140106868436240 -> 140106868436192
	140107053623680 [label="enc_p.2.encoder.attn_layers.1.conv_k.bias
 (192)" fillcolor=lightblue]
	140107053623680 -> 140106868436240
	140106868436240 [label=AccumulateGrad]
	140106868434992 -> 140106868434944
	140106868434992 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106868435472 -> 140106868434992
	140106868435472 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106868435952 -> 140106868435472
	140106868435952 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868436144 -> 140106868435952
	140106868436144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868435376 -> 140106868436144
	140106868435376 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106868436384 -> 140106868435376
	140106868436384 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106868436480 -> 140106868436384
	140106868436480 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106868436576 -> 140106868436480
	140106868436576 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106868436672 -> 140106868436576
	140106868436672 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106868436768 -> 140106868436672
	140106868436768 -> 140106814126208 [dir=none]
	140106814126208 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106868436768 -> 140106814125568 [dir=none]
	140106814125568 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868436768 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868436864 -> 140106868436768
	140106868436864 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868437008 -> 140106868436864
	140106868437008 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868437104 -> 140106868437008
	140106868437104 -> 140106814125328 [dir=none]
	140106814125328 [label="other
 ()" fillcolor=orange]
	140106868437104 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868435616 -> 140106868437104
	140106868436816 -> 140106868436768
	140106868436816 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106868437200 -> 140106868436816
	140106868437200 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106868436912 -> 140106868437200
	140106868436912 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868437344 -> 140106868436912
	140106868437344 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868437392 -> 140106868437344
	140106868437392 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868437536 -> 140106868437392
	140106868437536 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868437632 -> 140106868437536
	140107053621440 [label="enc_p.2.encoder.attn_layers.1.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107053621440 -> 140106868437632
	140106868437632 [label=AccumulateGrad]
	140106868434368 -> 140106868431824
	140106868434368 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868434704 -> 140106868434368
	140106868434704 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868434896 -> 140106868434704
	140106868434896 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868435088 -> 140106868434896
	140106868435088 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868435328 -> 140106868435088
	140106868435328 -> 140106838051392 [dir=none]
	140106838051392 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868435328 -> 140107053624000 [dir=none]
	140107053624000 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868435328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857855632 -> 140106868435328
	140106868436336 -> 140106868435328
	140107053624000 [label="enc_p.2.encoder.attn_layers.1.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053624000 -> 140106868436336
	140106868436336 [label=AccumulateGrad]
	140106868436096 -> 140106868435328
	140107053624160 [label="enc_p.2.encoder.attn_layers.1.conv_v.bias
 (192)" fillcolor=lightblue]
	140107053624160 -> 140106868436096
	140106868436096 [label=AccumulateGrad]
	140106868434176 -> 140106868433216
	140106868434176 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868434608 -> 140106868434176
	140106868434608 -> 140106814126288 [dir=none]
	140106814126288 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106868434608 -> 140106814126528 [dir=none]
	140106814126528 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106868434608 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868434464 -> 140106868434608
	140106868434464 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868436432 -> 140106868434464
	140106868436432 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868436624 -> 140106868436432
	140106868436624 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106868435136 -> 140106868436624
	140106868435136 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868437152 -> 140106868435136
	140106868437152 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868437248 -> 140106868437152
	140106868437248 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868437440 -> 140106868437248
	140106868437440 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106868437728 -> 140106868437440
	140106868437728 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106868436960 -> 140106868437728
	140106868436960 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868437824 -> 140106868436960
	140106868437824 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106868434656 -> 140106868437824
	140106868434800 -> 140106868434608
	140106868434800 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106868436720 -> 140106868434800
	140106868436720 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106868437296 -> 140106868436720
	140106868437296 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106868437584 -> 140106868437296
	140106868437584 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106868437776 -> 140106868437584
	140106868437776 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106868437872 -> 140106868437776
	140107053624800 [label="enc_p.2.encoder.attn_layers.1.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107053624800 -> 140106868437872
	140106868437872 [label=AccumulateGrad]
	140106857856112 -> 140106857856016
	140107053624320 [label="enc_p.2.encoder.attn_layers.1.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053624320 -> 140106857856112
	140106857856112 [label=AccumulateGrad]
	140106857855776 -> 140106857856016
	140107053624480 [label="enc_p.2.encoder.attn_layers.1.conv_o.bias
 (192)" fillcolor=lightblue]
	140107053624480 -> 140106857855776
	140106857855776 [label=AccumulateGrad]
	140106857855344 -> 140106857855248
	140107053625360 [label="enc_p.2.encoder.norm_layers_1.1.gamma
 (192)" fillcolor=lightblue]
	140107053625360 -> 140106857855344
	140106857855344 [label=AccumulateGrad]
	140106857855104 -> 140106857855248
	140107053625440 [label="enc_p.2.encoder.norm_layers_1.1.beta
 (192)" fillcolor=lightblue]
	140107053625440 -> 140106857855104
	140106857855104 [label=AccumulateGrad]
	140106857855056 -> 140106857854960
	140106857855056 -> 140106814125728 [dir=none]
	140106814125728 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106857855056 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106857855392 -> 140106857855056
	140106857855392 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857855392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857855536 -> 140106857855392
	140106857855536 -> 140106812686880 [dir=none]
	140106812686880 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106857855536 -> 140107053625760 [dir=none]
	140107053625760 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106857855536 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857855824 -> 140106857855536
	140106857855824 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106857856448 -> 140106857855824
	140106857856448 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857856448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868434320 -> 140106857856448
	140106868434320 -> 140106814126768 [dir=none]
	140106814126768 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106868434320 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868436528 -> 140106868434320
	140106868436528 -> 140106814126448 [dir=none]
	140106814126448 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106868436528 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106868437488 -> 140106868436528
	140106868437488 -> 140106834993424 [dir=none]
	140106834993424 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106868437488 -> 140107053625600 [dir=none]
	140107053625600 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106868437488 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106868437920 -> 140106868437488
	140106868437920 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106868438016 -> 140106868437920
	140106868438016 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106868438016 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857855008 -> 140106868438016
	140106868437680 -> 140106868437488
	140107053625600 [label="enc_p.2.encoder.ffn_layers.1.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107053625600 -> 140106868437680
	140106868437680 [label=AccumulateGrad]
	140106868430624 -> 140106868437488
	140107053625680 [label="enc_p.2.encoder.ffn_layers.1.conv_1.bias
 (768)" fillcolor=lightblue]
	140107053625680 -> 140106868430624
	140106868430624 [label=AccumulateGrad]
	140106857855440 -> 140106857855536
	140107053625760 [label="enc_p.2.encoder.ffn_layers.1.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107053625760 -> 140106857855440
	140106857855440 [label=AccumulateGrad]
	140106857855152 -> 140106857855536
	140107053625840 [label="enc_p.2.encoder.ffn_layers.1.conv_2.bias
 (192)" fillcolor=lightblue]
	140107053625840 -> 140106857855152
	140106857855152 [label=AccumulateGrad]
	140106857854720 -> 140106857854624
	140107053625920 [label="enc_p.2.encoder.norm_layers_2.1.gamma
 (192)" fillcolor=lightblue]
	140107053625920 -> 140106857854720
	140106857854720 [label=AccumulateGrad]
	140106857854480 -> 140106857854624
	140107053626000 [label="enc_p.2.encoder.norm_layers_2.1.beta
 (192)" fillcolor=lightblue]
	140107053626000 -> 140106857854480
	140106857854480 [label=AccumulateGrad]
	140106857854432 -> 140106857854336
	140106857854432 -> 140106814126848 [dir=none]
	140106814126848 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106857854432 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106857854768 -> 140106857854432
	140106857854768 -> 140106800812240 [dir=none]
	140106800812240 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106857854768 -> 140107053626640 [dir=none]
	140107053626640 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106857854768 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857854912 -> 140106857854768
	140106857854912 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106857856064 -> 140106857854912
	140106857856064 [label=CloneBackward0]
	140106857856784 -> 140106857856064
	140106857856784 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868437056 -> 140106857856784
	140106868437056 [label="AddBackward0
------------
alpha: 1"]
	140106868434512 -> 140106868437056
	140106868434512 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868433840 -> 140106868434512
	140106868433840 -> 140106814126128 [dir=none]
	140106814126128 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106868433840 -> 140106814126608 [dir=none]
	140106814126608 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106868433840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868438208 -> 140106868433840
	140106868438208 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868438352 -> 140106868438208
	140106868438352 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106868438448 -> 140106868438352
	140106868438448 -> 140106814127008 [dir=none]
	140106814127008 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106868438448 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868438544 -> 140106868438448
	140106868438544 -> 140106814125808 [dir=none]
	140106814125808 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106868438544 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106868438640 -> 140106868438544
	140106868438640 -> 140106809599264 [dir=none]
	140106809599264 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106868438640 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106868438736 -> 140106868438640
	140106868438736 [label="AddBackward0
------------
alpha: 1"]
	140106868438832 -> 140106868438736
	140106868438832 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106868438976 -> 140106868438832
	140106868438976 -> 140106814127408 [dir=none]
	140106814127408 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106868438976 -> 140106814127328 [dir=none]
	140106814127328 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868438976 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868439072 -> 140106868438976
	140106868439072 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868439216 -> 140106868439072
	140106868439216 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868439312 -> 140106868439216
	140106868439312 -> 140106814127248 [dir=none]
	140106814127248 [label="other
 ()" fillcolor=orange]
	140106868439312 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868439408 -> 140106868439312
	140106868439408 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868439504 -> 140106868439408
	140106868439504 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868439600 -> 140106868439504
	140106868439600 -> 140106809596464 [dir=none]
	140106809596464 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868439600 -> 140107053626080 [dir=none]
	140107053626080 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868439600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857854384 -> 140106868439600
	140106868439696 -> 140106868439600
	140107053626080 [label="enc_p.2.encoder.attn_layers.2.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053626080 -> 140106868439696
	140106868439696 [label=AccumulateGrad]
	140106868439648 -> 140106868439600
	140107053626160 [label="enc_p.2.encoder.attn_layers.2.conv_q.bias
 (192)" fillcolor=lightblue]
	140107053626160 -> 140106868439648
	140106868439648 [label=AccumulateGrad]
	140106868439024 -> 140106868438976
	140106868439024 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868439360 -> 140106868439024
	140106868439360 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106868439552 -> 140106868439360
	140106868439552 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868439792 -> 140106868439552
	140106868439792 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868439840 -> 140106868439792
	140106868439840 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868439984 -> 140106868439840
	140106868439984 -> 140106809596464 [dir=none]
	140106809596464 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868439984 -> 140107053626240 [dir=none]
	140107053626240 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868439984 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857854384 -> 140106868439984
	140106868440080 -> 140106868439984
	140107053626240 [label="enc_p.2.encoder.attn_layers.2.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053626240 -> 140106868440080
	140106868440080 [label=AccumulateGrad]
	140106868440032 -> 140106868439984
	140107053626320 [label="enc_p.2.encoder.attn_layers.2.conv_k.bias
 (192)" fillcolor=lightblue]
	140107053626320 -> 140106868440032
	140106868440032 [label=AccumulateGrad]
	140106868438784 -> 140106868438736
	140106868438784 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106868439264 -> 140106868438784
	140106868439264 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106868439744 -> 140106868439264
	140106868439744 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868439936 -> 140106868439744
	140106868439936 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106868439168 -> 140106868439936
	140106868439168 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106868440176 -> 140106868439168
	140106868440176 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106868440272 -> 140106868440176
	140106868440272 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106868440368 -> 140106868440272
	140106868440368 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106868440464 -> 140106868440368
	140106868440464 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106868440608 -> 140106868440464
	140106868440608 -> 140106814127568 [dir=none]
	140106814127568 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106868440608 -> 140106814126928 [dir=none]
	140106814126928 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106868440608 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868440704 -> 140106868440608
	140106868440704 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868440848 -> 140106868440704
	140106868440848 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868440944 -> 140106868440848
	140106868440944 -> 140106814126688 [dir=none]
	140106814126688 [label="other
 ()" fillcolor=orange]
	140106868440944 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868439408 -> 140106868440944
	140106868440656 -> 140106868440608
	140106868440656 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106868441040 -> 140106868440656
	140106868441040 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106868440752 -> 140106868441040
	140106868440752 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106868440800 -> 140106868440752
	140106868440800 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106832134352 -> 140106868440800
	140106832134352 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106832134496 -> 140106832134352
	140106832134496 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106832134592 -> 140106832134496
	140107053625120 [label="enc_p.2.encoder.attn_layers.2.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107053625120 -> 140106832134592
	140106832134592 [label=AccumulateGrad]
	140106868438160 -> 140106868433840
	140106868438160 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868438496 -> 140106868438160
	140106868438496 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106868438688 -> 140106868438496
	140106868438688 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106868438880 -> 140106868438688
	140106868438880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106868439120 -> 140106868438880
	140106868439120 -> 140106809596464 [dir=none]
	140106809596464 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106868439120 -> 140107053626400 [dir=none]
	140107053626400 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106868439120 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857854384 -> 140106868439120
	140106868440128 -> 140106868439120
	140107053626400 [label="enc_p.2.encoder.attn_layers.2.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053626400 -> 140106868440128
	140106868440128 [label=AccumulateGrad]
	140106868439888 -> 140106868439120
	140107053626480 [label="enc_p.2.encoder.attn_layers.2.conv_v.bias
 (192)" fillcolor=lightblue]
	140107053626480 -> 140106868439888
	140106868439888 [label=AccumulateGrad]
	140106868437968 -> 140106868437056
	140106868437968 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106868438400 -> 140106868437968
	140106868438400 -> 140106814127648 [dir=none]
	140106814127648 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106868438400 -> 140106814127488 [dir=none]
	140106814127488 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106868438400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106868438256 -> 140106868438400
	140106868438256 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868440224 -> 140106868438256
	140106868440224 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106868440416 -> 140106868440224
	140106868440416 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106868438928 -> 140106868440416
	140106868438928 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868440992 -> 140106868438928
	140106868440992 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106868439456 -> 140106868440992
	140106868439456 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106832134400 -> 140106868439456
	140106832134400 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106832134688 -> 140106832134400
	140106832134688 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106832134208 -> 140106832134688
	140106832134208 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106832134784 -> 140106832134208
	140106832134784 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106868438448 -> 140106832134784
	140106868438592 -> 140106868438400
	140106868438592 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106868440560 -> 140106868438592
	140106868440560 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106868440896 -> 140106868440560
	140106868440896 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106832134544 -> 140106868440896
	140106832134544 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106832134736 -> 140106832134544
	140106832134736 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106832134832 -> 140106832134736
	140107053626880 [label="enc_p.2.encoder.attn_layers.2.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107053626880 -> 140106832134832
	140106832134832 [label=AccumulateGrad]
	140106857854864 -> 140106857854768
	140107053626640 [label="enc_p.2.encoder.attn_layers.2.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053626640 -> 140106857854864
	140106857854864 [label=AccumulateGrad]
	140106857854528 -> 140106857854768
	140107053626720 [label="enc_p.2.encoder.attn_layers.2.conv_o.bias
 (192)" fillcolor=lightblue]
	140107053626720 -> 140106857854528
	140106857854528 [label=AccumulateGrad]
	140106857854096 -> 140106857854000
	140107053627120 [label="enc_p.2.encoder.norm_layers_1.2.gamma
 (192)" fillcolor=lightblue]
	140107053627120 -> 140106857854096
	140106857854096 [label=AccumulateGrad]
	140106857853856 -> 140106857854000
	140107053627200 [label="enc_p.2.encoder.norm_layers_1.2.beta
 (192)" fillcolor=lightblue]
	140107053627200 -> 140106857853856
	140106857853856 [label=AccumulateGrad]
	140106857853808 -> 140106857853712
	140106857853808 -> 140106814127088 [dir=none]
	140106814127088 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106857853808 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106857854144 -> 140106857853808
	140106857854144 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857854144 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857854288 -> 140106857854144
	140106857854288 -> 140106803962608 [dir=none]
	140106803962608 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106857854288 -> 140107053628160 [dir=none]
	140107053628160 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106857854288 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857854576 -> 140106857854288
	140106857854576 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106857855200 -> 140106857854576
	140106857855200 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857855200 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868438112 -> 140106857855200
	140106868438112 -> 140106814127968 [dir=none]
	140106814127968 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106868438112 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106868440320 -> 140106868438112
	140106868440320 -> 140106814127728 [dir=none]
	140106814127728 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106868440320 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106868438304 -> 140106868440320
	140106868438304 -> 140106809604704 [dir=none]
	140106809604704 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106868438304 -> 140107053627280 [dir=none]
	140107053627280 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106868438304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832134880 -> 140106868438304
	140106832134880 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106832134976 -> 140106832134880
	140106832134976 -> 140106851290944 [dir=none]
	140106851290944 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832134976 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857853760 -> 140106832134976
	140106832134640 -> 140106868438304
	140107053627280 [label="enc_p.2.encoder.ffn_layers.2.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107053627280 -> 140106832134640
	140106832134640 [label=AccumulateGrad]
	140106832134448 -> 140106868438304
	140107053628000 [label="enc_p.2.encoder.ffn_layers.2.conv_1.bias
 (768)" fillcolor=lightblue]
	140107053628000 -> 140106832134448
	140106832134448 [label=AccumulateGrad]
	140106857854192 -> 140106857854288
	140107053628160 [label="enc_p.2.encoder.ffn_layers.2.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107053628160 -> 140106857854192
	140106857854192 [label=AccumulateGrad]
	140106857853904 -> 140106857854288
	140107053628400 [label="enc_p.2.encoder.ffn_layers.2.conv_2.bias
 (192)" fillcolor=lightblue]
	140107053628400 -> 140106857853904
	140106857853904 [label=AccumulateGrad]
	140106857853472 -> 140106857853376
	140107053628480 [label="enc_p.2.encoder.norm_layers_2.2.gamma
 (192)" fillcolor=lightblue]
	140107053628480 -> 140106857853472
	140106857853472 [label=AccumulateGrad]
	140106857853136 -> 140106857853376
	140107053628720 [label="enc_p.2.encoder.norm_layers_2.2.beta
 (192)" fillcolor=lightblue]
	140107053628720 -> 140106857853136
	140106857853136 [label=AccumulateGrad]
	140106857852944 -> 140106857852656
	140107053628960 [label="enc_p.2.out_proj.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107053628960 -> 140106857852944
	140106857852944 [label=AccumulateGrad]
	140106857852848 -> 140106857852656
	140107053629040 [label="enc_p.2.out_proj.bias
 (192)" fillcolor=lightblue]
	140107053629040 -> 140106857852848
	140106857852848 [label=AccumulateGrad]
	140106857852560 -> 140106857852464
	140107053629360 [label="enc_p.2.proj.weight
 (384, 192, 1)" fillcolor=lightblue]
	140107053629360 -> 140106857852560
	140106857852560 [label=AccumulateGrad]
	140106857850688 -> 140106857852464
	140107046551616 [label="enc_p.2.proj.bias
 (384)" fillcolor=lightblue]
	140107046551616 -> 140106857850688
	140106857850688 [label=AccumulateGrad]
	140107604686112 -> 140106857851840
	140106857851648 -> 140106857851552
	140106857851648 -> 140106784286064 [dir=none]
	140106784286064 [label="other
 (7, 192, 286)" fillcolor=orange]
	140106857851648 -> 140106784290224 [dir=none]
	140106784290224 [label="self
 (7, 192, 286)" fillcolor=orange]
	140106857851648 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140106857852176 -> 140106857851648
	140106857852176 -> 140106814126368 [dir=none]
	140106814126368 [label="other
 ()" fillcolor=orange]
	140106857852176 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857853184 -> 140106857852176
	140106857853184 -> 140106761899680 [dir=none]
	140106761899680 [label="self
 (7, 192, 286)" fillcolor=orange]
	140106857853184 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106857853280 -> 140106857853184
	140106857853280 [label="SubBackward0
------------
alpha: 1"]
	140106857853520 -> 140106857853280
	140106857853520 [label="FlipBackward0
-------------
dims: (1,)"]
	140106857853664 -> 140106857853520
	140106857853664 [label="CatBackward0
------------
dim: 1"]
	140106857853952 -> 140106857853664
	140106857853952 [label="SplitWithSizesBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 192, 286)
split_sizes   :      (96, 96)"]
	140106857855488 -> 140106857853952
	140106857855488 [label="FlipBackward0
-------------
dims: (1,)"]
	140106857854240 -> 140106857855488
	140106857854240 [label="CatBackward0
------------
dim: 1"]
	140106868434272 -> 140106857854240
	140106868434272 [label="SplitWithSizesBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 192, 286)
split_sizes   :      (96, 96)"]
	140106832135024 -> 140106868434272
	140106832135024 [label="FlipBackward0
-------------
dims: (1,)"]
	140106832135120 -> 140106832135024
	140106832135120 [label="CatBackward0
------------
dim: 1"]
	140106832135216 -> 140106832135120
	140106832135216 [label="SplitWithSizesBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 192, 286)
split_sizes   :      (96, 96)"]
	140106832135360 -> 140106832135216
	140106832135360 [label="FlipBackward0
-------------
dims: (1,)"]
	140106832135456 -> 140106832135360
	140106832135456 [label="CatBackward0
------------
dim: 1"]
	140106832135552 -> 140106832135456
	140106832135552 [label="SplitWithSizesBackward0
-----------------------------
dim           :             1
self_sym_sizes: (7, 192, 286)
split_sizes   :      (96, 96)"]
	140107604685728 -> 140106832135552
	140106832135504 -> 140106832135456
	140106832135504 [label="AddBackward0
------------
alpha: 1"]
	140106832135600 -> 140106832135504
	140106832135600 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832135600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832135840 -> 140106832135600
	140106832135840 -> 140107053616800 [dir=none]
	140107053616800 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832135840 -> 140107031823040 [dir=none]
	140107031823040 [label="weight
 (96, 192, 1)" fillcolor=orange]
	140106832135840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (96,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106868440512 -> 140106832135840
	140106868440512 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106868440512 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832136032 -> 140106868440512
	140106832136032 [label="AddBackward0
------------
alpha: 1"]
	140106832136128 -> 140106832136032
	140106832136128 [label="AddBackward0
------------
alpha: 1"]
	140106832136272 -> 140106832136128
	140106832136272 [label="AddBackward0
------------
alpha: 1"]
	140106832136416 -> 140106832136272
	140106832136416 [label="AddBackward0
------------
alpha: 1"]
	140106832136560 -> 140106832136416
	140106832136560 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832136656 -> 140106832136560
	140106832136656 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832136752 -> 140106832136656
	140106832136752 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832136848 -> 140106832136752
	140106832136848 -> 140107053613440 [dir=none]
	140107053613440 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832136848 -> 140107053613520 [dir=none]
	140107053613520 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832136848 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832136944 -> 140106832136848
	140106832136944 [label=CppFunction]
	140106832137136 -> 140106832136944
	140106832137136 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832137280 -> 140106832137136
	140106832137280 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832137376 -> 140106832137280
	140106832137376 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832137472 -> 140106832137376
	140106832137472 [label="AddBackward0
------------
alpha: 1"]
	140106832137568 -> 140106832137472
	140106832137568 -> 140107058264752 [dir=none]
	140107058264752 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832137568 -> 140107058265872 [dir=none]
	140107058265872 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832137568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832137712 -> 140106832137568
	140106832137712 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832137712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832137904 -> 140106832137712
	140106832137904 -> 140107058262992 [dir=none]
	140107058262992 [label="input
 (7, 96, 286)" fillcolor=orange]
	140106832137904 -> 140107040385168 [dir=none]
	140107040385168 [label="weight
 (192, 96, 1)" fillcolor=orange]
	140106832137904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832135552 -> 140106832137904
	140106832138000 -> 140106832137904
	140107040385168 [label="flow.flows.0.pre.weight
 (192, 96, 1)" fillcolor=lightblue]
	140107040385168 -> 140106832138000
	140106832138000 [label=AccumulateGrad]
	140106832137952 -> 140106832137904
	140107040385328 [label="flow.flows.0.pre.bias
 (192)" fillcolor=lightblue]
	140107040385328 -> 140106832137952
	140106832137952 [label=AccumulateGrad]
	140106832137664 -> 140106832137568
	140106832137664 -> 140107040385408 [dir=none]
	140107040385408 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832137664 -> 140106752177664 [dir=none]
	140106752177664 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832137664 -> 140107040387168 [dir=none]
	140107040387168 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832137664 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832138048 -> 140106832137664
	140107040387168 [label="flow.flows.0.enc.in_layers.0.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040387168 -> 140106832138048
	140106832138048 [label=AccumulateGrad]
	140106832137808 -> 140106832137664
	140107040385408 [label="flow.flows.0.enc.in_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040385408 -> 140106832137808
	140106832137808 [label=AccumulateGrad]
	140106832137616 -> 140106832137568
	140107040386688 [label="flow.flows.0.enc.in_layers.0.bias
 (384)" fillcolor=lightblue]
	140107040386688 -> 140106832137616
	140106832137616 [label=AccumulateGrad]
	140106832137520 -> 140106832137472
	140106832137520 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832138144 -> 140106832137520
	140106832138144 [label="SliceBackward0
----------------------------
dim           :            1
end           :          384
self_sym_sizes: (7, 1536, 1)
start         :            0
step          :            1"]
	140106832138192 -> 140106832138144
	140106832138192 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832138288 -> 140106832138192
	140106832138288 -> 140107092170592 [dir=none]
	140107092170592 [label="input
 (7, 512, 1)" fillcolor=orange]
	140106832138288 -> 140107058265312 [dir=none]
	140107058265312 [label="weight
 (1536, 512, 1)" fillcolor=orange]
	140106832138288 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1536,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604685488 -> 140106832138288
	140106832138384 -> 140106832138288
	140106832138384 -> 140107040385088 [dir=none]
	140107040385088 [label="g
 (1536, 1, 1)" fillcolor=orange]
	140106832138384 -> 140106814127168 [dir=none]
	140106814127168 [label="result1
 (1536, 1, 1)" fillcolor=orange]
	140106832138384 -> 140107040386288 [dir=none]
	140107040386288 [label="v
 (1536, 512, 1)" fillcolor=orange]
	140106832138384 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832138528 -> 140106832138384
	140107040386288 [label="flow.flows.0.enc.cond_layer.weight_v
 (1536, 512, 1)" fillcolor=lightblue]
	140107040386288 -> 140106832138528
	140106832138528 [label=AccumulateGrad]
	140106832138480 -> 140106832138384
	140107040385088 [label="flow.flows.0.enc.cond_layer.weight_g
 (1536, 1, 1)" fillcolor=lightblue]
	140107040385088 -> 140106832138480
	140106832138480 [label=AccumulateGrad]
	140106832138336 -> 140106832138288
	140107040385888 [label="flow.flows.0.enc.cond_layer.bias
 (1536)" fillcolor=lightblue]
	140107040385888 -> 140106832138336
	140106832138336 [label=AccumulateGrad]
	140106832137088 -> 140106832136944
	140106832137088 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832137424 -> 140106832137088
	140106832137424 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832137376 -> 140106832137424
	140106832136896 -> 140106832136848
	140106832136896 -> 140107040386928 [dir=none]
	140107040386928 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832136896 -> 140106814127808 [dir=none]
	140106814127808 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832136896 -> 140107040388288 [dir=none]
	140107040388288 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832136896 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832138096 -> 140106832136896
	140107040388288 [label="flow.flows.0.enc.res_skip_layers.0.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040388288 -> 140106832138096
	140106832138096 [label=AccumulateGrad]
	140106832137328 -> 140106832136896
	140107040386928 [label="flow.flows.0.enc.res_skip_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040386928 -> 140106832137328
	140106832137328 [label=AccumulateGrad]
	140106832136464 -> 140106832136848
	140107040387408 [label="flow.flows.0.enc.res_skip_layers.0.bias
 (384)" fillcolor=lightblue]
	140107040387408 -> 140106832136464
	140106832136464 [label=AccumulateGrad]
	140106832136368 -> 140106832136272
	140106832136368 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832136704 -> 140106832136368
	140106832136704 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832136992 -> 140106832136704
	140106832136992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832137184 -> 140106832136992
	140106832137184 -> 140107053613360 [dir=none]
	140107053613360 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832137184 -> 140107053615040 [dir=none]
	140107053615040 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832137184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832137856 -> 140106832137184
	140106832137856 [label=CppFunction]
	140106832138576 -> 140106832137856
	140106832138576 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832138720 -> 140106832138576
	140106832138720 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832138816 -> 140106832138720
	140106832138816 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106807866656 -> 140106832138816
	140106807866656 [label="AddBackward0
------------
alpha: 1"]
	140106832138912 -> 140106807866656
	140106832138912 -> 140107053614720 [dir=none]
	140107053614720 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832138912 -> 140107058264432 [dir=none]
	140107058264432 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832138912 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832139056 -> 140106832138912
	140106832139056 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832139056 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832139248 -> 140106832139056
	140106832139248 [label="AddBackward0
------------
alpha: 1"]
	140106832137712 -> 140106832139248
	140106832139344 -> 140106832139248
	140106832139344 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832139440 -> 140106832139344
	140106832139440 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106832139536 -> 140106832139440
	140106832139536 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832136848 -> 140106832139536
	140106832139008 -> 140106832138912
	140106832139008 -> 140107040387568 [dir=none]
	140107040387568 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832139008 -> 140106814127888 [dir=none]
	140106814127888 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832139008 -> 140107040388688 [dir=none]
	140107040388688 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832139008 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832139392 -> 140106832139008
	140107040388688 [label="flow.flows.0.enc.in_layers.1.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040388688 -> 140106832139392
	140106832139392 [label=AccumulateGrad]
	140106832139296 -> 140106832139008
	140107040387568 [label="flow.flows.0.enc.in_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040387568 -> 140106832139296
	140106832139296 [label=AccumulateGrad]
	140106832138960 -> 140106832138912
	140107040388368 [label="flow.flows.0.enc.in_layers.1.bias
 (384)" fillcolor=lightblue]
	140107040388368 -> 140106832138960
	140106832138960 [label=AccumulateGrad]
	140106832138864 -> 140106807866656
	140106832138864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832139488 -> 140106832138864
	140106832139488 [label="SliceBackward0
----------------------------
dim           :            1
end           :          768
self_sym_sizes: (7, 1536, 1)
start         :          384
step          :            1"]
	140106832139584 -> 140106832139488
	140106832139584 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832138288 -> 140106832139584
	140106832138624 -> 140106832137856
	140106832138624 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106807866704 -> 140106832138624
	140106807866704 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832138816 -> 140106807866704
	140106832137040 -> 140106832137184
	140106832137040 -> 140107040388448 [dir=none]
	140107040388448 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832137040 -> 140106814128048 [dir=none]
	140106814128048 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832137040 -> 140107040389248 [dir=none]
	140107040389248 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832137040 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832139632 -> 140106832137040
	140107040389248 [label="flow.flows.0.enc.res_skip_layers.1.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040389248 -> 140106832139632
	140106832139632 [label=AccumulateGrad]
	140106832138768 -> 140106832137040
	140107040388448 [label="flow.flows.0.enc.res_skip_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040388448 -> 140106832138768
	140106832138768 [label=AccumulateGrad]
	140106832136512 -> 140106832137184
	140107040388768 [label="flow.flows.0.enc.res_skip_layers.1.bias
 (384)" fillcolor=lightblue]
	140107040388768 -> 140106832136512
	140106832136512 [label=AccumulateGrad]
	140106832136224 -> 140106832136128
	140106832136224 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832136800 -> 140106832136224
	140106832136800 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832138240 -> 140106832136800
	140106832138240 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832138432 -> 140106832138240
	140106832138432 -> 140107058266032 [dir=none]
	140107058266032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832138432 -> 140107053615600 [dir=none]
	140107053615600 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832138432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832139680 -> 140106832138432
	140106832139680 [label=CppFunction]
	140106832139872 -> 140106832139680
	140106832139872 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832140016 -> 140106832139872
	140106832140016 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832140112 -> 140106832140016
	140106832140112 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832140208 -> 140106832140112
	140106832140208 [label="AddBackward0
------------
alpha: 1"]
	140106832140304 -> 140106832140208
	140106832140304 -> 140107053614000 [dir=none]
	140107053614000 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832140304 -> 140107053614800 [dir=none]
	140107053614800 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832140304 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832140448 -> 140106832140304
	140106832140448 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832140448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832140640 -> 140106832140448
	140106832140640 [label="AddBackward0
------------
alpha: 1"]
	140106832139056 -> 140106832140640
	140106832140736 -> 140106832140640
	140106832140736 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832140832 -> 140106832140736
	140106832140832 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106832140928 -> 140106832140832
	140106832140928 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832137184 -> 140106832140928
	140106832140400 -> 140106832140304
	140106832140400 -> 140107040388848 [dir=none]
	140107040388848 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832140400 -> 140106814123088 [dir=none]
	140106814123088 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832140400 -> 140107040389568 [dir=none]
	140107040389568 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832140400 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832140784 -> 140106832140400
	140107040389568 [label="flow.flows.0.enc.in_layers.2.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107040389568 -> 140106832140784
	140106832140784 [label=AccumulateGrad]
	140106832140688 -> 140106832140400
	140107040388848 [label="flow.flows.0.enc.in_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040388848 -> 140106832140688
	140106832140688 [label=AccumulateGrad]
	140106832140352 -> 140106832140304
	140107040389328 [label="flow.flows.0.enc.in_layers.2.bias
 (384)" fillcolor=lightblue]
	140107040389328 -> 140106832140352
	140106832140352 [label=AccumulateGrad]
	140106832140256 -> 140106832140208
	140106832140256 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832140880 -> 140106832140256
	140106832140880 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1152
self_sym_sizes: (7, 1536, 1)
start         :          768
step          :            1"]
	140106832140976 -> 140106832140880
	140106832140976 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832138288 -> 140106832140976
	140106832139824 -> 140106832139680
	140106832139824 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832140160 -> 140106832139824
	140106832140160 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832140112 -> 140106832140160
	140106832139104 -> 140106832138432
	140106832139104 -> 140107040389408 [dir=none]
	140107040389408 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832139104 -> 140106832790208 [dir=none]
	140106832790208 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832139104 -> 140107040390768 [dir=none]
	140107040390768 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832139104 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832141024 -> 140106832139104
	140107040390768 [label="flow.flows.0.enc.res_skip_layers.2.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107040390768 -> 140106832141024
	140106832141024 [label=AccumulateGrad]
	140106832140064 -> 140106832139104
	140107040389408 [label="flow.flows.0.enc.res_skip_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040389408 -> 140106832140064
	140106832140064 [label=AccumulateGrad]
	140106832136320 -> 140106832138432
	140107040390448 [label="flow.flows.0.enc.res_skip_layers.2.bias
 (384)" fillcolor=lightblue]
	140107040390448 -> 140106832136320
	140106832136320 [label=AccumulateGrad]
	140106832136080 -> 140106832136032
	140106832136080 -> 140107053614640 [dir=none]
	140107053614640 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832136080 -> 140107053617040 [dir=none]
	140107053617040 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106832136080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832137232 -> 140106832136080
	140106832137232 [label=CppFunction]
	140106832139920 -> 140106832137232
	140106832139920 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832141072 -> 140106832139920
	140106832141072 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832140496 -> 140106832141072
	140106832140496 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832141168 -> 140106832140496
	140106832141168 [label="AddBackward0
------------
alpha: 1"]
	140106832141264 -> 140106832141168
	140106832141264 -> 140107053615120 [dir=none]
	140107053615120 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832141264 -> 140107053615360 [dir=none]
	140107053615360 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832141264 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832141408 -> 140106832141264
	140106832141408 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832141408 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832141600 -> 140106832141408
	140106832141600 [label="AddBackward0
------------
alpha: 1"]
	140106832140448 -> 140106832141600
	140106832141696 -> 140106832141600
	140106832141696 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832141792 -> 140106832141696
	140106832141792 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106832141888 -> 140106832141792
	140106832141888 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832138432 -> 140106832141888
	140106832141360 -> 140106832141264
	140106832141360 -> 140107040390048 [dir=none]
	140107040390048 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832141360 -> 140106832790448 [dir=none]
	140106832790448 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832141360 -> 140107031822480 [dir=none]
	140107031822480 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832141360 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832141744 -> 140106832141360
	140107031822480 [label="flow.flows.0.enc.in_layers.3.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031822480 -> 140106832141744
	140106832141744 [label=AccumulateGrad]
	140106832141648 -> 140106832141360
	140107040390048 [label="flow.flows.0.enc.in_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107040390048 -> 140106832141648
	140106832141648 [label=AccumulateGrad]
	140106832141312 -> 140106832141264
	140107040390848 [label="flow.flows.0.enc.in_layers.3.bias
 (384)" fillcolor=lightblue]
	140107040390848 -> 140106832141312
	140106832141312 [label=AccumulateGrad]
	140106832141216 -> 140106832141168
	140106832141216 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832141840 -> 140106832141216
	140106832141840 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1536
self_sym_sizes: (7, 1536, 1)
start         :         1152
step          :            1"]
	140106832141936 -> 140106832141840
	140106832141936 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832138288 -> 140106832141936
	140106832139968 -> 140106832137232
	140106832139968 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832141120 -> 140106832139968
	140106832141120 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832140496 -> 140106832141120
	140106832136608 -> 140106832136080
	140106832136608 -> 140107040390928 [dir=none]
	140107040390928 [label="g
 (192, 1, 1)" fillcolor=orange]
	140106832136608 -> 140106832790608 [dir=none]
	140106832790608 [label="result1
 (192, 1, 1)" fillcolor=orange]
	140106832136608 -> 140107031822800 [dir=none]
	140107031822800 [label="v
 (192, 192, 1)" fillcolor=orange]
	140106832136608 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832141984 -> 140106832136608
	140107031822800 [label="flow.flows.0.enc.res_skip_layers.3.weight_v
 (192, 192, 1)" fillcolor=lightblue]
	140107031822800 -> 140106832141984
	140106832141984 [label=AccumulateGrad]
	140106832140544 -> 140106832136608
	140107040390928 [label="flow.flows.0.enc.res_skip_layers.3.weight_g
 (192, 1, 1)" fillcolor=lightblue]
	140107040390928 -> 140106832140544
	140106832140544 [label=AccumulateGrad]
	140106832136176 -> 140106832136080
	140107031822560 [label="flow.flows.0.enc.res_skip_layers.3.bias
 (192)" fillcolor=lightblue]
	140107031822560 -> 140106832136176
	140106832136176 [label=AccumulateGrad]
	140106868435664 -> 140106832135840
	140107031823040 [label="flow.flows.0.post.weight
 (96, 192, 1)" fillcolor=lightblue]
	140107031823040 -> 140106868435664
	140106868435664 [label=AccumulateGrad]
	140106832135744 -> 140106832135840
	140107031822960 [label="flow.flows.0.post.bias
 (96)" fillcolor=lightblue]
	140107031822960 -> 140106832135744
	140106832135744 [label=AccumulateGrad]
	140106832135648 -> 140106832135504
	140106832135648 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832135648 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832135984 -> 140106832135648
	140106832135984 -> 140107053616560 [dir=none]
	140107053616560 [label="other
 (7, 96, 286)" fillcolor=orange]
	140106832135984 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832135552 -> 140106832135984
	140106832135168 -> 140106832135120
	140106832135168 [label="AddBackward0
------------
alpha: 1"]
	140106832135264 -> 140106832135168
	140106832135264 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832135264 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832138672 -> 140106832135264
	140106832138672 -> 140107053627360 [dir=none]
	140107053627360 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832138672 -> 140107031828720 [dir=none]
	140107031828720 [label="weight
 (96, 192, 1)" fillcolor=orange]
	140106832138672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (96,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832135792 -> 140106832138672
	140106832135792 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832135792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832141504 -> 140106832135792
	140106832141504 [label="AddBackward0
------------
alpha: 1"]
	140106832142080 -> 140106832141504
	140106832142080 [label="AddBackward0
------------
alpha: 1"]
	140106832142224 -> 140106832142080
	140106832142224 [label="AddBackward0
------------
alpha: 1"]
	140106832142368 -> 140106832142224
	140106832142368 [label="AddBackward0
------------
alpha: 1"]
	140106832142512 -> 140106832142368
	140106832142512 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832142608 -> 140106832142512
	140106832142608 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832142704 -> 140106832142608
	140106832142704 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832142800 -> 140106832142704
	140106832142800 -> 140107053621600 [dir=none]
	140107053621600 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832142800 -> 140107053621760 [dir=none]
	140107053621760 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832142800 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832142896 -> 140106832142800
	140106832142896 [label=CppFunction]
	140106832143088 -> 140106832142896
	140106832143088 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832143232 -> 140106832143088
	140106832143232 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832143328 -> 140106832143232
	140106832143328 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832143424 -> 140106832143328
	140106832143424 [label="AddBackward0
------------
alpha: 1"]
	140106832143520 -> 140106832143424
	140106832143520 -> 140107053616080 [dir=none]
	140107053616080 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832143520 -> 140107053619280 [dir=none]
	140107053619280 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832143520 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832143664 -> 140106832143520
	140106832143664 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832143664 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832143856 -> 140106832143664
	140106832143856 -> 140107053617280 [dir=none]
	140107053617280 [label="input
 (7, 96, 286)" fillcolor=orange]
	140106832143856 -> 140107031823120 [dir=none]
	140107031823120 [label="weight
 (192, 96, 1)" fillcolor=orange]
	140106832143856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832135216 -> 140106832143856
	140106832143952 -> 140106832143856
	140107031823120 [label="flow.flows.2.pre.weight
 (192, 96, 1)" fillcolor=lightblue]
	140107031823120 -> 140106832143952
	140106832143952 [label=AccumulateGrad]
	140106832143904 -> 140106832143856
	140107031823440 [label="flow.flows.2.pre.bias
 (192)" fillcolor=lightblue]
	140107031823440 -> 140106832143904
	140106832143904 [label=AccumulateGrad]
	140106832143616 -> 140106832143520
	140106832143616 -> 140107031823280 [dir=none]
	140107031823280 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832143616 -> 140106832790368 [dir=none]
	140106832790368 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832143616 -> 140107031824080 [dir=none]
	140107031824080 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832143616 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832144000 -> 140106832143616
	140107031824080 [label="flow.flows.2.enc.in_layers.0.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031824080 -> 140106832144000
	140106832144000 [label=AccumulateGrad]
	140106832143760 -> 140106832143616
	140107031823280 [label="flow.flows.2.enc.in_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031823280 -> 140106832143760
	140106832143760 [label=AccumulateGrad]
	140106832143568 -> 140106832143520
	140107031823760 [label="flow.flows.2.enc.in_layers.0.bias
 (384)" fillcolor=lightblue]
	140107031823760 -> 140106832143568
	140106832143568 [label=AccumulateGrad]
	140106832143472 -> 140106832143424
	140106832143472 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832144048 -> 140106832143472
	140106832144048 [label="SliceBackward0
----------------------------
dim           :            1
end           :          384
self_sym_sizes: (7, 1536, 1)
start         :            0
step          :            1"]
	140106832144144 -> 140106832144048
	140106832144144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832144240 -> 140106832144144
	140106832144240 -> 140107092170592 [dir=none]
	140107092170592 [label="input
 (7, 512, 1)" fillcolor=orange]
	140106832144240 -> 140107053618400 [dir=none]
	140107053618400 [label="weight
 (1536, 512, 1)" fillcolor=orange]
	140106832144240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1536,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604685488 -> 140106832144240
	140106832144336 -> 140106832144240
	140106832144336 -> 140107031822640 [dir=none]
	140107031822640 [label="g
 (1536, 1, 1)" fillcolor=orange]
	140106832144336 -> 140106832790128 [dir=none]
	140106832790128 [label="result1
 (1536, 1, 1)" fillcolor=orange]
	140106832144336 -> 140107031823680 [dir=none]
	140107031823680 [label="v
 (1536, 512, 1)" fillcolor=orange]
	140106832144336 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832144480 -> 140106832144336
	140107031823680 [label="flow.flows.2.enc.cond_layer.weight_v
 (1536, 512, 1)" fillcolor=lightblue]
	140107031823680 -> 140106832144480
	140106832144480 [label=AccumulateGrad]
	140106832144432 -> 140106832144336
	140107031822640 [label="flow.flows.2.enc.cond_layer.weight_g
 (1536, 1, 1)" fillcolor=lightblue]
	140107031822640 -> 140106832144432
	140106832144432 [label=AccumulateGrad]
	140106832144288 -> 140106832144240
	140107031823520 [label="flow.flows.2.enc.cond_layer.bias
 (1536)" fillcolor=lightblue]
	140107031823520 -> 140106832144288
	140106832144288 [label=AccumulateGrad]
	140106832143040 -> 140106832142896
	140106832143040 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832143376 -> 140106832143040
	140106832143376 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832143328 -> 140106832143376
	140106832142848 -> 140106832142800
	140106832142848 -> 140107031823920 [dir=none]
	140107031823920 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832142848 -> 140106832790288 [dir=none]
	140106832790288 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832142848 -> 140107031824720 [dir=none]
	140107031824720 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832142848 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832144096 -> 140106832142848
	140107031824720 [label="flow.flows.2.enc.res_skip_layers.0.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031824720 -> 140106832144096
	140106832144096 [label=AccumulateGrad]
	140106832143280 -> 140106832142848
	140107031823920 [label="flow.flows.2.enc.res_skip_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031823920 -> 140106832143280
	140106832143280 [label=AccumulateGrad]
	140106832142416 -> 140106832142800
	140107031824240 [label="flow.flows.2.enc.res_skip_layers.0.bias
 (384)" fillcolor=lightblue]
	140107031824240 -> 140106832142416
	140106832142416 [label=AccumulateGrad]
	140106832142320 -> 140106832142224
	140106832142320 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832142656 -> 140106832142320
	140106832142656 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832142944 -> 140106832142656
	140106832142944 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832143136 -> 140106832142944
	140106832143136 -> 140107053620480 [dir=none]
	140107053620480 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832143136 -> 140107053624560 [dir=none]
	140107053624560 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832143136 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832143808 -> 140106832143136
	140106832143808 [label=CppFunction]
	140106832144528 -> 140106832143808
	140106832144528 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832144672 -> 140106832144528
	140106832144672 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832144768 -> 140106832144672
	140106832144768 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832144864 -> 140106832144768
	140106832144864 [label="AddBackward0
------------
alpha: 1"]
	140106832144960 -> 140106832144864
	140106832144960 -> 140107053623040 [dir=none]
	140107053623040 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832144960 -> 140107053614480 [dir=none]
	140107053614480 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832144960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832145104 -> 140106832144960
	140106832145104 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832145104 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832145296 -> 140106832145104
	140106832145296 [label="AddBackward0
------------
alpha: 1"]
	140106832143664 -> 140106832145296
	140106832145392 -> 140106832145296
	140106832145392 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832145488 -> 140106832145392
	140106832145488 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106832145584 -> 140106832145488
	140106832145584 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832142800 -> 140106832145584
	140106832145056 -> 140106832144960
	140106832145056 -> 140107031824480 [dir=none]
	140107031824480 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832145056 -> 140106832789568 [dir=none]
	140106832789568 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832145056 -> 140107031825120 [dir=none]
	140107031825120 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832145056 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832145440 -> 140106832145056
	140107031825120 [label="flow.flows.2.enc.in_layers.1.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031825120 -> 140106832145440
	140106832145440 [label=AccumulateGrad]
	140106832145344 -> 140106832145056
	140107031824480 [label="flow.flows.2.enc.in_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031824480 -> 140106832145344
	140106832145344 [label=AccumulateGrad]
	140106832145008 -> 140106832144960
	140107031824800 [label="flow.flows.2.enc.in_layers.1.bias
 (384)" fillcolor=lightblue]
	140107031824800 -> 140106832145008
	140106832145008 [label=AccumulateGrad]
	140106832144912 -> 140106832144864
	140106832144912 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832145536 -> 140106832144912
	140106832145536 [label="SliceBackward0
----------------------------
dim           :            1
end           :          768
self_sym_sizes: (7, 1536, 1)
start         :          384
step          :            1"]
	140106832145632 -> 140106832145536
	140106832145632 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832144240 -> 140106832145632
	140106832144576 -> 140106832143808
	140106832144576 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832144384 -> 140106832144576
	140106832144384 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832144768 -> 140106832144384
	140106832142992 -> 140106832143136
	140106832142992 -> 140107031824960 [dir=none]
	140107031824960 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832142992 -> 140106832791008 [dir=none]
	140106832791008 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832142992 -> 140107031825760 [dir=none]
	140107031825760 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832142992 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832145200 -> 140106832142992
	140107031825760 [label="flow.flows.2.enc.res_skip_layers.1.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031825760 -> 140106832145200
	140106832145200 [label=AccumulateGrad]
	140106832145680 -> 140106832142992
	140107031824960 [label="flow.flows.2.enc.res_skip_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031824960 -> 140106832145680
	140106832145680 [label=AccumulateGrad]
	140106832142464 -> 140106832143136
	140107031825280 [label="flow.flows.2.enc.res_skip_layers.1.bias
 (384)" fillcolor=lightblue]
	140107031825280 -> 140106832142464
	140106832142464 [label=AccumulateGrad]
	140106832142176 -> 140106832142080
	140106832142176 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832142752 -> 140106832142176
	140106832142752 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832144192 -> 140106832142752
	140106832144192 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832145728 -> 140106832144192
	140106832145728 -> 140107053619600 [dir=none]
	140107053619600 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832145728 -> 140107053625040 [dir=none]
	140107053625040 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832145728 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832145152 -> 140106832145728
	140106832145152 [label=CppFunction]
	140106832145920 -> 140106832145152
	140106832145920 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832146064 -> 140106832145920
	140106832146064 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832146160 -> 140106832146064
	140106832146160 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832146256 -> 140106832146160
	140106832146256 [label="AddBackward0
------------
alpha: 1"]
	140106832146352 -> 140106832146256
	140106832146352 -> 140107053622240 [dir=none]
	140107053622240 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832146352 -> 140107053623920 [dir=none]
	140107053623920 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832146352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832146496 -> 140106832146352
	140106832146496 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832146496 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832146688 -> 140106832146496
	140106832146688 [label="AddBackward0
------------
alpha: 1"]
	140106832145104 -> 140106832146688
	140106832146784 -> 140106832146688
	140106832146784 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832146880 -> 140106832146784
	140106832146880 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106832146976 -> 140106832146880
	140106832146976 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832143136 -> 140106832146976
	140106832146448 -> 140106832146352
	140106832146448 -> 140107031825360 [dir=none]
	140107031825360 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832146448 -> 140106832790688 [dir=none]
	140106832790688 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832146448 -> 140107031826720 [dir=none]
	140107031826720 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832146448 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832146832 -> 140106832146448
	140107031826720 [label="flow.flows.2.enc.in_layers.2.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031826720 -> 140106832146832
	140106832146832 [label=AccumulateGrad]
	140106832146736 -> 140106832146448
	140107031825360 [label="flow.flows.2.enc.in_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031825360 -> 140106832146736
	140106832146736 [label=AccumulateGrad]
	140106832146400 -> 140106832146352
	140107031826320 [label="flow.flows.2.enc.in_layers.2.bias
 (384)" fillcolor=lightblue]
	140107031826320 -> 140106832146400
	140106832146400 [label=AccumulateGrad]
	140106832146304 -> 140106832146256
	140106832146304 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832146928 -> 140106832146304
	140106832146928 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1152
self_sym_sizes: (7, 1536, 1)
start         :          768
step          :            1"]
	140106832147024 -> 140106832146928
	140106832147024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832144240 -> 140106832147024
	140106832145872 -> 140106832145152
	140106832145872 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832146208 -> 140106832145872
	140106832146208 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832146160 -> 140106832146208
	140106832143712 -> 140106832145728
	140106832143712 -> 140107031826560 [dir=none]
	140107031826560 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832143712 -> 140106832791088 [dir=none]
	140106832791088 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832143712 -> 140107031827200 [dir=none]
	140107031827200 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832143712 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832147072 -> 140106832143712
	140107031827200 [label="flow.flows.2.enc.res_skip_layers.2.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031827200 -> 140106832147072
	140106832147072 [label=AccumulateGrad]
	140106832146112 -> 140106832143712
	140107031826560 [label="flow.flows.2.enc.res_skip_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031826560 -> 140106832146112
	140106832146112 [label=AccumulateGrad]
	140106832142272 -> 140106832145728
	140107031826800 [label="flow.flows.2.enc.res_skip_layers.2.bias
 (384)" fillcolor=lightblue]
	140107031826800 -> 140106832142272
	140106832142272 [label=AccumulateGrad]
	140106832141456 -> 140106832141504
	140106832141456 -> 140107053622720 [dir=none]
	140107053622720 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832141456 -> 140107053627520 [dir=none]
	140107053627520 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106832141456 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832143184 -> 140106832141456
	140106832143184 [label=CppFunction]
	140106832145968 -> 140106832143184
	140106832145968 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832147120 -> 140106832145968
	140106832147120 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832146544 -> 140106832147120
	140106832146544 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832147216 -> 140106832146544
	140106832147216 [label="AddBackward0
------------
alpha: 1"]
	140106832147312 -> 140106832147216
	140106832147312 -> 140107053624720 [dir=none]
	140107053624720 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832147312 -> 140107053624960 [dir=none]
	140107053624960 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832147312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832147456 -> 140106832147312
	140106832147456 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832147456 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832147648 -> 140106832147456
	140106832147648 [label="AddBackward0
------------
alpha: 1"]
	140106832146496 -> 140106832147648
	140106832147744 -> 140106832147648
	140106832147744 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832147840 -> 140106832147744
	140106832147840 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106832147936 -> 140106832147840
	140106832147936 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832145728 -> 140106832147936
	140106832147408 -> 140106832147312
	140106832147408 -> 140107031826880 [dir=none]
	140107031826880 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832147408 -> 140106832790768 [dir=none]
	140106832790768 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832147408 -> 140107031827920 [dir=none]
	140107031827920 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832147408 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832147792 -> 140106832147408
	140107031827920 [label="flow.flows.2.enc.in_layers.3.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031827920 -> 140106832147792
	140106832147792 [label=AccumulateGrad]
	140106832147696 -> 140106832147408
	140107031826880 [label="flow.flows.2.enc.in_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031826880 -> 140106832147696
	140106832147696 [label=AccumulateGrad]
	140106832147360 -> 140106832147312
	140107031827360 [label="flow.flows.2.enc.in_layers.3.bias
 (384)" fillcolor=lightblue]
	140107031827360 -> 140106832147360
	140106832147360 [label=AccumulateGrad]
	140106832147264 -> 140106832147216
	140106832147264 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832147888 -> 140106832147264
	140106832147888 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1536
self_sym_sizes: (7, 1536, 1)
start         :         1152
step          :            1"]
	140106832147984 -> 140106832147888
	140106832147984 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832144240 -> 140106832147984
	140106832146016 -> 140106832143184
	140106832146016 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832147168 -> 140106832146016
	140106832147168 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832146544 -> 140106832147168
	140106832142560 -> 140106832141456
	140106832142560 -> 140107031827440 [dir=none]
	140107031827440 [label="g
 (192, 1, 1)" fillcolor=orange]
	140106832142560 -> 140106832790848 [dir=none]
	140106832790848 [label="result1
 (192, 1, 1)" fillcolor=orange]
	140106832142560 -> 140107031828480 [dir=none]
	140107031828480 [label="v
 (192, 192, 1)" fillcolor=orange]
	140106832142560 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832148032 -> 140106832142560
	140107031828480 [label="flow.flows.2.enc.res_skip_layers.3.weight_v
 (192, 192, 1)" fillcolor=lightblue]
	140107031828480 -> 140106832148032
	140106832148032 [label=AccumulateGrad]
	140106832146592 -> 140106832142560
	140107031827440 [label="flow.flows.2.enc.res_skip_layers.3.weight_g
 (192, 1, 1)" fillcolor=lightblue]
	140107031827440 -> 140106832146592
	140106832146592 [label=AccumulateGrad]
	140106832142128 -> 140106832141456
	140107031828080 [label="flow.flows.2.enc.res_skip_layers.3.bias
 (192)" fillcolor=lightblue]
	140107031828080 -> 140106832142128
	140106832142128 [label=AccumulateGrad]
	140106832135936 -> 140106832138672
	140107031828720 [label="flow.flows.2.post.weight
 (96, 192, 1)" fillcolor=lightblue]
	140107031828720 -> 140106832135936
	140106832135936 [label=AccumulateGrad]
	140106832135696 -> 140106832138672
	140107031828560 [label="flow.flows.2.post.bias
 (96)" fillcolor=lightblue]
	140107031828560 -> 140106832135696
	140106832135696 [label=AccumulateGrad]
	140106832135408 -> 140106832135168
	140106832135408 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832135408 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832139728 -> 140106832135408
	140106832139728 -> 140107053627600 [dir=none]
	140107053627600 [label="other
 (7, 96, 286)" fillcolor=orange]
	140106832139728 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832135216 -> 140106832139728
	140106868426784 -> 140106857854240
	140106868426784 [label="AddBackward0
------------
alpha: 1"]
	140106832134304 -> 140106868426784
	140106832134304 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832134304 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832145248 -> 140106832134304
	140106832145248 -> 140107046559056 [dir=none]
	140107046559056 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832145248 -> 140107031835280 [dir=none]
	140107031835280 [label="weight
 (96, 192, 1)" fillcolor=orange]
	140106832145248 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (96,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832135888 -> 140106832145248
	140106832135888 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832135888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832147600 -> 140106832135888
	140106832147600 [label="AddBackward0
------------
alpha: 1"]
	140106832147552 -> 140106832147600
	140106832147552 [label="AddBackward0
------------
alpha: 1"]
	140106832148176 -> 140106832147552
	140106832148176 [label="AddBackward0
------------
alpha: 1"]
	140106832148320 -> 140106832148176
	140106832148320 [label="AddBackward0
------------
alpha: 1"]
	140106832148464 -> 140106832148320
	140106832148464 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832148560 -> 140106832148464
	140106832148560 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832148656 -> 140106832148560
	140106832148656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832148752 -> 140106832148656
	140106832148752 -> 140107046566416 [dir=none]
	140107046566416 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832148752 -> 140107046565936 [dir=none]
	140107046565936 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832148752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832148848 -> 140106832148752
	140106832148848 [label=CppFunction]
	140106832149040 -> 140106832148848
	140106832149040 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832149184 -> 140106832149040
	140106832149184 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106832149280 -> 140106832149184
	140106832149280 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832149376 -> 140106832149280
	140106832149376 [label="AddBackward0
------------
alpha: 1"]
	140106832149472 -> 140106832149376
	140106832149472 -> 140107053626960 [dir=none]
	140107053626960 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832149472 -> 140107053629280 [dir=none]
	140107053629280 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106832149472 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832149616 -> 140106832149472
	140106832149616 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832149616 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832149808 -> 140106832149616
	140106832149808 -> 140107053618560 [dir=none]
	140107053618560 [label="input
 (7, 96, 286)" fillcolor=orange]
	140106832149808 -> 140107031828800 [dir=none]
	140107031828800 [label="weight
 (192, 96, 1)" fillcolor=orange]
	140106832149808 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106868434272 -> 140106832149808
	140106832149904 -> 140106832149808
	140107031828800 [label="flow.flows.4.pre.weight
 (192, 96, 1)" fillcolor=lightblue]
	140107031828800 -> 140106832149904
	140106832149904 [label=AccumulateGrad]
	140106832149856 -> 140106832149808
	140107031828880 [label="flow.flows.4.pre.bias
 (192)" fillcolor=lightblue]
	140107031828880 -> 140106832149856
	140106832149856 [label=AccumulateGrad]
	140106832149568 -> 140106832149472
	140106832149568 -> 140107031828960 [dir=none]
	140107031828960 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832149568 -> 140106832791248 [dir=none]
	140106832791248 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832149568 -> 140107031830160 [dir=none]
	140107031830160 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106832149568 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832149952 -> 140106832149568
	140107031830160 [label="flow.flows.4.enc.in_layers.0.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031830160 -> 140106832149952
	140106832149952 [label=AccumulateGrad]
	140106832149712 -> 140106832149568
	140107031828960 [label="flow.flows.4.enc.in_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031828960 -> 140106832149712
	140106832149712 [label=AccumulateGrad]
	140106832149520 -> 140106832149472
	140107031829680 [label="flow.flows.4.enc.in_layers.0.bias
 (384)" fillcolor=lightblue]
	140107031829680 -> 140106832149520
	140106832149520 [label=AccumulateGrad]
	140106832149424 -> 140106832149376
	140106832149424 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106832150000 -> 140106832149424
	140106832150000 [label="SliceBackward0
----------------------------
dim           :            1
end           :          384
self_sym_sizes: (7, 1536, 1)
start         :            0
step          :            1"]
	140106832150096 -> 140106832150000
	140106832150096 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832150192 -> 140106832150096
	140106832150192 -> 140107092170592 [dir=none]
	140107092170592 [label="input
 (7, 512, 1)" fillcolor=orange]
	140106832150192 -> 140107053627920 [dir=none]
	140107053627920 [label="weight
 (1536, 512, 1)" fillcolor=orange]
	140106832150192 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1536,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604685488 -> 140106832150192
	140106832150288 -> 140106832150192
	140106832150288 -> 140107031828320 [dir=none]
	140107031828320 [label="g
 (1536, 1, 1)" fillcolor=orange]
	140106832150288 -> 140106832791408 [dir=none]
	140106832791408 [label="result1
 (1536, 1, 1)" fillcolor=orange]
	140106832150288 -> 140107031829200 [dir=none]
	140107031829200 [label="v
 (1536, 512, 1)" fillcolor=orange]
	140106832150288 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832150432 -> 140106832150288
	140107031829200 [label="flow.flows.4.enc.cond_layer.weight_v
 (1536, 512, 1)" fillcolor=lightblue]
	140107031829200 -> 140106832150432
	140106832150432 [label=AccumulateGrad]
	140106832150384 -> 140106832150288
	140107031828320 [label="flow.flows.4.enc.cond_layer.weight_g
 (1536, 1, 1)" fillcolor=lightblue]
	140107031828320 -> 140106832150384
	140106832150384 [label=AccumulateGrad]
	140106832150240 -> 140106832150192
	140107031829040 [label="flow.flows.4.enc.cond_layer.bias
 (1536)" fillcolor=lightblue]
	140107031829040 -> 140106832150240
	140106832150240 [label=AccumulateGrad]
	140106832148992 -> 140106832148848
	140106832148992 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106832149328 -> 140106832148992
	140106832149328 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106832149280 -> 140106832149328
	140106832148800 -> 140106832148752
	140106832148800 -> 140107031829840 [dir=none]
	140107031829840 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832148800 -> 140106832791488 [dir=none]
	140106832791488 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832148800 -> 140107031830720 [dir=none]
	140107031830720 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832148800 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832150048 -> 140106832148800
	140107031830720 [label="flow.flows.4.enc.res_skip_layers.0.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031830720 -> 140106832150048
	140106832150048 [label=AccumulateGrad]
	140106832149232 -> 140106832148800
	140107031829840 [label="flow.flows.4.enc.res_skip_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031829840 -> 140106832149232
	140106832149232 [label=AccumulateGrad]
	140106832148368 -> 140106832148752
	140107031830240 [label="flow.flows.4.enc.res_skip_layers.0.bias
 (384)" fillcolor=lightblue]
	140107031830240 -> 140106832148368
	140106832148368 [label=AccumulateGrad]
	140106832148272 -> 140106832148176
	140106832148272 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832148608 -> 140106832148272
	140106832148608 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832148896 -> 140106832148608
	140106832148896 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832149088 -> 140106832148896
	140106832149088 -> 140107046567056 [dir=none]
	140107046567056 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832149088 -> 140107046562896 [dir=none]
	140107046562896 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832149088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832149760 -> 140106832149088
	140106832149760 [label=CppFunction]
	140106832150336 -> 140106832149760
	140106832150336 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804658384 -> 140106832150336
	140106804658384 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804658480 -> 140106804658384
	140106804658480 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804658576 -> 140106804658480
	140106804658576 [label="AddBackward0
------------
alpha: 1"]
	140106804658672 -> 140106804658576
	140106804658672 -> 140107046563856 [dir=none]
	140107046563856 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804658672 -> 140107053625520 [dir=none]
	140107053625520 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804658672 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804658816 -> 140106804658672
	140106804658816 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804658816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804659008 -> 140106804658816
	140106804659008 [label="AddBackward0
------------
alpha: 1"]
	140106832149616 -> 140106804659008
	140106804659104 -> 140106804659008
	140106804659104 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804659200 -> 140106804659104
	140106804659200 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106804659296 -> 140106804659200
	140106804659296 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832148752 -> 140106804659296
	140106804658768 -> 140106804658672
	140106804658768 -> 140107031830400 [dir=none]
	140107031830400 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804658768 -> 140106832790528 [dir=none]
	140106832790528 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804658768 -> 140107031831200 [dir=none]
	140107031831200 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804658768 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106832146640 -> 140106804658768
	140107031831200 [label="flow.flows.4.enc.in_layers.1.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031831200 -> 140106832146640
	140106832146640 [label=AccumulateGrad]
	140106832150480 -> 140106804658768
	140107031830400 [label="flow.flows.4.enc.in_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031830400 -> 140106832150480
	140106832150480 [label=AccumulateGrad]
	140106804658720 -> 140106804658672
	140107031830800 [label="flow.flows.4.enc.in_layers.1.bias
 (384)" fillcolor=lightblue]
	140107031830800 -> 140106804658720
	140106804658720 [label=AccumulateGrad]
	140106804658624 -> 140106804658576
	140106804658624 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804659056 -> 140106804658624
	140106804659056 [label="SliceBackward0
----------------------------
dim           :            1
end           :          768
self_sym_sizes: (7, 1536, 1)
start         :          384
step          :            1"]
	140106804659248 -> 140106804659056
	140106804659248 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832150192 -> 140106804659248
	140106832149664 -> 140106832149760
	140106832149664 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804658528 -> 140106832149664
	140106804658528 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804658480 -> 140106804658528
	140106832148944 -> 140106832149088
	140106832148944 -> 140107031830880 [dir=none]
	140107031830880 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106832148944 -> 140106832790048 [dir=none]
	140106832790048 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106832148944 -> 140107031832000 [dir=none]
	140107031832000 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106832148944 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804659152 -> 140106832148944
	140107031832000 [label="flow.flows.4.enc.res_skip_layers.1.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031832000 -> 140106804659152
	140106804659152 [label=AccumulateGrad]
	140106804658432 -> 140106832148944
	140107031830880 [label="flow.flows.4.enc.res_skip_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031830880 -> 140106804658432
	140106804658432 [label=AccumulateGrad]
	140106832148416 -> 140106832149088
	140107031831520 [label="flow.flows.4.enc.res_skip_layers.1.bias
 (384)" fillcolor=lightblue]
	140107031831520 -> 140106832148416
	140106832148416 [label=AccumulateGrad]
	140106832148128 -> 140106832147552
	140106832148128 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106832148704 -> 140106832148128
	140106832148704 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106832150144 -> 140106832148704
	140106832150144 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832148224 -> 140106832150144
	140106832148224 -> 140107046567376 [dir=none]
	140107046567376 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832148224 -> 140107046560816 [dir=none]
	140107046560816 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106832148224 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804658960 -> 140106832148224
	140106804658960 [label=CppFunction]
	140106804658912 -> 140106804658960
	140106804658912 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804659536 -> 140106804658912
	140106804659536 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804659632 -> 140106804659536
	140106804659632 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804659728 -> 140106804659632
	140106804659728 [label="AddBackward0
------------
alpha: 1"]
	140106804659824 -> 140106804659728
	140106804659824 -> 140107046565296 [dir=none]
	140107046565296 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804659824 -> 140107046563296 [dir=none]
	140107046563296 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804659824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804659968 -> 140106804659824
	140106804659968 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804659968 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804660160 -> 140106804659968
	140106804660160 [label="AddBackward0
------------
alpha: 1"]
	140106804658816 -> 140106804660160
	140106804660256 -> 140106804660160
	140106804660256 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804660352 -> 140106804660256
	140106804660352 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106804660448 -> 140106804660352
	140106804660448 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832149088 -> 140106804660448
	140106804659920 -> 140106804659824
	140106804659920 -> 140107031831600 [dir=none]
	140107031831600 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804659920 -> 140106832791808 [dir=none]
	140106832791808 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804659920 -> 140107031832480 [dir=none]
	140107031832480 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804659920 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804660304 -> 140106804659920
	140107031832480 [label="flow.flows.4.enc.in_layers.2.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031832480 -> 140106804660304
	140106804660304 [label=AccumulateGrad]
	140106804660208 -> 140106804659920
	140107031831600 [label="flow.flows.4.enc.in_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031831600 -> 140106804660208
	140106804660208 [label=AccumulateGrad]
	140106804659872 -> 140106804659824
	140107031832080 [label="flow.flows.4.enc.in_layers.2.bias
 (384)" fillcolor=lightblue]
	140107031832080 -> 140106804659872
	140106804659872 [label=AccumulateGrad]
	140106804659776 -> 140106804659728
	140106804659776 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804660400 -> 140106804659776
	140106804660400 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1152
self_sym_sizes: (7, 1536, 1)
start         :          768
step          :            1"]
	140106804660496 -> 140106804660400
	140106804660496 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832150192 -> 140106804660496
	140106804658864 -> 140106804658960
	140106804658864 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804659680 -> 140106804658864
	140106804659680 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804659632 -> 140106804659680
	140106804658288 -> 140106832148224
	140106804658288 -> 140107031832240 [dir=none]
	140107031832240 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804658288 -> 140106832791168 [dir=none]
	140106832791168 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804658288 -> 140107031833520 [dir=none]
	140107031833520 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106804658288 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804660544 -> 140106804658288
	140107031833520 [label="flow.flows.4.enc.res_skip_layers.2.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031833520 -> 140106804660544
	140106804660544 [label=AccumulateGrad]
	140106804659584 -> 140106804658288
	140107031832240 [label="flow.flows.4.enc.res_skip_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031832240 -> 140106804659584
	140106804659584 [label=AccumulateGrad]
	140106804658336 -> 140106832148224
	140107031832560 [label="flow.flows.4.enc.res_skip_layers.2.bias
 (384)" fillcolor=lightblue]
	140107031832560 -> 140106804658336
	140106804658336 [label=AccumulateGrad]
	140106832148080 -> 140106832147600
	140106832148080 -> 140107046564176 [dir=none]
	140107046564176 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832148080 -> 140107046557296 [dir=none]
	140107046557296 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106832148080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832149136 -> 140106832148080
	140106832149136 [label=CppFunction]
	140106804659440 -> 140106832149136
	140106804659440 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804660592 -> 140106804659440
	140106804660592 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804660016 -> 140106804660592
	140106804660016 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804660688 -> 140106804660016
	140106804660688 [label="AddBackward0
------------
alpha: 1"]
	140106804660784 -> 140106804660688
	140106804660784 -> 140107046562576 [dir=none]
	140107046562576 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804660784 -> 140107046561456 [dir=none]
	140107046561456 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804660784 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804660928 -> 140106804660784
	140106804660928 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804660928 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804661168 -> 140106804660928
	140106804661168 [label="AddBackward0
------------
alpha: 1"]
	140106804659968 -> 140106804661168
	140106804661264 -> 140106804661168
	140106804661264 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804661360 -> 140106804661264
	140106804661360 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106804661456 -> 140106804661360
	140106804661456 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106832148224 -> 140106804661456
	140106804660880 -> 140106804660784
	140106804660880 -> 140107031832640 [dir=none]
	140107031832640 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804660880 -> 140106832791568 [dir=none]
	140106832791568 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804660880 -> 140107031834160 [dir=none]
	140107031834160 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804660880 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804661312 -> 140106804660880
	140107031834160 [label="flow.flows.4.enc.in_layers.3.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031834160 -> 140106804661312
	140106804661312 [label=AccumulateGrad]
	140106804661216 -> 140106804660880
	140107031832640 [label="flow.flows.4.enc.in_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031832640 -> 140106804661216
	140106804661216 [label=AccumulateGrad]
	140106804660832 -> 140106804660784
	140107031833760 [label="flow.flows.4.enc.in_layers.3.bias
 (384)" fillcolor=lightblue]
	140107031833760 -> 140106804660832
	140106804660832 [label=AccumulateGrad]
	140106804660736 -> 140106804660688
	140106804660736 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804661408 -> 140106804660736
	140106804661408 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1536
self_sym_sizes: (7, 1536, 1)
start         :         1152
step          :            1"]
	140106804661504 -> 140106804661408
	140106804661504 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106832150192 -> 140106804661504
	140106804659488 -> 140106832149136
	140106804659488 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804660640 -> 140106804659488
	140106804660640 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804660016 -> 140106804660640
	140106832148512 -> 140106832148080
	140106832148512 -> 140107031833920 [dir=none]
	140107031833920 [label="g
 (192, 1, 1)" fillcolor=orange]
	140106832148512 -> 140106832791648 [dir=none]
	140106832791648 [label="result1
 (192, 1, 1)" fillcolor=orange]
	140106832148512 -> 140107031834960 [dir=none]
	140107031834960 [label="v
 (192, 192, 1)" fillcolor=orange]
	140106832148512 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804661552 -> 140106832148512
	140107031834960 [label="flow.flows.4.enc.res_skip_layers.3.weight_v
 (192, 192, 1)" fillcolor=lightblue]
	140107031834960 -> 140106804661552
	140106804661552 [label=AccumulateGrad]
	140106804660064 -> 140106832148512
	140107031833920 [label="flow.flows.4.enc.res_skip_layers.3.weight_g
 (192, 1, 1)" fillcolor=lightblue]
	140107031833920 -> 140106804660064
	140106804660064 [label=AccumulateGrad]
	140106832147504 -> 140106832148080
	140107031834240 [label="flow.flows.4.enc.res_skip_layers.3.bias
 (192)" fillcolor=lightblue]
	140107031834240 -> 140106832147504
	140106832147504 [label=AccumulateGrad]
	140106832139776 -> 140106832145248
	140107031835280 [label="flow.flows.4.post.weight
 (96, 192, 1)" fillcolor=lightblue]
	140107031835280 -> 140106832139776
	140106832139776 [label=AccumulateGrad]
	140106832135312 -> 140106832145248
	140107031835040 [label="flow.flows.4.post.bias
 (96)" fillcolor=lightblue]
	140107031835040 -> 140106832135312
	140106832135312 [label=AccumulateGrad]
	140106832134256 -> 140106868426784
	140106832134256 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832134256 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832145776 -> 140106832134256
	140106832145776 -> 140107046559776 [dir=none]
	140107046559776 [label="other
 (7, 96, 286)" fillcolor=orange]
	140106832145776 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106868434272 -> 140106832145776
	140106857853568 -> 140106857853664
	140106857853568 [label="AddBackward0
------------
alpha: 1"]
	140106868438064 -> 140106857853568
	140106868438064 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106868438064 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832145824 -> 140106868438064
	140106832145824 -> 140107046557136 [dir=none]
	140107046557136 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106832145824 -> 140107019749632 [dir=none]
	140107019749632 [label="weight
 (96, 192, 1)" fillcolor=orange]
	140106832145824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:          (96,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106832140592 -> 140106832145824
	140106832140592 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106832140592 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804661120 -> 140106832140592
	140106804661120 [label="AddBackward0
------------
alpha: 1"]
	140106804661024 -> 140106804661120
	140106804661024 [label="AddBackward0
------------
alpha: 1"]
	140106804661696 -> 140106804661024
	140106804661696 [label="AddBackward0
------------
alpha: 1"]
	140106804661840 -> 140106804661696
	140106804661840 [label="AddBackward0
------------
alpha: 1"]
	140106804661984 -> 140106804661840
	140106804661984 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804662080 -> 140106804661984
	140106804662080 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106804662176 -> 140106804662080
	140106804662176 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106804662272 -> 140106804662176
	140106804662272 -> 140107046552656 [dir=none]
	140107046552656 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804662272 -> 140107046552816 [dir=none]
	140107046552816 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106804662272 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804662368 -> 140106804662272
	140106804662368 [label=CppFunction]
	140106804662560 -> 140106804662368
	140106804662560 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804662704 -> 140106804662560
	140106804662704 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804662800 -> 140106804662704
	140106804662800 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804662896 -> 140106804662800
	140106804662896 [label="AddBackward0
------------
alpha: 1"]
	140106804662992 -> 140106804662896
	140106804662992 -> 140107046555616 [dir=none]
	140107046555616 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804662992 -> 140107046553456 [dir=none]
	140107046553456 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804662992 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804663136 -> 140106804662992
	140106804663136 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804663136 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804663328 -> 140106804663136
	140106804663328 -> 140107053615280 [dir=none]
	140107053615280 [label="input
 (7, 96, 286)" fillcolor=orange]
	140106804663328 -> 140107031835440 [dir=none]
	140107031835440 [label="weight
 (192, 96, 1)" fillcolor=orange]
	140106804663328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106857853952 -> 140106804663328
	140106804663424 -> 140106804663328
	140107031835440 [label="flow.flows.6.pre.weight
 (192, 96, 1)" fillcolor=lightblue]
	140107031835440 -> 140106804663424
	140106804663424 [label=AccumulateGrad]
	140106804663376 -> 140106804663328
	140107031835600 [label="flow.flows.6.pre.bias
 (192)" fillcolor=lightblue]
	140107031835600 -> 140106804663376
	140106804663376 [label=AccumulateGrad]
	140106804663088 -> 140106804662992
	140106804663088 -> 140107031835760 [dir=none]
	140107031835760 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804663088 -> 140106832792048 [dir=none]
	140106832792048 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804663088 -> 140107031836480 [dir=none]
	140107031836480 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804663088 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804663472 -> 140106804663088
	140107031836480 [label="flow.flows.6.enc.in_layers.0.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031836480 -> 140106804663472
	140106804663472 [label=AccumulateGrad]
	140106804663232 -> 140106804663088
	140107031835760 [label="flow.flows.6.enc.in_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031835760 -> 140106804663232
	140106804663232 [label=AccumulateGrad]
	140106804663040 -> 140106804662992
	140107031836160 [label="flow.flows.6.enc.in_layers.0.bias
 (384)" fillcolor=lightblue]
	140107031836160 -> 140106804663040
	140106804663040 [label=AccumulateGrad]
	140106804662944 -> 140106804662896
	140106804662944 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804663520 -> 140106804662944
	140106804663520 [label="SliceBackward0
----------------------------
dim           :            1
end           :          384
self_sym_sizes: (7, 1536, 1)
start         :            0
step          :            1"]
	140106804661072 -> 140106804663520
	140106804661072 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106804663712 -> 140106804661072
	140106804663712 -> 140107092170592 [dir=none]
	140107092170592 [label="input
 (7, 512, 1)" fillcolor=orange]
	140106804663712 -> 140107046555056 [dir=none]
	140107046555056 [label="weight
 (1536, 512, 1)" fillcolor=orange]
	140106804663712 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:        (1536,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140107604685488 -> 140106804663712
	140106804663808 -> 140106804663712
	140106804663808 -> 140107031834560 [dir=none]
	140107031834560 [label="g
 (1536, 1, 1)" fillcolor=orange]
	140106804663808 -> 140106832789728 [dir=none]
	140106832789728 [label="result1
 (1536, 1, 1)" fillcolor=orange]
	140106804663808 -> 140107031836000 [dir=none]
	140107031836000 [label="v
 (1536, 512, 1)" fillcolor=orange]
	140106804663808 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804663952 -> 140106804663808
	140107031836000 [label="flow.flows.6.enc.cond_layer.weight_v
 (1536, 512, 1)" fillcolor=lightblue]
	140107031836000 -> 140106804663952
	140106804663952 [label=AccumulateGrad]
	140106804663904 -> 140106804663808
	140107031834560 [label="flow.flows.6.enc.cond_layer.weight_g
 (1536, 1, 1)" fillcolor=lightblue]
	140107031834560 -> 140106804663904
	140106804663904 [label=AccumulateGrad]
	140106804663760 -> 140106804663712
	140107031835840 [label="flow.flows.6.enc.cond_layer.bias
 (1536)" fillcolor=lightblue]
	140107031835840 -> 140106804663760
	140106804663760 [label=AccumulateGrad]
	140106804662512 -> 140106804662368
	140106804662512 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804662848 -> 140106804662512
	140106804662848 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804662800 -> 140106804662848
	140106804662320 -> 140106804662272
	140106804662320 -> 140107031836240 [dir=none]
	140107031836240 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804662320 -> 140106832791328 [dir=none]
	140106832791328 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804662320 -> 140107031836960 [dir=none]
	140107031836960 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106804662320 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804663568 -> 140106804662320
	140107031836960 [label="flow.flows.6.enc.res_skip_layers.0.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031836960 -> 140106804663568
	140106804663568 [label=AccumulateGrad]
	140106804662752 -> 140106804662320
	140107031836240 [label="flow.flows.6.enc.res_skip_layers.0.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031836240 -> 140106804662752
	140106804662752 [label=AccumulateGrad]
	140106804661888 -> 140106804662272
	140107031836560 [label="flow.flows.6.enc.res_skip_layers.0.bias
 (384)" fillcolor=lightblue]
	140107031836560 -> 140106804661888
	140106804661888 [label=AccumulateGrad]
	140106804661792 -> 140106804661696
	140106804661792 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804662128 -> 140106804661792
	140106804662128 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106804662416 -> 140106804662128
	140106804662416 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106804662608 -> 140106804662416
	140106804662608 -> 140107046552336 [dir=none]
	140107046552336 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804662608 -> 140107046554896 [dir=none]
	140107046554896 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106804662608 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804663280 -> 140106804662608
	140106804663280 [label=CppFunction]
	140106804664000 -> 140106804663280
	140106804664000 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804664144 -> 140106804664000
	140106804664144 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804664240 -> 140106804664144
	140106804664240 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804664336 -> 140106804664240
	140106804664336 [label="AddBackward0
------------
alpha: 1"]
	140106804664432 -> 140106804664336
	140106804664432 -> 140107046554176 [dir=none]
	140107046554176 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804664432 -> 140107053628800 [dir=none]
	140107053628800 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804664432 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804664576 -> 140106804664432
	140106804664576 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804664576 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804664768 -> 140106804664576
	140106804664768 [label="AddBackward0
------------
alpha: 1"]
	140106804663136 -> 140106804664768
	140106804664864 -> 140106804664768
	140106804664864 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804664960 -> 140106804664864
	140106804664960 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106804665056 -> 140106804664960
	140106804665056 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106804662272 -> 140106804665056
	140106804664528 -> 140106804664432
	140106804664528 -> 140107031836640 [dir=none]
	140107031836640 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804664528 -> 140106832792208 [dir=none]
	140106832792208 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804664528 -> 140107031837440 [dir=none]
	140107031837440 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804664528 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804664912 -> 140106804664528
	140107031837440 [label="flow.flows.6.enc.in_layers.1.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031837440 -> 140106804664912
	140106804664912 [label=AccumulateGrad]
	140106804664816 -> 140106804664528
	140107031836640 [label="flow.flows.6.enc.in_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031836640 -> 140106804664816
	140106804664816 [label=AccumulateGrad]
	140106804664480 -> 140106804664432
	140107031837040 [label="flow.flows.6.enc.in_layers.1.bias
 (384)" fillcolor=lightblue]
	140107031837040 -> 140106804664480
	140106804664480 [label=AccumulateGrad]
	140106804664384 -> 140106804664336
	140106804664384 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804665008 -> 140106804664384
	140106804665008 [label="SliceBackward0
----------------------------
dim           :            1
end           :          768
self_sym_sizes: (7, 1536, 1)
start         :          384
step          :            1"]
	140106804665104 -> 140106804665008
	140106804665104 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106804663712 -> 140106804665104
	140106804664048 -> 140106804663280
	140106804664048 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804664288 -> 140106804664048
	140106804664288 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804664240 -> 140106804664288
	140106804662464 -> 140106804662608
	140106804662464 -> 140107031837200 [dir=none]
	140107031837200 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804662464 -> 140106832792528 [dir=none]
	140106832792528 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804662464 -> 140107031837840 [dir=none]
	140107031837840 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106804662464 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804665152 -> 140106804662464
	140107031837840 [label="flow.flows.6.enc.res_skip_layers.1.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107031837840 -> 140106804665152
	140106804665152 [label=AccumulateGrad]
	140106804664192 -> 140106804662464
	140107031837200 [label="flow.flows.6.enc.res_skip_layers.1.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031837200 -> 140106804664192
	140106804664192 [label=AccumulateGrad]
	140106804661936 -> 140106804662608
	140107031837520 [label="flow.flows.6.enc.res_skip_layers.1.bias
 (384)" fillcolor=lightblue]
	140107031837520 -> 140106804661936
	140106804661936 [label=AccumulateGrad]
	140106804661648 -> 140106804661024
	140106804661648 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804662224 -> 140106804661648
	140106804662224 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                 192
step          :                   1"]
	140106804663664 -> 140106804662224
	140106804663664 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106804663856 -> 140106804663664
	140106804663856 -> 140107046552176 [dir=none]
	140107046552176 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804663856 -> 140107046556416 [dir=none]
	140107046556416 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106804663856 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804664720 -> 140106804663856
	140106804664720 [label=CppFunction]
	140106804665344 -> 140106804664720
	140106804665344 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804665488 -> 140106804665344
	140106804665488 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804665584 -> 140106804665488
	140106804665584 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804665680 -> 140106804665584
	140106804665680 [label="AddBackward0
------------
alpha: 1"]
	140106804665776 -> 140106804665680
	140106804665776 -> 140107046552896 [dir=none]
	140107046552896 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804665776 -> 140107046554496 [dir=none]
	140107046554496 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804665776 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804665920 -> 140106804665776
	140106804665920 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804665920 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804666112 -> 140106804665920
	140106804666112 [label="AddBackward0
------------
alpha: 1"]
	140106804664576 -> 140106804666112
	140106804666208 -> 140106804666112
	140106804666208 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804666304 -> 140106804666208
	140106804666304 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106804666400 -> 140106804666304
	140106804666400 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106804662608 -> 140106804666400
	140106804665872 -> 140106804665776
	140106804665872 -> 140107031837600 [dir=none]
	140107031837600 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804665872 -> 140106832792688 [dir=none]
	140106832792688 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804665872 -> 140107031838240 [dir=none]
	140107031838240 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804665872 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804666256 -> 140106804665872
	140107031838240 [label="flow.flows.6.enc.in_layers.2.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107031838240 -> 140106804666256
	140106804666256 [label=AccumulateGrad]
	140106804666160 -> 140106804665872
	140107031837600 [label="flow.flows.6.enc.in_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031837600 -> 140106804666160
	140106804666160 [label=AccumulateGrad]
	140106804665824 -> 140106804665776
	140107031837920 [label="flow.flows.6.enc.in_layers.2.bias
 (384)" fillcolor=lightblue]
	140107031837920 -> 140106804665824
	140106804665824 [label=AccumulateGrad]
	140106804665728 -> 140106804665680
	140106804665728 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804666352 -> 140106804665728
	140106804666352 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1152
self_sym_sizes: (7, 1536, 1)
start         :          768
step          :            1"]
	140106804666448 -> 140106804666352
	140106804666448 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106804663712 -> 140106804666448
	140106804665296 -> 140106804664720
	140106804665296 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804665632 -> 140106804665296
	140106804665632 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804665584 -> 140106804665632
	140106804663184 -> 140106804663856
	140106804663184 -> 140107031838080 [dir=none]
	140107031838080 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804663184 -> 140106832792768 [dir=none]
	140106832792768 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804663184 -> 140107019748192 [dir=none]
	140107019748192 [label="v
 (384, 192, 1)" fillcolor=orange]
	140106804663184 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804666496 -> 140106804663184
	140107019748192 [label="flow.flows.6.enc.res_skip_layers.2.weight_v
 (384, 192, 1)" fillcolor=lightblue]
	140107019748192 -> 140106804666496
	140106804666496 [label=AccumulateGrad]
	140106804665536 -> 140106804663184
	140107031838080 [label="flow.flows.6.enc.res_skip_layers.2.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031838080 -> 140106804665536
	140106804665536 [label=AccumulateGrad]
	140106804661744 -> 140106804663856
	140107019747552 [label="flow.flows.6.enc.res_skip_layers.2.bias
 (384)" fillcolor=lightblue]
	140107019747552 -> 140106804661744
	140106804661744 [label=AccumulateGrad]
	140106804661600 -> 140106804661120
	140106804661600 -> 140107046557456 [dir=none]
	140107046557456 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804661600 -> 140107046557696 [dir=none]
	140107046557696 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106804661600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804662656 -> 140106804661600
	140106804662656 [label=CppFunction]
	140106804666064 -> 140106804662656
	140106804666064 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804666592 -> 140106804666064
	140106804666592 [label="SliceBackward0
-----------------------------
dim           :             1
end           :          None
self_sym_sizes: (7, 384, 286)
start         :           192
step          :             1"]
	140106804666688 -> 140106804666592
	140106804666688 [label="SliceBackward0
-----------------------------
dim           :             0
end           :          None
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804666784 -> 140106804666688
	140106804666784 [label="AddBackward0
------------
alpha: 1"]
	140106804666880 -> 140106804666784
	140106804666880 -> 140107046555696 [dir=none]
	140107046555696 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804666880 -> 140107046556336 [dir=none]
	140107046556336 [label="weight
 (384, 192, 5)" fillcolor=orange]
	140106804666880 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (2,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804667024 -> 140106804666880
	140106804667024 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804667024 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804667216 -> 140106804667024
	140106804667216 [label="AddBackward0
------------
alpha: 1"]
	140106804665920 -> 140106804667216
	140106804667312 -> 140106804667216
	140106804667312 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:       (7, 192, 286)
start         :                   0
step          :                   1"]
	140106804667408 -> 140106804667312
	140106804667408 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :             0
step          :             1"]
	140106804667504 -> 140106804667408
	140106804667504 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:       (7, 384, 286)
start         :                   0
step          :                   1"]
	140106804663856 -> 140106804667504
	140106804666976 -> 140106804666880
	140106804666976 -> 140107031838320 [dir=none]
	140107031838320 [label="g
 (384, 1, 1)" fillcolor=orange]
	140106804666976 -> 140106832792368 [dir=none]
	140106832792368 [label="result1
 (384, 1, 1)" fillcolor=orange]
	140106804666976 -> 140107019748912 [dir=none]
	140107019748912 [label="v
 (384, 192, 5)" fillcolor=orange]
	140106804666976 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804667360 -> 140106804666976
	140107019748912 [label="flow.flows.6.enc.in_layers.3.weight_v
 (384, 192, 5)" fillcolor=lightblue]
	140107019748912 -> 140106804667360
	140106804667360 [label=AccumulateGrad]
	140106804667264 -> 140106804666976
	140107031838320 [label="flow.flows.6.enc.in_layers.3.weight_g
 (384, 1, 1)" fillcolor=lightblue]
	140107031838320 -> 140106804667264
	140106804667264 [label=AccumulateGrad]
	140106804666928 -> 140106804666880
	140107019748432 [label="flow.flows.6.enc.in_layers.3.bias
 (384)" fillcolor=lightblue]
	140107019748432 -> 140106804666928
	140106804666928 [label=AccumulateGrad]
	140106804666832 -> 140106804666784
	140106804666832 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:         (7, 384, 1)
start         :                   0
step          :                   1"]
	140106804667456 -> 140106804666832
	140106804667456 [label="SliceBackward0
----------------------------
dim           :            1
end           :         1536
self_sym_sizes: (7, 1536, 1)
start         :         1152
step          :            1"]
	140106804667552 -> 140106804667456
	140106804667552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (7, 1536, 1)
start         :                   0
step          :                   1"]
	140106804663712 -> 140106804667552
	140106804666016 -> 140106804662656
	140106804666016 [label="SliceBackward0
-----------------------------
dim           :             2
end           :          None
self_sym_sizes: (7, 192, 286)
start         :          None
step          :             1"]
	140106804666736 -> 140106804666016
	140106804666736 [label="SliceBackward0
-----------------------------
dim           :             1
end           :           192
self_sym_sizes: (7, 384, 286)
start         :          None
step          :             1"]
	140106804666688 -> 140106804666736
	140106804662032 -> 140106804661600
	140106804662032 -> 140107019748512 [dir=none]
	140107019748512 [label="g
 (192, 1, 1)" fillcolor=orange]
	140106804662032 -> 140106832793008 [dir=none]
	140106832793008 [label="result1
 (192, 1, 1)" fillcolor=orange]
	140106804662032 -> 140107019749472 [dir=none]
	140107019749472 [label="v
 (192, 192, 1)" fillcolor=orange]
	140106804662032 [label="WeightNormInterfaceBackward0
----------------------------
dim    :              0
g      : [saved tensor]
result1: [saved tensor]
v      : [saved tensor]"]
	140106804667600 -> 140106804662032
	140107019749472 [label="flow.flows.6.enc.res_skip_layers.3.weight_v
 (192, 192, 1)" fillcolor=lightblue]
	140107019749472 -> 140106804667600
	140106804667600 [label=AccumulateGrad]
	140106804666640 -> 140106804662032
	140107019748512 [label="flow.flows.6.enc.res_skip_layers.3.weight_g
 (192, 1, 1)" fillcolor=lightblue]
	140107019748512 -> 140106804666640
	140106804666640 [label=AccumulateGrad]
	140106804660976 -> 140106804661600
	140107019748992 [label="flow.flows.6.enc.res_skip_layers.3.bias
 (192)" fillcolor=lightblue]
	140107019748992 -> 140106804660976
	140106804660976 [label=AccumulateGrad]
	140106832135072 -> 140106832145824
	140107019749632 [label="flow.flows.6.post.weight
 (96, 192, 1)" fillcolor=lightblue]
	140107019749632 -> 140106832135072
	140106832135072 [label=AccumulateGrad]
	140106804658240 -> 140106832145824
	140107019749552 [label="flow.flows.6.post.bias
 (96)" fillcolor=lightblue]
	140107019749552 -> 140106804658240
	140106804658240 [label=AccumulateGrad]
	140106857854816 -> 140106857853568
	140106857854816 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857854816 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832144624 -> 140106857854816
	140106832144624 -> 140107046557776 [dir=none]
	140107046557776 [label="other
 (7, 96, 286)" fillcolor=orange]
	140106832144624 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857853952 -> 140106832144624
	140106857852032 -> 140106857853280
	140106857851936 -> 140106857851648
	140106857851936 -> 140106832792288 [dir=none]
	140106832792288 [label="result
 (7, 192, 286)" fillcolor=orange]
	140106857851936 [label="ExpBackward0
----------------------
result: [saved tensor]"]
	140106857853328 -> 140106857851936
	140106857853328 -> 140106832792448 [dir=none]
	140106832792448 [label="other
 ()" fillcolor=orange]
	140106857853328 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857852032 -> 140106857853328
	140106869109712 -> 140106869103568
	140106869109712 -> 140106778590592 [dir=none]
	140106778590592 [label="other
 ()" fillcolor=orange]
	140106869109712 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869107840 -> 140106869109712
	140106869107840 [label="SumBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106857850784 -> 140106869107840
	140106857850784 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106857850784 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857842096 -> 140106857850784
	140106857842096 [label="AddBackward0
------------
alpha: 1"]
	140106857853616 -> 140106857842096
	140106857853616 [label="SubBackward0
------------
alpha: 1"]
	140106832134928 -> 140106857853616
	140106832134928 [label="SubBackward0
------------
alpha: 1"]
	140106804659392 -> 140106832134928
	140106804659392 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106804665968 -> 140106804659392
	140106804665968 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 192)"]
	140106804665200 -> 140106804665968
	140106804665200 -> 140106832793168 [dir=none]
	140106832793168 [label="self
 (7, 286, 28)" fillcolor=orange]
	140106804665200 [label="BmmBackward0
--------------------
mat2:           None
self: [saved tensor]"]
	140106804667648 -> 140106804665200
	140106804667648 [label="ReshapeAliasBackward0
----------------------------
self_sym_sizes: (7, 28, 192)"]
	140106804667072 -> 140106804667648
	140106804667072 [label="ExpandBackward0
----------------------------
self_sym_sizes: (7, 28, 192)"]
	140106804667744 -> 140106804667072
	140106804667744 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106804666544 -> 140106804667744
	140106804666544 [label="SplitBackward0
----------------------------
dim           :            1
self_sym_sizes: (7, 384, 28)
split_size    :          192"]
	140106804667936 -> 140106804666544
	140106804667936 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106804667936 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804668032 -> 140106804667936
	140106804668032 -> 140107077230944 [dir=none]
	140107077230944 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106804668032 -> 140107071062720 [dir=none]
	140107071062720 [label="weight
 (384, 192, 1)" fillcolor=orange]
	140106804668032 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (384,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106854696032 -> 140106804668032
	140106804668128 -> 140106804668032
	140107071062720 [label="t_enc.proj.weight
 (384, 192, 1)" fillcolor=lightblue]
	140107071062720 -> 140106804668128
	140106804668128 [label=AccumulateGrad]
	140106804668080 -> 140106804668032
	140107071063120 [label="t_enc.proj.bias
 (384)" fillcolor=lightblue]
	140107071063120 -> 140106804668080
	140106804668080 [label=AccumulateGrad]
	140107604686112 -> 140106832134928
	140106857852368 -> 140106857842096
	140106857852368 -> 140106778582752 [dir=none]
	140106778582752 [label="other
 (7, 192, 286)" fillcolor=orange]
	140106857852368 -> 140106778588272 [dir=none]
	140106778588272 [label="self
 (7, 192, 286)" fillcolor=orange]
	140106857852368 [label="MulBackward0
---------------------
other: [saved tensor]
self : [saved tensor]"]
	140106857852752 -> 140106857852368
	140106857852752 -> 140106832792128 [dir=none]
	140106832792128 [label="other
 ()" fillcolor=orange]
	140106857852752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804667120 -> 140106857852752
	140106804667120 -> 140106778589872 [dir=none]
	140106778589872 [label="self
 (7, 192, 286)" fillcolor=orange]
	140106804667120 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106804667792 -> 140106804667120
	140106804667792 [label="SubBackward0
------------
alpha: 1"]
	140106857853520 -> 140106804667792
	140106804667984 -> 140106804667792
	140106804667984 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106804668176 -> 140106804667984
	140106804668176 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (7, 286, 192)"]
	140106804668272 -> 140106804668176
	140106804668272 -> 140106832793248 [dir=none]
	140106832793248 [label="self
 (7, 286, 28)" fillcolor=orange]
	140106804668272 [label="BmmBackward0
--------------------
mat2:           None
self: [saved tensor]"]
	140106804668368 -> 140106804668272
	140106804668368 [label="ReshapeAliasBackward0
----------------------------
self_sym_sizes: (7, 28, 192)"]
	140106804668464 -> 140106804668368
	140106804668464 [label="ExpandBackward0
----------------------------
self_sym_sizes: (7, 28, 192)"]
	140106804668560 -> 140106804668464
	140106804668560 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106804666544 -> 140106804668560
	140106804660112 -> 140106857852368
	140106804660112 -> 140106832792848 [dir=none]
	140106832792848 [label="result
 (7, 192, 286)" fillcolor=orange]
	140106804660112 [label="ExpBackward0
----------------------
result: [saved tensor]"]
	140106804667888 -> 140106804660112
	140106804667888 -> 140106832793488 [dir=none]
	140106832793488 [label="other
 ()" fillcolor=orange]
	140106804667888 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804659392 -> 140106804667888
	140106869103760 -> 140106869108800
	140106869103760 [label="SumBackward0
--------------------
self_sym_sizes: (7,)"]
	140106869107888 -> 140106869103760
	140106869107888 -> 140107031829520 [dir=none]
	140107031829520 [label="other
 ()" fillcolor=orange]
	140106869107888 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857851360 -> 140106869107888
	140106857851360 [label="SumBackward1
--------------------------
dim           :     (1, 2)
keepdim       :      False
self_sym_sizes: (7, 1, 28)"]
	140106857851456 -> 140106857851360
	140106857851456 -> 140107031829280 [dir=none]
	140107031829280 [label="self
 (7, 1, 28)" fillcolor=orange]
	140106857851456 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106804659344 -> 140106857851456
	140106804659344 [label="SubBackward0
------------
alpha: 1"]
	140106804668320 -> 140106804659344
	140106804668320 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106804668320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804668512 -> 140106804668320
	140106804668512 -> 140107031827280 [dir=none]
	140107031827280 [label="input
 (7, 256, 28)" fillcolor=orange]
	140106804668512 -> 140107003793536 [dir=none]
	140107003793536 [label="weight
 (1, 256, 1)" fillcolor=orange]
	140106804668512 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:           (1,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804668608 -> 140106804668512
	140106804668608 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106804668608 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804668800 -> 140106804668608
	140106804668800 -> 140106832793648 [dir=none]
	140106832793648 [label="result1
 (7, 256, 28)" fillcolor=orange]
	140106804668800 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	140106804668896 -> 140106804668800
	140106804668896 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804669040 -> 140106804668896
	140106804669040 -> 140107003793296 [dir=none]
	140107003793296 [label="bias
 (256)" fillcolor=orange]
	140106804669040 -> 140107031826080 [dir=none]
	140107031826080 [label="input
 (7, 28, 256)" fillcolor=orange]
	140106804669040 -> 140106832793568 [dir=none]
	140106832793568 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106804669040 -> 140106832793088 [dir=none]
	140106832793088 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106804669040 -> 140107003793136 [dir=none]
	140107003793136 [label="weight
 (256)" fillcolor=orange]
	140106804669040 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804669088 -> 140106804669040
	140106804669088 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804669376 -> 140106804669088
	140106804669376 -> 140106832793808 [dir=none]
	140106832793808 [label="result
 (7, 256, 28)" fillcolor=orange]
	140106804669376 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106804669424 -> 140106804669376
	140106804669424 -> 140107040378208 [dir=none]
	140107040378208 [label="input
 (7, 256, 28)" fillcolor=orange]
	140106804669424 -> 140107003792896 [dir=none]
	140107003792896 [label="weight
 (256, 256, 3)" fillcolor=orange]
	140106804669424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804669568 -> 140106804669424
	140106804669568 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106804669568 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804669760 -> 140106804669568
	140106804669760 -> 140106832793408 [dir=none]
	140106832793408 [label="result1
 (7, 256, 28)" fillcolor=orange]
	140106804669760 [label="NativeDropoutBackward0
-----------------------
p      :            0.5
result1: [saved tensor]"]
	140106804669856 -> 140106804669760
	140106804669856 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804670000 -> 140106804669856
	140106804670000 -> 140107003792816 [dir=none]
	140107003792816 [label="bias
 (256)" fillcolor=orange]
	140106804670000 -> 140107031822720 [dir=none]
	140107031822720 [label="input
 (7, 28, 256)" fillcolor=orange]
	140106804670000 -> 140106832791968 [dir=none]
	140106832791968 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106804670000 -> 140106832793888 [dir=none]
	140106832793888 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106804670000 -> 140107003792656 [dir=none]
	140107003792656 [label="weight
 (256)" fillcolor=orange]
	140106804670000 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (256,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804670048 -> 140106804670000
	140106804670048 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804670336 -> 140106804670048
	140106804670336 -> 140106832794128 [dir=none]
	140106832794128 [label="result
 (7, 256, 28)" fillcolor=orange]
	140106804670336 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106804670384 -> 140106804670336
	140106804670384 -> 140107040377168 [dir=none]
	140107040377168 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106804670384 -> 140107003792256 [dir=none]
	140107003792256 [label="weight
 (256, 192, 3)" fillcolor=orange]
	140106804670384 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (256,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (1,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804670528 -> 140106804670384
	140106804670528 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106804670528 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804670720 -> 140106804670528
	140106804670720 [label="AddBackward0
------------
alpha: 1"]
	140106804670816 -> 140106804670720
	140106804670816 [label="AddBackward0
------------
alpha: 1"]
	140106804670960 -> 140106804670816
	140106804670960 -> 140107031832320 [dir=none]
	140107031832320 [label="input
 (7, 512, 1)" fillcolor=orange]
	140106804670960 -> 140107003793856 [dir=none]
	140107003793856 [label="weight
 (192, 512, 1)" fillcolor=orange]
	140106804670960 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804671056 -> 140106804670960
	140107003793856 [label="dp.cond.weight
 (192, 512, 1)" fillcolor=lightblue]
	140107003793856 -> 140106804671056
	140106804671056 [label=AccumulateGrad]
	140106804671008 -> 140106804670960
	140107003794416 [label="dp.cond.bias
 (192)" fillcolor=lightblue]
	140107003794416 -> 140106804671008
	140106804671008 [label=AccumulateGrad]
	140106804670768 -> 140106804670720
	140106804670768 [label="TransposeBackward0
------------------
dim0: 1
dim1: 2"]
	140106804671104 -> 140106804670768
	140106804671104 -> 140107053618000 [dir=none]
	140107053618000 [label="indices
 (7, 28)" fillcolor=orange]
	140106804671104 [label="EmbeddingBackward0
------------------------------------------
indices             :       [saved tensor]
padding_idx         : 18446744073709551615
scale_grad_by_freq  :                False
sparse              :                False
weight_sym_argsize_0:                 4096"]
	140106804671200 -> 140106804671104
	140107011980032 [label="dur_detail_emb.weight
 (4096, 192)" fillcolor=lightblue]
	140107011980032 -> 140106804671200
	140106804671200 [label=AccumulateGrad]
	140106804670480 -> 140106804670384
	140107003792256 [label="dp.conv_1.weight
 (256, 192, 3)" fillcolor=lightblue]
	140107003792256 -> 140106804670480
	140106804670480 [label=AccumulateGrad]
	140106804670192 -> 140106804670384
	140107003792416 [label="dp.conv_1.bias
 (256)" fillcolor=lightblue]
	140107003792416 -> 140106804670192
	140106804670192 [label=AccumulateGrad]
	140106804670096 -> 140106804670000
	140107003792656 [label="dp.norm_1.gamma
 (256)" fillcolor=lightblue]
	140107003792656 -> 140106804670096
	140106804670096 [label=AccumulateGrad]
	140106804669664 -> 140106804670000
	140107003792816 [label="dp.norm_1.beta
 (256)" fillcolor=lightblue]
	140107003792816 -> 140106804669664
	140106804669664 [label=AccumulateGrad]
	140106804669520 -> 140106804669424
	140107003792896 [label="dp.conv_2.weight
 (256, 256, 3)" fillcolor=lightblue]
	140107003792896 -> 140106804669520
	140106804669520 [label=AccumulateGrad]
	140106804669232 -> 140106804669424
	140107003793056 [label="dp.conv_2.bias
 (256)" fillcolor=lightblue]
	140107003793056 -> 140106804669232
	140106804669232 [label=AccumulateGrad]
	140106804669136 -> 140106804669040
	140107003793136 [label="dp.norm_2.gamma
 (256)" fillcolor=lightblue]
	140107003793136 -> 140106804669136
	140106804669136 [label=AccumulateGrad]
	140106804668704 -> 140106804669040
	140107003793296 [label="dp.norm_2.beta
 (256)" fillcolor=lightblue]
	140107003793296 -> 140106804668704
	140106804668704 [label=AccumulateGrad]
	140106804668656 -> 140106804668512
	140107003793536 [label="dp.proj.weight
 (1, 256, 1)" fillcolor=lightblue]
	140107003793536 -> 140106804668656
	140106804668656 [label=AccumulateGrad]
	140106804664096 -> 140106804668512
	140107003793696 [label="dp.proj.bias
 (1)" fillcolor=lightblue]
	140107003793696 -> 140106804664096
	140106804664096 [label=AccumulateGrad]
	140106869110000 -> 140106869107120
	140106869110000 -> 140107011975792 [dir=none]
	140107011975792 [label="other
 ()" fillcolor=orange]
	140106869110000 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106857851744 -> 140106869110000
	140106857851744 [label="SumBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106869102032 -> 140106857851744
	140106869102032 -> 140107099113728 [dir=none]
	140107099113728 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106869102032 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804668416 -> 140106869102032
	140106804668416 -> 140107058258832 [dir=none]
	140107058258832 [label="self
 (7, 192, 286)" fillcolor=orange]
	140106804668416 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106804668752 -> 140106804668416
	140106804668752 [label="SubBackward0
------------
alpha: 1"]
	140106854695360 -> 140106804668752
	140106804668944 -> 140106804668752
	140106804668944 -> 140106854232000 [dir=none]
	140106854232000 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804668944 -> 140107065183904 [dir=none]
	140107065183904 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106804668944 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804669184 -> 140106804668944
	140106804669184 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804669184 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106813177472 -> 140106804669184
	140106813177472 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106809804016 -> 140106813177472
	140106809804016 -> 140107065183744 [dir=none]
	140107065183744 [label="bias
 (192)" fillcolor=orange]
	140106809804016 -> 140106856381184 [dir=none]
	140106856381184 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106809804016 -> 140106814118368 [dir=none]
	140106814118368 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106809804016 -> 140106814114688 [dir=none]
	140106814114688 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106809804016 -> 140107065183504 [dir=none]
	140107065183504 [label="weight
 (192)" fillcolor=orange]
	140106809804016 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106807863344 -> 140106809804016
	140106807863344 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106832144816 -> 140106807863344
	140106832144816 [label="AddBackward0
------------
alpha: 1"]
	140106832139200 -> 140106832144816
	140106832139200 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804669472 -> 140106832139200
	140106804669472 -> 140107065182064 [dir=none]
	140107065182064 [label="bias
 (192)" fillcolor=orange]
	140106804669472 -> 140106853831824 [dir=none]
	140106853831824 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804669472 -> 140106832793728 [dir=none]
	140106832793728 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804669472 -> 140106832792928 [dir=none]
	140106832792928 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804669472 -> 140107065181904 [dir=none]
	140107065181904 [label="weight
 (192)" fillcolor=orange]
	140106804669472 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804669328 -> 140106804669472
	140106804669328 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804670144 -> 140106804669328
	140106804670144 [label="AddBackward0
------------
alpha: 1"]
	140106804670240 -> 140106804670144
	140106804670240 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804670624 -> 140106804670240
	140106804670624 -> 140107071078320 [dir=none]
	140107071078320 [label="bias
 (192)" fillcolor=orange]
	140106804670624 -> 140106866488672 [dir=none]
	140106866488672 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804670624 -> 140106832790928 [dir=none]
	140106832790928 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804670624 -> 140106832794048 [dir=none]
	140106832794048 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804670624 -> 140107071078240 [dir=none]
	140107071078240 [label="weight
 (192)" fillcolor=orange]
	140106804670624 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804670864 -> 140106804670624
	140106804670864 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804671392 -> 140106804670864
	140106804671392 [label="AddBackward0
------------
alpha: 1"]
	140106804671440 -> 140106804671392
	140106804671440 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804671680 -> 140106804671440
	140106804671680 -> 140107071076880 [dir=none]
	140107071076880 [label="bias
 (192)" fillcolor=orange]
	140106804671680 -> 140106861801168 [dir=none]
	140106861801168 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804671680 -> 140106832792608 [dir=none]
	140106832792608 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804671680 -> 140106832794208 [dir=none]
	140106832794208 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804671680 -> 140107071076720 [dir=none]
	140107071076720 [label="weight
 (192)" fillcolor=orange]
	140106804671680 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804671728 -> 140106804671680
	140106804671728 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804672016 -> 140106804671728
	140106804672016 [label="AddBackward0
------------
alpha: 1"]
	140106804672064 -> 140106804672016
	140106804672064 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804672304 -> 140106804672064
	140106804672304 -> 140107071074320 [dir=none]
	140107071074320 [label="bias
 (192)" fillcolor=orange]
	140106804672304 -> 140106868546416 [dir=none]
	140106868546416 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804672304 -> 140106832793968 [dir=none]
	140106832793968 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804672304 -> 140106832794448 [dir=none]
	140106832794448 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804672304 -> 140107071074240 [dir=none]
	140107071074240 [label="weight
 (192)" fillcolor=orange]
	140106804672304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804672352 -> 140106804672304
	140106804672352 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804672640 -> 140106804672352
	140106804672640 [label="AddBackward0
------------
alpha: 1"]
	140106804672688 -> 140106804672640
	140106804672688 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804672928 -> 140106804672688
	140106804672928 -> 140107071073520 [dir=none]
	140107071073520 [label="bias
 (192)" fillcolor=orange]
	140106804672928 -> 140106869326688 [dir=none]
	140106869326688 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804672928 -> 140106832794368 [dir=none]
	140106832794368 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804672928 -> 140106832794608 [dir=none]
	140106832794608 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804672928 -> 140107071073280 [dir=none]
	140107071073280 [label="weight
 (192)" fillcolor=orange]
	140106804672928 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804672976 -> 140106804672928
	140106804672976 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804673264 -> 140106804672976
	140106804673264 [label="AddBackward0
------------
alpha: 1"]
	140106804673312 -> 140106804673264
	140106804673312 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804673552 -> 140106804673312
	140106804673552 -> 140107071071520 [dir=none]
	140107071071520 [label="bias
 (192)" fillcolor=orange]
	140106804673552 -> 140107003800896 [dir=none]
	140107003800896 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804673552 -> 140106832794528 [dir=none]
	140106832794528 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804673552 -> 140106832794768 [dir=none]
	140106832794768 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804673552 -> 140107071071280 [dir=none]
	140107071071280 [label="weight
 (192)" fillcolor=orange]
	140106804673552 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804673600 -> 140106804673552
	140106804673600 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804673888 -> 140106804673600
	140106804673888 [label="AddBackward0
------------
alpha: 1"]
	140106804673936 -> 140106804673888
	140106804673936 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804674176 -> 140106804673936
	140106804674176 -> 140107071070640 [dir=none]
	140107071070640 [label="bias
 (192)" fillcolor=orange]
	140106804674176 -> 140107003794496 [dir=none]
	140107003794496 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106804674176 -> 140106832794688 [dir=none]
	140106832794688 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106804674176 -> 140106832794928 [dir=none]
	140106832794928 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106804674176 -> 140107071070480 [dir=none]
	140107071070480 [label="weight
 (192)" fillcolor=orange]
	140106804674176 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106804674224 -> 140106804674176
	140106804674224 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804674512 -> 140106804674224
	140106804674512 [label="AddBackward0
------------
alpha: 1"]
	140106804674368 -> 140106804674512
	140106804674368 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835493168 -> 140106804674368
	140106835493168 -> 140107071069040 [dir=none]
	140107071069040 [label="bias
 (192)" fillcolor=orange]
	140106835493168 -> 140107003794096 [dir=none]
	140107003794096 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106835493168 -> 140106832794848 [dir=none]
	140106832794848 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106835493168 -> 140106832795088 [dir=none]
	140106832795088 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106835493168 -> 140107071068960 [dir=none]
	140107071068960 [label="weight
 (192)" fillcolor=orange]
	140106835493168 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106835493216 -> 140106835493168
	140106835493216 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835493504 -> 140106835493216
	140106835493504 [label="AddBackward0
------------
alpha: 1"]
	140106835493552 -> 140106835493504
	140106835493552 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835493792 -> 140106835493552
	140106835493792 -> 140107071068480 [dir=none]
	140107071068480 [label="bias
 (192)" fillcolor=orange]
	140106835493792 -> 140107003799136 [dir=none]
	140107003799136 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106835493792 -> 140106832795008 [dir=none]
	140106832795008 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106835493792 -> 140106832795248 [dir=none]
	140106832795248 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106835493792 -> 140107071068320 [dir=none]
	140107071068320 [label="weight
 (192)" fillcolor=orange]
	140106835493792 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106835493840 -> 140106835493792
	140106835493840 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835494128 -> 140106835493840
	140106835494128 [label="AddBackward0
------------
alpha: 1"]
	140106835494176 -> 140106835494128
	140106835494176 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835494416 -> 140106835494176
	140106835494416 -> 140107071066400 [dir=none]
	140107071066400 [label="bias
 (192)" fillcolor=orange]
	140106835494416 -> 140107003790736 [dir=none]
	140107003790736 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106835494416 -> 140106832795168 [dir=none]
	140106832795168 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106835494416 -> 140106832795408 [dir=none]
	140106832795408 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106835494416 -> 140107071066080 [dir=none]
	140107071066080 [label="weight
 (192)" fillcolor=orange]
	140106835494416 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106835494464 -> 140106835494416
	140106835494464 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835494752 -> 140106835494464
	140106835494752 [label="AddBackward0
------------
alpha: 1"]
	140106835494800 -> 140106835494752
	140106835494800 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835495040 -> 140106835494800
	140106835495040 -> 140107071065360 [dir=none]
	140107071065360 [label="bias
 (192)" fillcolor=orange]
	140106835495040 -> 140107011979792 [dir=none]
	140107011979792 [label="input
 (7, 286, 192)" fillcolor=orange]
	140106835495040 -> 140106832795328 [dir=none]
	140106832795328 [label="result1
 (7, 286, 1)" fillcolor=orange]
	140106835495040 -> 140106832795568 [dir=none]
	140106832795568 [label="result2
 (7, 286, 1)" fillcolor=orange]
	140106835495040 -> 140107071065200 [dir=none]
	140107071065200 [label="weight
 (192)" fillcolor=orange]
	140106835495040 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106835495088 -> 140106835495040
	140106835495088 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106835495376 -> 140106835495088
	140106835495376 [label="AddBackward0
------------
alpha: 1"]
	140106835495424 -> 140106835495376
	140106835495424 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835495424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835495616 -> 140106835495424
	140106835495616 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835495616 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835495712 -> 140106835495616
	140106835495712 [label="AddBackward0
------------
alpha: 1"]
	140106854695408 -> 140106835495712
	140106835495808 -> 140106835495712
	140106835495808 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551615"]
	140106835495952 -> 140106835495808
	140106835495952 -> 140107019754752 [dir=none]
	140107019754752 [label="mat1
 (7, 512)" fillcolor=orange]
	140106835495952 -> 140106832795968 [dir=none]
	140106832795968 [label="mat2
 (512, 192)" fillcolor=orange]
	140106835495952 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (7, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (512, 192)
mat2_sym_strides:       (1, 512)"]
	140106835496000 -> 140106835495952
	140107065184464 [label="detail_enc.ge_proj.bias
 (192)" fillcolor=lightblue]
	140107065184464 -> 140106835496000
	140106835496000 [label=AccumulateGrad]
	140106835496048 -> 140106835495952
	140106835496048 [label="SqueezeBackward1
------------------------------------
dim           : 18446744073709551615
self_sym_sizes:          (7, 512, 1)"]
	140107604685488 -> 140106835496048
	140106835495520 -> 140106835495952
	140106835495520 [label=TBackward0]
	140106835496240 -> 140106835495520
	140107065184304 [label="detail_enc.ge_proj.weight
 (192, 512)" fillcolor=lightblue]
	140107065184304 -> 140106835496240
	140106835496240 [label=AccumulateGrad]
	140106835495472 -> 140106835495376
	140106835495472 -> 140106832795728 [dir=none]
	140106832795728 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106835495472 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835495760 -> 140106835495472
	140106835495760 -> 140107003790896 [dir=none]
	140107003790896 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835495760 -> 140107071064480 [dir=none]
	140107071064480 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835495760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835495904 -> 140106835495760
	140106835495904 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835496288 -> 140106835495904
	140106835496288 [label=CloneBackward0]
	140106835496384 -> 140106835496288
	140106835496384 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835496480 -> 140106835496384
	140106835496480 [label="AddBackward0
------------
alpha: 1"]
	140106835496576 -> 140106835496480
	140106835496576 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835496720 -> 140106835496576
	140106835496720 -> 140106832796048 [dir=none]
	140106832796048 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106835496720 -> 140106832795888 [dir=none]
	140106832795888 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106835496720 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835496816 -> 140106835496720
	140106835496816 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835496960 -> 140106835496816
	140106835496960 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835497056 -> 140106835496960
	140106835497056 -> 140106832796128 [dir=none]
	140106832796128 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106835497056 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835497152 -> 140106835497056
	140106835497152 -> 140106832795648 [dir=none]
	140106832795648 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106835497152 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106835497248 -> 140106835497152
	140106835497248 -> 140107011978112 [dir=none]
	140107011978112 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106835497248 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106835497344 -> 140106835497248
	140106835497344 [label="AddBackward0
------------
alpha: 1"]
	140106835497440 -> 140106835497344
	140106835497440 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106835497584 -> 140106835497440
	140106835497584 -> 140106832796528 [dir=none]
	140106832796528 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106835497584 -> 140106832796448 [dir=none]
	140106832796448 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835497584 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835497680 -> 140106835497584
	140106835497680 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835497824 -> 140106835497680
	140106835497824 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835497920 -> 140106835497824
	140106835497920 -> 140106832796368 [dir=none]
	140106832796368 [label="other
 ()" fillcolor=orange]
	140106835497920 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835498016 -> 140106835497920
	140106835498016 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835498112 -> 140106835498016
	140106835498112 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835498208 -> 140106835498112
	140106835498208 -> 140107011970752 [dir=none]
	140107011970752 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835498208 -> 140107071063520 [dir=none]
	140107071063520 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835498208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835495424 -> 140106835498208
	140106835498304 -> 140106835498208
	140107071063520 [label="detail_enc.encoder.attn_layers.0.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071063520 -> 140106835498304
	140106835498304 [label=AccumulateGrad]
	140106835498256 -> 140106835498208
	140107071063600 [label="detail_enc.encoder.attn_layers.0.conv_q.bias
 (192)" fillcolor=lightblue]
	140107071063600 -> 140106835498256
	140106835498256 [label=AccumulateGrad]
	140106835497632 -> 140106835497584
	140106835497632 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835497968 -> 140106835497632
	140106835497968 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835498160 -> 140106835497968
	140106835498160 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106835498400 -> 140106835498160
	140106835498400 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835498448 -> 140106835498400
	140106835498448 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835498592 -> 140106835498448
	140106835498592 -> 140107011970752 [dir=none]
	140107011970752 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835498592 -> 140107071063840 [dir=none]
	140107071063840 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835498592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835495424 -> 140106835498592
	140106835498688 -> 140106835498592
	140107071063840 [label="detail_enc.encoder.attn_layers.0.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071063840 -> 140106835498688
	140106835498688 [label=AccumulateGrad]
	140106835498640 -> 140106835498592
	140107071064080 [label="detail_enc.encoder.attn_layers.0.conv_k.bias
 (192)" fillcolor=lightblue]
	140107071064080 -> 140106835498640
	140106835498640 [label=AccumulateGrad]
	140106835497392 -> 140106835497344
	140106835497392 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106835497872 -> 140106835497392
	140106835497872 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106835498352 -> 140106835497872
	140106835498352 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106835498544 -> 140106835498352
	140106835498544 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106835497776 -> 140106835498544
	140106835497776 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106835498784 -> 140106835497776
	140106835498784 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106835498880 -> 140106835498784
	140106835498880 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106835498976 -> 140106835498880
	140106835498976 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106857853424 -> 140106835498976
	140106857853424 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106835497536 -> 140106857853424
	140106835497536 -> 140106722774048 [dir=none]
	140106722774048 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106835497536 -> 140106814113088 [dir=none]
	140106814113088 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835497536 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835499168 -> 140106835497536
	140106835499168 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835499312 -> 140106835499168
	140106835499312 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835499408 -> 140106835499312
	140106835499408 -> 140106832796288 [dir=none]
	140106832796288 [label="other
 ()" fillcolor=orange]
	140106835499408 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835498016 -> 140106835499408
	140106835499120 -> 140106835497536
	140106835499120 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106835499504 -> 140106835499120
	140106835499504 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106835499216 -> 140106835499504
	140106835499216 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106835499648 -> 140106835499216
	140106835499648 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106835499696 -> 140106835499648
	140106835499696 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106835499840 -> 140106835499696
	140106835499840 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106835499936 -> 140106835499840
	140107071063440 [label="detail_enc.encoder.attn_layers.0.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107071063440 -> 140106835499936
	140106835499936 [label=AccumulateGrad]
	140106835496768 -> 140106835496720
	140106835496768 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106857856544 -> 140106835496768
	140106857856544 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835497200 -> 140106857856544
	140106835497200 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835496864 -> 140106835497200
	140106835496864 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835498064 -> 140106835496864
	140106835498064 -> 140107011970752 [dir=none]
	140107011970752 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835498064 -> 140107071064160 [dir=none]
	140107071064160 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835498064 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835495424 -> 140106835498064
	140106835498496 -> 140106835498064
	140107071064160 [label="detail_enc.encoder.attn_layers.0.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071064160 -> 140106835498496
	140106835498496 [label=AccumulateGrad]
	140106835497728 -> 140106835498064
	140107071064240 [label="detail_enc.encoder.attn_layers.0.conv_v.bias
 (192)" fillcolor=lightblue]
	140107071064240 -> 140106835497728
	140106835497728 [label=AccumulateGrad]
	140106835496528 -> 140106835496480
	140106835496528 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835497008 -> 140106835496528
	140106835497008 -> 140106832796768 [dir=none]
	140106832796768 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106835497008 -> 140106832797088 [dir=none]
	140106832797088 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106835497008 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835497296 -> 140106835497008
	140106835497296 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835498736 -> 140106835497296
	140106835498736 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835498928 -> 140106835498736
	140106835498928 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106835499072 -> 140106835498928
	140106835499072 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835499456 -> 140106835499072
	140106835499456 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835499552 -> 140106835499456
	140106835499552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835499744 -> 140106835499552
	140106835499744 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106835500032 -> 140106835499744
	140106835500032 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106835499264 -> 140106835500032
	140106835499264 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835500128 -> 140106835499264
	140106835500128 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106835497056 -> 140106835500128
	140106835497104 -> 140106835497008
	140106835497104 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106835499024 -> 140106835497104
	140106835499024 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106835499600 -> 140106835499024
	140106835499600 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106835499888 -> 140106835499600
	140106835499888 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106835500080 -> 140106835499888
	140106835500080 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106835500176 -> 140106835500080
	140107071064720 [label="detail_enc.encoder.attn_layers.0.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107071064720 -> 140106835500176
	140106835500176 [label=AccumulateGrad]
	140106835495856 -> 140106835495760
	140107071064480 [label="detail_enc.encoder.attn_layers.0.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071064480 -> 140106835495856
	140106835495856 [label=AccumulateGrad]
	140106835495568 -> 140106835495760
	140107071064560 [label="detail_enc.encoder.attn_layers.0.conv_o.bias
 (192)" fillcolor=lightblue]
	140107071064560 -> 140106835495568
	140106835495568 [label=AccumulateGrad]
	140106835495136 -> 140106835495040
	140107071065200 [label="detail_enc.encoder.norm_layers_1.0.gamma
 (192)" fillcolor=lightblue]
	140107071065200 -> 140106835495136
	140106835495136 [label=AccumulateGrad]
	140106835494896 -> 140106835495040
	140107071065360 [label="detail_enc.encoder.norm_layers_1.0.beta
 (192)" fillcolor=lightblue]
	140107071065360 -> 140106835494896
	140106835494896 [label=AccumulateGrad]
	140106835494848 -> 140106835494752
	140106835494848 -> 140106814115728 [dir=none]
	140106814115728 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106835494848 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835495184 -> 140106835494848
	140106835495184 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835495184 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835495328 -> 140106835495184
	140106835495328 -> 140107011980672 [dir=none]
	140107011980672 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106835495328 -> 140107071065840 [dir=none]
	140107071065840 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106835495328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835495664 -> 140106835495328
	140106835495664 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835496432 -> 140106835495664
	140106835496432 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835496432 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835496624 -> 140106835496432
	140106835496624 -> 140106832789968 [dir=none]
	140106832789968 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106835496624 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835498832 -> 140106835496624
	140106835498832 -> 140106832796608 [dir=none]
	140106832796608 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106835498832 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106835499792 -> 140106835498832
	140106835499792 -> 140107011978512 [dir=none]
	140107011978512 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106835499792 -> 140107071065440 [dir=none]
	140107071065440 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106835499792 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835500224 -> 140106835499792
	140106835500224 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835500320 -> 140106835500224
	140106835500320 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835500320 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835494800 -> 140106835500320
	140106835499984 -> 140106835499792
	140107071065440 [label="detail_enc.encoder.ffn_layers.0.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107071065440 -> 140106835499984
	140106835499984 [label=AccumulateGrad]
	140106835496336 -> 140106835499792
	140107071065760 [label="detail_enc.encoder.ffn_layers.0.conv_1.bias
 (768)" fillcolor=lightblue]
	140107071065760 -> 140106835496336
	140106835496336 [label=AccumulateGrad]
	140106835495232 -> 140106835495328
	140107071065840 [label="detail_enc.encoder.ffn_layers.0.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107071065840 -> 140106835495232
	140106835495232 [label=AccumulateGrad]
	140106835494944 -> 140106835495328
	140107071065920 [label="detail_enc.encoder.ffn_layers.0.conv_2.bias
 (192)" fillcolor=lightblue]
	140107071065920 -> 140106835494944
	140106835494944 [label=AccumulateGrad]
	140106835494512 -> 140106835494416
	140107071066080 [label="detail_enc.encoder.norm_layers_2.0.gamma
 (192)" fillcolor=lightblue]
	140107071066080 -> 140106835494512
	140106835494512 [label=AccumulateGrad]
	140106835494272 -> 140106835494416
	140107071066400 [label="detail_enc.encoder.norm_layers_2.0.beta
 (192)" fillcolor=lightblue]
	140107071066400 -> 140106835494272
	140106835494272 [label=AccumulateGrad]
	140106835494224 -> 140106835494128
	140106835494224 -> 140106832797248 [dir=none]
	140106832797248 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106835494224 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835494560 -> 140106835494224
	140106835494560 -> 140107003795376 [dir=none]
	140107003795376 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835494560 -> 140107071067040 [dir=none]
	140107071067040 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835494560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835494704 -> 140106835494560
	140106835494704 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835496096 -> 140106835494704
	140106835496096 [label=CloneBackward0]
	140106835496192 -> 140106835496096
	140106835496192 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835499360 -> 140106835496192
	140106835499360 [label="AddBackward0
------------
alpha: 1"]
	140106835496912 -> 140106835499360
	140106835496912 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835497488 -> 140106835496912
	140106835497488 -> 140106832797168 [dir=none]
	140106832797168 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106835497488 -> 140106832796848 [dir=none]
	140106832796848 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106835497488 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835500512 -> 140106835497488
	140106835500512 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835500656 -> 140106835500512
	140106835500656 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835500752 -> 140106835500656
	140106835500752 -> 140106832797408 [dir=none]
	140106832797408 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106835500752 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835500848 -> 140106835500752
	140106835500848 -> 140106832797008 [dir=none]
	140106832797008 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106835500848 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106835500944 -> 140106835500848
	140106835500944 -> 140107003792176 [dir=none]
	140107003792176 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106835500944 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106835501040 -> 140106835500944
	140106835501040 [label="AddBackward0
------------
alpha: 1"]
	140106835501136 -> 140106835501040
	140106835501136 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106835501280 -> 140106835501136
	140106835501280 -> 140106832797808 [dir=none]
	140106832797808 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106835501280 -> 140106832797728 [dir=none]
	140106832797728 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835501280 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835501376 -> 140106835501280
	140106835501376 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835501520 -> 140106835501376
	140106835501520 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835501616 -> 140106835501520
	140106835501616 -> 140106832797648 [dir=none]
	140106832797648 [label="other
 ()" fillcolor=orange]
	140106835501616 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835501712 -> 140106835501616
	140106835501712 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835501808 -> 140106835501712
	140106835501808 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835501904 -> 140106835501808
	140106835501904 -> 140107003789776 [dir=none]
	140107003789776 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835501904 -> 140107071066480 [dir=none]
	140107071066480 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835501904 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835494176 -> 140106835501904
	140106835502000 -> 140106835501904
	140107071066480 [label="detail_enc.encoder.attn_layers.1.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071066480 -> 140106835502000
	140106835502000 [label=AccumulateGrad]
	140106835501952 -> 140106835501904
	140107071066560 [label="detail_enc.encoder.attn_layers.1.conv_q.bias
 (192)" fillcolor=lightblue]
	140107071066560 -> 140106835501952
	140106835501952 [label=AccumulateGrad]
	140106835501328 -> 140106835501280
	140106835501328 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835501664 -> 140106835501328
	140106835501664 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835501856 -> 140106835501664
	140106835501856 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106835502096 -> 140106835501856
	140106835502096 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835502144 -> 140106835502096
	140106835502144 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835502288 -> 140106835502144
	140106835502288 -> 140107003789776 [dir=none]
	140107003789776 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835502288 -> 140107071066640 [dir=none]
	140107071066640 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835502288 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835494176 -> 140106835502288
	140106835502384 -> 140106835502288
	140107071066640 [label="detail_enc.encoder.attn_layers.1.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071066640 -> 140106835502384
	140106835502384 [label=AccumulateGrad]
	140106835502336 -> 140106835502288
	140107071066720 [label="detail_enc.encoder.attn_layers.1.conv_k.bias
 (192)" fillcolor=lightblue]
	140107071066720 -> 140106835502336
	140106835502336 [label=AccumulateGrad]
	140106835501088 -> 140106835501040
	140106835501088 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106835501568 -> 140106835501088
	140106835501568 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106835502048 -> 140106835501568
	140106835502048 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106835502240 -> 140106835502048
	140106835502240 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106835501472 -> 140106835502240
	140106835501472 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106835502480 -> 140106835501472
	140106835502480 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106835502576 -> 140106835502480
	140106835502576 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106835502672 -> 140106835502576
	140106835502672 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106835502768 -> 140106835502672
	140106835502768 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106835502864 -> 140106835502768
	140106835502864 -> 140106814118528 [dir=none]
	140106814118528 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106835502864 -> 140106832797488 [dir=none]
	140106832797488 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835502864 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835502960 -> 140106835502864
	140106835502960 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835503104 -> 140106835502960
	140106835503104 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835503200 -> 140106835503104
	140106835503200 -> 140106832797968 [dir=none]
	140106832797968 [label="other
 ()" fillcolor=orange]
	140106835503200 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835501712 -> 140106835503200
	140106835502912 -> 140106835502864
	140106835502912 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106835503296 -> 140106835502912
	140106835503296 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106835503008 -> 140106835503296
	140106835503008 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106835503440 -> 140106835503008
	140106835503440 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106835503488 -> 140106835503440
	140106835503488 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106835503632 -> 140106835503488
	140106835503632 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106835503728 -> 140106835503632
	140107071064880 [label="detail_enc.encoder.attn_layers.1.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107071064880 -> 140106835503728
	140106835503728 [label=AccumulateGrad]
	140106835500464 -> 140106835497488
	140106835500464 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835500800 -> 140106835500464
	140106835500800 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835500992 -> 140106835500800
	140106835500992 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835501184 -> 140106835500992
	140106835501184 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835501424 -> 140106835501184
	140106835501424 -> 140107003789776 [dir=none]
	140107003789776 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835501424 -> 140107071066880 [dir=none]
	140107071066880 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835501424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835494176 -> 140106835501424
	140106835502432 -> 140106835501424
	140107071066880 [label="detail_enc.encoder.attn_layers.1.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071066880 -> 140106835502432
	140106835502432 [label=AccumulateGrad]
	140106835502192 -> 140106835501424
	140107071066960 [label="detail_enc.encoder.attn_layers.1.conv_v.bias
 (192)" fillcolor=lightblue]
	140107071066960 -> 140106835502192
	140106835502192 [label=AccumulateGrad]
	140106835500272 -> 140106835499360
	140106835500272 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835500704 -> 140106835500272
	140106835500704 -> 140106832798048 [dir=none]
	140106832798048 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106835500704 -> 140106832798208 [dir=none]
	140106832798208 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106835500704 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835500560 -> 140106835500704
	140106835500560 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835502528 -> 140106835500560
	140106835502528 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835502720 -> 140106835502528
	140106835502720 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106835501232 -> 140106835502720
	140106835501232 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835503248 -> 140106835501232
	140106835503248 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835503344 -> 140106835503248
	140106835503344 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835503536 -> 140106835503344
	140106835503536 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106835503824 -> 140106835503536
	140106835503824 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106835503056 -> 140106835503824
	140106835503056 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835503920 -> 140106835503056
	140106835503920 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106835500752 -> 140106835503920
	140106835500896 -> 140106835500704
	140106835500896 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106835502816 -> 140106835500896
	140106835502816 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106835503392 -> 140106835502816
	140106835503392 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106835503680 -> 140106835503392
	140106835503680 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106835503872 -> 140106835503680
	140106835503872 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106835503968 -> 140106835503872
	140107071067440 [label="detail_enc.encoder.attn_layers.1.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107071067440 -> 140106835503968
	140106835503968 [label=AccumulateGrad]
	140106835494656 -> 140106835494560
	140107071067040 [label="detail_enc.encoder.attn_layers.1.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071067040 -> 140106835494656
	140106835494656 [label=AccumulateGrad]
	140106835494320 -> 140106835494560
	140107071067360 [label="detail_enc.encoder.attn_layers.1.conv_o.bias
 (192)" fillcolor=lightblue]
	140107071067360 -> 140106835494320
	140106835494320 [label=AccumulateGrad]
	140106835493888 -> 140106835493792
	140107071068320 [label="detail_enc.encoder.norm_layers_1.1.gamma
 (192)" fillcolor=lightblue]
	140107071068320 -> 140106835493888
	140106835493888 [label=AccumulateGrad]
	140106835493648 -> 140106835493792
	140107071068480 [label="detail_enc.encoder.norm_layers_1.1.beta
 (192)" fillcolor=lightblue]
	140107071068480 -> 140106835493648
	140106835493648 [label=AccumulateGrad]
	140106835493600 -> 140106835493504
	140106835493600 -> 140106832797888 [dir=none]
	140106832797888 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106835493600 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835493936 -> 140106835493600
	140106835493936 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835493936 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835494080 -> 140106835493936
	140106835494080 -> 140107003796576 [dir=none]
	140107003796576 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106835494080 -> 140107071068720 [dir=none]
	140107071068720 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106835494080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835494368 -> 140106835494080
	140106835494368 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835496672 -> 140106835494368
	140106835496672 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835496672 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835500416 -> 140106835496672
	140106835500416 -> 140106832797328 [dir=none]
	140106832797328 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106835500416 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835502624 -> 140106835500416
	140106835502624 -> 140106832798288 [dir=none]
	140106832798288 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106835502624 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106835503584 -> 140106835502624
	140106835503584 -> 140107003794736 [dir=none]
	140107003794736 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106835503584 -> 140107071068560 [dir=none]
	140107071068560 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106835503584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835504016 -> 140106835503584
	140106835504016 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835504112 -> 140106835504016
	140106835504112 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835504112 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835493552 -> 140106835504112
	140106835503776 -> 140106835503584
	140107071068560 [label="detail_enc.encoder.ffn_layers.1.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107071068560 -> 140106835503776
	140106835503776 [label=AccumulateGrad]
	140106835495280 -> 140106835503584
	140107071068640 [label="detail_enc.encoder.ffn_layers.1.conv_1.bias
 (768)" fillcolor=lightblue]
	140107071068640 -> 140106835495280
	140106835495280 [label=AccumulateGrad]
	140106835493984 -> 140106835494080
	140107071068720 [label="detail_enc.encoder.ffn_layers.1.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107071068720 -> 140106835493984
	140106835493984 [label=AccumulateGrad]
	140106835493696 -> 140106835494080
	140107071068880 [label="detail_enc.encoder.ffn_layers.1.conv_2.bias
 (192)" fillcolor=lightblue]
	140107071068880 -> 140106835493696
	140106835493696 [label=AccumulateGrad]
	140106835493264 -> 140106835493168
	140107071068960 [label="detail_enc.encoder.norm_layers_2.1.gamma
 (192)" fillcolor=lightblue]
	140107071068960 -> 140106835493264
	140106835493264 [label=AccumulateGrad]
	140106835493024 -> 140106835493168
	140107071069040 [label="detail_enc.encoder.norm_layers_2.1.beta
 (192)" fillcolor=lightblue]
	140107071069040 -> 140106835493024
	140106835493024 [label=AccumulateGrad]
	140106835492928 -> 140106804674512
	140106835492928 -> 140106832798368 [dir=none]
	140106832798368 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106835492928 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835493312 -> 140106835492928
	140106835493312 -> 140106873116112 [dir=none]
	140106873116112 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835493312 -> 140107071070000 [dir=none]
	140107071070000 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835493312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835493456 -> 140106835493312
	140106835493456 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835494608 -> 140106835493456
	140106835494608 [label=CloneBackward0]
	140106835494992 -> 140106835494608
	140106835494992 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835503152 -> 140106835494992
	140106835503152 [label="AddBackward0
------------
alpha: 1"]
	140106835500608 -> 140106835503152
	140106835500608 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835501760 -> 140106835500608
	140106835501760 -> 140106832797568 [dir=none]
	140106832797568 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106835501760 -> 140106832789648 [dir=none]
	140106832789648 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106835501760 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835504304 -> 140106835501760
	140106835504304 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835504448 -> 140106835504304
	140106835504448 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835504544 -> 140106835504448
	140106835504544 -> 140106832798528 [dir=none]
	140106832798528 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106835504544 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835504640 -> 140106835504544
	140106835504640 -> 140106832798128 [dir=none]
	140106832798128 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106835504640 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106835504736 -> 140106835504640
	140106835504736 -> 140107003799376 [dir=none]
	140107003799376 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106835504736 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106835504832 -> 140106835504736
	140106835504832 [label="AddBackward0
------------
alpha: 1"]
	140106835504928 -> 140106835504832
	140106835504928 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106835505072 -> 140106835504928
	140106835505072 -> 140106832798928 [dir=none]
	140106832798928 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106835505072 -> 140106832798848 [dir=none]
	140106832798848 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835505072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835505168 -> 140106835505072
	140106835505168 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835505312 -> 140106835505168
	140106835505312 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835505408 -> 140106835505312
	140106835505408 -> 140106832798768 [dir=none]
	140106832798768 [label="other
 ()" fillcolor=orange]
	140106835505408 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835505504 -> 140106835505408
	140106835505504 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835505600 -> 140106835505504
	140106835505600 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835505696 -> 140106835505600
	140106835505696 -> 140107003792976 [dir=none]
	140107003792976 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835505696 -> 140107071069200 [dir=none]
	140107071069200 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835505696 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804674368 -> 140106835505696
	140106835505792 -> 140106835505696
	140107071069200 [label="detail_enc.encoder.attn_layers.2.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071069200 -> 140106835505792
	140106835505792 [label=AccumulateGrad]
	140106835505744 -> 140106835505696
	140107071069280 [label="detail_enc.encoder.attn_layers.2.conv_q.bias
 (192)" fillcolor=lightblue]
	140107071069280 -> 140106835505744
	140106835505744 [label=AccumulateGrad]
	140106835505120 -> 140106835505072
	140106835505120 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835505456 -> 140106835505120
	140106835505456 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835505648 -> 140106835505456
	140106835505648 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106835505888 -> 140106835505648
	140106835505888 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835505936 -> 140106835505888
	140106835505936 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835506080 -> 140106835505936
	140106835506080 -> 140107003792976 [dir=none]
	140107003792976 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835506080 -> 140107071069360 [dir=none]
	140107071069360 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835506080 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804674368 -> 140106835506080
	140106835506176 -> 140106835506080
	140107071069360 [label="detail_enc.encoder.attn_layers.2.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071069360 -> 140106835506176
	140106835506176 [label=AccumulateGrad]
	140106835506128 -> 140106835506080
	140107071069440 [label="detail_enc.encoder.attn_layers.2.conv_k.bias
 (192)" fillcolor=lightblue]
	140107071069440 -> 140106835506128
	140106835506128 [label=AccumulateGrad]
	140106835504880 -> 140106835504832
	140106835504880 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106835505360 -> 140106835504880
	140106835505360 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106835505840 -> 140106835505360
	140106835505840 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106835506032 -> 140106835505840
	140106835506032 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106835505264 -> 140106835506032
	140106835505264 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106835506272 -> 140106835505264
	140106835506272 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106835506368 -> 140106835506272
	140106835506368 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106835506464 -> 140106835506368
	140106835506464 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106835506560 -> 140106835506464
	140106835506560 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106835506656 -> 140106835506560
	140106835506656 -> 140106832798608 [dir=none]
	140106832798608 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106835506656 -> 140106832791888 [dir=none]
	140106832791888 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835506656 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835506752 -> 140106835506656
	140106835506752 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835506896 -> 140106835506752
	140106835506896 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835506992 -> 140106835506896
	140106835506992 -> 140106832799008 [dir=none]
	140106832799008 [label="other
 ()" fillcolor=orange]
	140106835506992 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835505504 -> 140106835506992
	140106835506704 -> 140106835506656
	140106835506704 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106835507088 -> 140106835506704
	140106835507088 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106835506800 -> 140106835507088
	140106835506800 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106835507232 -> 140106835506800
	140106835507232 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106835507280 -> 140106835507232
	140106835507280 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106835507424 -> 140106835507280
	140106835507424 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106835507520 -> 140106835507424
	140107071068000 [label="detail_enc.encoder.attn_layers.2.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107071068000 -> 140106835507520
	140106835507520 [label=AccumulateGrad]
	140106835504256 -> 140106835501760
	140106835504256 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835504592 -> 140106835504256
	140106835504592 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835505984 -> 140106835504592
	140106835505984 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835506320 -> 140106835505984
	140106835506320 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835506512 -> 140106835506320
	140106835506512 -> 140107003792976 [dir=none]
	140107003792976 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835506512 -> 140107071069680 [dir=none]
	140107071069680 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835506512 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804674368 -> 140106835506512
	140106835505024 -> 140106835506512
	140107071069680 [label="detail_enc.encoder.attn_layers.2.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071069680 -> 140106835505024
	140106835505024 [label=AccumulateGrad]
	140106835506608 -> 140106835506512
	140107071069840 [label="detail_enc.encoder.attn_layers.2.conv_v.bias
 (192)" fillcolor=lightblue]
	140107071069840 -> 140106835506608
	140106835506608 [label=AccumulateGrad]
	140106835504064 -> 140106835503152
	140106835504064 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835504496 -> 140106835504064
	140106835504496 -> 140106832799328 [dir=none]
	140106832799328 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106835504496 -> 140106832799168 [dir=none]
	140106832799168 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106835504496 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835506224 -> 140106835504496
	140106835506224 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835506944 -> 140106835506224
	140106835506944 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835507184 -> 140106835506944
	140106835507184 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106835507376 -> 140106835507184
	140106835507376 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835507472 -> 140106835507376
	140106835507472 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835507568 -> 140106835507472
	140106835507568 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106835507664 -> 140106835507568
	140106835507664 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106835507760 -> 140106835507664
	140106835507760 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106835507856 -> 140106835507760
	140106835507856 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106835507952 -> 140106835507856
	140106835507952 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106835504544 -> 140106835507952
	140106835505216 -> 140106835504496
	140106835505216 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106835507136 -> 140106835505216
	140106835507136 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106835507616 -> 140106835507136
	140106835507616 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106835507712 -> 140106835507616
	140106835507712 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106835507904 -> 140106835507712
	140106835507904 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106835508000 -> 140106835507904
	140107071070320 [label="detail_enc.encoder.attn_layers.2.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107071070320 -> 140106835508000
	140106835508000 [label=AccumulateGrad]
	140106835493408 -> 140106835493312
	140107071070000 [label="detail_enc.encoder.attn_layers.2.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071070000 -> 140106835493408
	140106835493408 [label=AccumulateGrad]
	140106835493072 -> 140106835493312
	140107071070240 [label="detail_enc.encoder.attn_layers.2.conv_o.bias
 (192)" fillcolor=lightblue]
	140107071070240 -> 140106835493072
	140106835493072 [label=AccumulateGrad]
	140106804674272 -> 140106804674176
	140107071070480 [label="detail_enc.encoder.norm_layers_1.2.gamma
 (192)" fillcolor=lightblue]
	140107071070480 -> 140106804674272
	140106804674272 [label=AccumulateGrad]
	140106804674032 -> 140106804674176
	140107071070640 [label="detail_enc.encoder.norm_layers_1.2.beta
 (192)" fillcolor=lightblue]
	140107071070640 -> 140106804674032
	140106804674032 [label=AccumulateGrad]
	140106804673984 -> 140106804673888
	140106804673984 -> 140106832796928 [dir=none]
	140106832796928 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804673984 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804674416 -> 140106804673984
	140106804674416 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804674416 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804674464 -> 140106804674416
	140106804674464 -> 140107003804496 [dir=none]
	140107003804496 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106804674464 -> 140107071071120 [dir=none]
	140107071071120 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106804674464 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835504976 -> 140106804674464
	140106835504976 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835493120 -> 140106835504976
	140106835493120 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835493120 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835494032 -> 140106835493120
	140106835494032 -> 140106832799248 [dir=none]
	140106832799248 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106835494032 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835500368 -> 140106835494032
	140106835500368 -> 140106832798688 [dir=none]
	140106832798688 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106835500368 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106835504208 -> 140106835500368
	140106835504208 -> 140107003803376 [dir=none]
	140107003803376 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106835504208 -> 140107071070720 [dir=none]
	140107071070720 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106835504208 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106835507040 -> 140106835504208
	140106835507040 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835508048 -> 140106835507040
	140106835508048 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835508048 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804673936 -> 140106835508048
	140106835504160 -> 140106835504208
	140107071070720 [label="detail_enc.encoder.ffn_layers.2.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107071070720 -> 140106835504160
	140106835504160 [label=AccumulateGrad]
	140106835504352 -> 140106835504208
	140107071070800 [label="detail_enc.encoder.ffn_layers.2.conv_1.bias
 (768)" fillcolor=lightblue]
	140107071070800 -> 140106835504352
	140106835504352 [label=AccumulateGrad]
	140106835504688 -> 140106804674464
	140107071071120 [label="detail_enc.encoder.ffn_layers.2.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107071071120 -> 140106835504688
	140106835504688 [label=AccumulateGrad]
	140106835504400 -> 140106804674464
	140107071071200 [label="detail_enc.encoder.ffn_layers.2.conv_2.bias
 (192)" fillcolor=lightblue]
	140107071071200 -> 140106835504400
	140106835504400 [label=AccumulateGrad]
	140106804673648 -> 140106804673552
	140107071071280 [label="detail_enc.encoder.norm_layers_2.2.gamma
 (192)" fillcolor=lightblue]
	140107071071280 -> 140106804673648
	140106804673648 [label=AccumulateGrad]
	140106804673408 -> 140106804673552
	140107071071520 [label="detail_enc.encoder.norm_layers_2.2.beta
 (192)" fillcolor=lightblue]
	140107071071520 -> 140106804673408
	140106804673408 [label=AccumulateGrad]
	140106804673360 -> 140106804673264
	140106804673360 -> 140106832799568 [dir=none]
	140106832799568 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804673360 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804673696 -> 140106804673360
	140106804673696 -> 140106869337568 [dir=none]
	140106869337568 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804673696 -> 140107071072560 [dir=none]
	140107071072560 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106804673696 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804673840 -> 140106804673696
	140106804673840 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106804674080 -> 140106804673840
	140106804674080 [label=CloneBackward0]
	140106835493360 -> 140106804674080
	140106835493360 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835493744 -> 140106835493360
	140106835493744 [label="AddBackward0
------------
alpha: 1"]
	140106835507808 -> 140106835493744
	140106835507808 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835506848 -> 140106835507808
	140106835506848 -> 140106832799408 [dir=none]
	140106832799408 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106835506848 -> 140106832799088 [dir=none]
	140106832799088 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106835506848 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835508144 -> 140106835506848
	140106835508144 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835508288 -> 140106835508144
	140106835508288 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106835508384 -> 140106835508288
	140106835508384 -> 140106832799728 [dir=none]
	140106832799728 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106835508384 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835508480 -> 140106835508384
	140106835508480 -> 140106832799488 [dir=none]
	140106832799488 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106835508480 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106835508576 -> 140106835508480
	140106835508576 -> 140106873116912 [dir=none]
	140106873116912 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106835508576 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106835508672 -> 140106835508576
	140106835508672 [label="AddBackward0
------------
alpha: 1"]
	140106835508768 -> 140106835508672
	140106835508768 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106835508912 -> 140106835508768
	140106835508912 -> 140106832800128 [dir=none]
	140106832800128 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106835508912 -> 140106832800048 [dir=none]
	140106832800048 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106835508912 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835509008 -> 140106835508912
	140106835509008 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835509152 -> 140106835509008
	140106835509152 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835509200 -> 140106835509152
	140106835509200 -> 140106832799968 [dir=none]
	140106832799968 [label="other
 ()" fillcolor=orange]
	140106835509200 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827727008 -> 140106835509200
	140106827727008 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827727104 -> 140106827727008
	140106827727104 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827727200 -> 140106827727104
	140106827727200 -> 140107003800496 [dir=none]
	140107003800496 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827727200 -> 140107071071600 [dir=none]
	140107071071600 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827727200 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804673312 -> 140106827727200
	140106827727296 -> 140106827727200
	140107071071600 [label="detail_enc.encoder.attn_layers.3.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071071600 -> 140106827727296
	140106827727296 [label=AccumulateGrad]
	140106827727248 -> 140106827727200
	140107071071760 [label="detail_enc.encoder.attn_layers.3.conv_q.bias
 (192)" fillcolor=lightblue]
	140107071071760 -> 140106827727248
	140106827727248 [label=AccumulateGrad]
	140106835508960 -> 140106835508912
	140106835508960 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106835509056 -> 140106835508960
	140106835509056 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106827727152 -> 140106835509056
	140106827727152 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106827727392 -> 140106827727152
	140106827727392 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827727440 -> 140106827727392
	140106827727440 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827727584 -> 140106827727440
	140106827727584 -> 140107003800496 [dir=none]
	140107003800496 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827727584 -> 140107071071920 [dir=none]
	140107071071920 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827727584 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804673312 -> 140106827727584
	140106827727680 -> 140106827727584
	140107071071920 [label="detail_enc.encoder.attn_layers.3.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071071920 -> 140106827727680
	140106827727680 [label=AccumulateGrad]
	140106827727632 -> 140106827727584
	140107071072000 [label="detail_enc.encoder.attn_layers.3.conv_k.bias
 (192)" fillcolor=lightblue]
	140107071072000 -> 140106827727632
	140106827727632 [label=AccumulateGrad]
	140106835508720 -> 140106835508672
	140106835508720 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106835509104 -> 140106835508720
	140106835509104 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106835508864 -> 140106835509104
	140106835508864 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106827727536 -> 140106835508864
	140106827727536 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106827726960 -> 140106827727536
	140106827726960 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106827727776 -> 140106827726960
	140106827727776 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106827727872 -> 140106827727776
	140106827727872 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106827728016 -> 140106827727872
	140106827728016 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106827728112 -> 140106827728016
	140106827728112 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106827728208 -> 140106827728112
	140106827728208 -> 140106832800288 [dir=none]
	140106832800288 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106827728208 -> 140106832799648 [dir=none]
	140106832799648 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106827728208 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827728304 -> 140106827728208
	140106827728304 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827728448 -> 140106827728304
	140106827728448 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827728544 -> 140106827728448
	140106827728544 -> 140106832800208 [dir=none]
	140106832800208 [label="other
 ()" fillcolor=orange]
	140106827728544 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827727008 -> 140106827728544
	140106827728256 -> 140106827728208
	140106827728256 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106827728640 -> 140106827728256
	140106827728640 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106827728352 -> 140106827728640
	140106827728352 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106827728784 -> 140106827728352
	140106827728784 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106827728832 -> 140106827728784
	140106827728832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106827728976 -> 140106827728832
	140106827728976 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106827729072 -> 140106827728976
	140107071070400 [label="detail_enc.encoder.attn_layers.3.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107071070400 -> 140106827729072
	140106827729072 [label=AccumulateGrad]
	140106835505552 -> 140106835506848
	140106835505552 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835508432 -> 140106835505552
	140106835508432 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106835508624 -> 140106835508432
	140106835508624 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835508816 -> 140106835508624
	140106835508816 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106835508240 -> 140106835508816
	140106835508240 -> 140107003800496 [dir=none]
	140107003800496 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106835508240 -> 140107071072160 [dir=none]
	140107071072160 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106835508240 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804673312 -> 140106835508240
	140106827727728 -> 140106835508240
	140107071072160 [label="detail_enc.encoder.attn_layers.3.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071072160 -> 140106827727728
	140106827727728 [label=AccumulateGrad]
	140106827727488 -> 140106835508240
	140107071072400 [label="detail_enc.encoder.attn_layers.3.conv_v.bias
 (192)" fillcolor=lightblue]
	140107071072400 -> 140106827727488
	140106827727488 [label=AccumulateGrad]
	140106835507328 -> 140106835493744
	140106835507328 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106835508336 -> 140106835507328
	140106835508336 -> 140106832800528 [dir=none]
	140106832800528 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106835508336 -> 140106832800848 [dir=none]
	140106832800848 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106835508336 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106835508192 -> 140106835508336
	140106835508192 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827727824 -> 140106835508192
	140106827727824 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827728064 -> 140106827727824
	140106827728064 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106827727056 -> 140106827728064
	140106827727056 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827728592 -> 140106827727056
	140106827728592 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827728688 -> 140106827728592
	140106827728688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827728880 -> 140106827728688
	140106827728880 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106827729216 -> 140106827728880
	140106827729216 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106827729312 -> 140106827729216
	140106827729312 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827729408 -> 140106827729312
	140106827729408 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106835508384 -> 140106827729408
	140106835508528 -> 140106835508336
	140106835508528 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106827728160 -> 140106835508528
	140106827728160 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106827728736 -> 140106827728160
	140106827728736 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106827729120 -> 140106827728736
	140106827729120 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106827729360 -> 140106827729120
	140106827729360 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106827729456 -> 140106827729360
	140107071072800 [label="detail_enc.encoder.attn_layers.3.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107071072800 -> 140106827729456
	140106827729456 [label=AccumulateGrad]
	140106804673792 -> 140106804673696
	140107071072560 [label="detail_enc.encoder.attn_layers.3.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071072560 -> 140106804673792
	140106804673792 [label=AccumulateGrad]
	140106804673456 -> 140106804673696
	140107071072640 [label="detail_enc.encoder.attn_layers.3.conv_o.bias
 (192)" fillcolor=lightblue]
	140107071072640 -> 140106804673456
	140106804673456 [label=AccumulateGrad]
	140106804673024 -> 140106804672928
	140107071073280 [label="detail_enc.encoder.norm_layers_1.3.gamma
 (192)" fillcolor=lightblue]
	140107071073280 -> 140106804673024
	140106804673024 [label=AccumulateGrad]
	140106804672784 -> 140106804672928
	140107071073520 [label="detail_enc.encoder.norm_layers_1.3.beta
 (192)" fillcolor=lightblue]
	140107071073520 -> 140106804672784
	140106804672784 [label=AccumulateGrad]
	140106804672736 -> 140106804672640
	140106804672736 -> 140106832800368 [dir=none]
	140106832800368 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804672736 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804673072 -> 140106804672736
	140106804673072 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804673072 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804673216 -> 140106804673072
	140106804673216 -> 140106873126672 [dir=none]
	140106873126672 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106804673216 -> 140107071073920 [dir=none]
	140107071073920 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106804673216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804673504 -> 140106804673216
	140106804673504 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106804674128 -> 140106804673504
	140106804674128 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804674128 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106835506416 -> 140106804674128
	140106835506416 -> 140106832801008 [dir=none]
	140106832801008 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106835506416 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106835508096 -> 140106835506416
	140106835508096 -> 140106832800688 [dir=none]
	140106832800688 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106835508096 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106827728928 -> 140106835508096
	140106827728928 -> 140106872438848 [dir=none]
	140106872438848 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106827728928 -> 140107071073600 [dir=none]
	140107071073600 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106827728928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827729504 -> 140106827728928
	140106827729504 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106827729600 -> 140106827729504
	140106827729600 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106827729600 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804672688 -> 140106827729600
	140106827729264 -> 140106827728928
	140107071073600 [label="detail_enc.encoder.ffn_layers.3.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107071073600 -> 140106827729264
	140106827729264 [label=AccumulateGrad]
	140106827727920 -> 140106827728928
	140107071073680 [label="detail_enc.encoder.ffn_layers.3.conv_1.bias
 (768)" fillcolor=lightblue]
	140107071073680 -> 140106827727920
	140106827727920 [label=AccumulateGrad]
	140106804673120 -> 140106804673216
	140107071073920 [label="detail_enc.encoder.ffn_layers.3.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107071073920 -> 140106804673120
	140106804673120 [label=AccumulateGrad]
	140106804672832 -> 140106804673216
	140107071074000 [label="detail_enc.encoder.ffn_layers.3.conv_2.bias
 (192)" fillcolor=lightblue]
	140107071074000 -> 140106804672832
	140106804672832 [label=AccumulateGrad]
	140106804672400 -> 140106804672304
	140107071074240 [label="detail_enc.encoder.norm_layers_2.3.gamma
 (192)" fillcolor=lightblue]
	140107071074240 -> 140106804672400
	140106804672400 [label=AccumulateGrad]
	140106804672160 -> 140106804672304
	140107071074320 [label="detail_enc.encoder.norm_layers_2.3.beta
 (192)" fillcolor=lightblue]
	140107071074320 -> 140106804672160
	140106804672160 [label=AccumulateGrad]
	140106804672112 -> 140106804672016
	140106804672112 -> 140106832801088 [dir=none]
	140106832801088 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804672112 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804672448 -> 140106804672112
	140106804672448 -> 140106861004096 [dir=none]
	140106861004096 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804672448 -> 140107071075280 [dir=none]
	140107071075280 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106804672448 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804672592 -> 140106804672448
	140106804672592 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106804673744 -> 140106804672592
	140106804673744 [label=CloneBackward0]
	140106804672880 -> 140106804673744
	140106804672880 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106835492976 -> 140106804672880
	140106835492976 [label="AddBackward0
------------
alpha: 1"]
	140106827727344 -> 140106835492976
	140106827727344 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106827726912 -> 140106827727344
	140106827726912 -> 140106832800928 [dir=none]
	140106832800928 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106827726912 -> 140106832800608 [dir=none]
	140106832800608 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106827726912 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827729792 -> 140106827726912
	140106827729792 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106827729936 -> 140106827729792
	140106827729936 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106827730032 -> 140106827729936
	140106827730032 -> 140106832801248 [dir=none]
	140106832801248 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106827730032 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827730128 -> 140106827730032
	140106827730128 -> 140106832800448 [dir=none]
	140106832800448 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106827730128 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106827730224 -> 140106827730128
	140106827730224 -> 140106868552496 [dir=none]
	140106868552496 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106827730224 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106827730320 -> 140106827730224
	140106827730320 [label="AddBackward0
------------
alpha: 1"]
	140106827730416 -> 140106827730320
	140106827730416 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106827730560 -> 140106827730416
	140106827730560 -> 140106832801648 [dir=none]
	140106832801648 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106827730560 -> 140106832801568 [dir=none]
	140106832801568 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106827730560 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827730656 -> 140106827730560
	140106827730656 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827730800 -> 140106827730656
	140106827730800 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827730896 -> 140106827730800
	140106827730896 -> 140106832801488 [dir=none]
	140106832801488 [label="other
 ()" fillcolor=orange]
	140106827730896 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827730992 -> 140106827730896
	140106827730992 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827731088 -> 140106827730992
	140106827731088 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827731184 -> 140106827731088
	140106827731184 -> 140106868543216 [dir=none]
	140106868543216 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827731184 -> 140107071074400 [dir=none]
	140107071074400 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827731184 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804672064 -> 140106827731184
	140106827731280 -> 140106827731184
	140107071074400 [label="detail_enc.encoder.attn_layers.4.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071074400 -> 140106827731280
	140106827731280 [label=AccumulateGrad]
	140106827731232 -> 140106827731184
	140107071074640 [label="detail_enc.encoder.attn_layers.4.conv_q.bias
 (192)" fillcolor=lightblue]
	140107071074640 -> 140106827731232
	140106827731232 [label=AccumulateGrad]
	140106827730608 -> 140106827730560
	140106827730608 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106827730944 -> 140106827730608
	140106827730944 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106827731136 -> 140106827730944
	140106827731136 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106827731376 -> 140106827731136
	140106827731376 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827731424 -> 140106827731376
	140106827731424 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827731568 -> 140106827731424
	140106827731568 -> 140106868543216 [dir=none]
	140106868543216 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827731568 -> 140107071074800 [dir=none]
	140107071074800 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827731568 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804672064 -> 140106827731568
	140106827731664 -> 140106827731568
	140107071074800 [label="detail_enc.encoder.attn_layers.4.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071074800 -> 140106827731664
	140106827731664 [label=AccumulateGrad]
	140106827731616 -> 140106827731568
	140107071074960 [label="detail_enc.encoder.attn_layers.4.conv_k.bias
 (192)" fillcolor=lightblue]
	140107071074960 -> 140106827731616
	140106827731616 [label=AccumulateGrad]
	140106827730368 -> 140106827730320
	140106827730368 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106827730848 -> 140106827730368
	140106827730848 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106827731328 -> 140106827730848
	140106827731328 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106827731520 -> 140106827731328
	140106827731520 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106827730752 -> 140106827731520
	140106827730752 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106827731760 -> 140106827730752
	140106827731760 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106827731856 -> 140106827731760
	140106827731856 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106827731952 -> 140106827731856
	140106827731952 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106827732048 -> 140106827731952
	140106827732048 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106827728400 -> 140106827732048
	140106827728400 -> 140106832801808 [dir=none]
	140106832801808 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106827728400 -> 140106832801168 [dir=none]
	140106832801168 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106827728400 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827732240 -> 140106827728400
	140106827732240 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827732384 -> 140106827732240
	140106827732384 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827732480 -> 140106827732384
	140106827732480 -> 140106832799808 [dir=none]
	140106832799808 [label="other
 ()" fillcolor=orange]
	140106827732480 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827730992 -> 140106827732480
	140106827732192 -> 140106827728400
	140106827732192 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106827732576 -> 140106827732192
	140106827732576 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106827732288 -> 140106827732576
	140106827732288 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106827732720 -> 140106827732288
	140106827732720 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106827732768 -> 140106827732720
	140106827732768 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106827732912 -> 140106827732768
	140106827732912 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106827733008 -> 140106827732912
	140107071072880 [label="detail_enc.encoder.attn_layers.4.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107071072880 -> 140106827733008
	140106827733008 [label=AccumulateGrad]
	140106827729744 -> 140106827726912
	140106827729744 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827730080 -> 140106827729744
	140106827730080 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827730272 -> 140106827730080
	140106827730272 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827730464 -> 140106827730272
	140106827730464 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827730704 -> 140106827730464
	140106827730704 -> 140106868543216 [dir=none]
	140106868543216 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827730704 -> 140107071075040 [dir=none]
	140107071075040 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827730704 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804672064 -> 140106827730704
	140106827731712 -> 140106827730704
	140107071075040 [label="detail_enc.encoder.attn_layers.4.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071075040 -> 140106827731712
	140106827731712 [label=AccumulateGrad]
	140106827731472 -> 140106827730704
	140107071075120 [label="detail_enc.encoder.attn_layers.4.conv_v.bias
 (192)" fillcolor=lightblue]
	140107071075120 -> 140106827731472
	140106827731472 [label=AccumulateGrad]
	140106827729552 -> 140106835492976
	140106827729552 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106827729984 -> 140106827729552
	140106827729984 -> 140106832801888 [dir=none]
	140106832801888 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106827729984 -> 140106832802128 [dir=none]
	140106832802128 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106827729984 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827729840 -> 140106827729984
	140106827729840 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827731808 -> 140106827729840
	140106827731808 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827732000 -> 140106827731808
	140106827732000 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106827730512 -> 140106827732000
	140106827730512 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827732528 -> 140106827730512
	140106827732528 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827732624 -> 140106827732528
	140106827732624 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827732816 -> 140106827732624
	140106827732816 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106827733104 -> 140106827732816
	140106827733104 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106827732336 -> 140106827733104
	140106827732336 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827733200 -> 140106827732336
	140106827733200 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106827730032 -> 140106827733200
	140106827730176 -> 140106827729984
	140106827730176 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106827732096 -> 140106827730176
	140106827732096 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106827732672 -> 140106827732096
	140106827732672 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106827732960 -> 140106827732672
	140106827732960 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106827733152 -> 140106827732960
	140106827733152 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106827733248 -> 140106827733152
	140107071075520 [label="detail_enc.encoder.attn_layers.4.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107071075520 -> 140106827733248
	140106827733248 [label=AccumulateGrad]
	140106804672544 -> 140106804672448
	140107071075280 [label="detail_enc.encoder.attn_layers.4.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107071075280 -> 140106804672544
	140106804672544 [label=AccumulateGrad]
	140106804672208 -> 140106804672448
	140107071075360 [label="detail_enc.encoder.attn_layers.4.conv_o.bias
 (192)" fillcolor=lightblue]
	140107071075360 -> 140106804672208
	140106804672208 [label=AccumulateGrad]
	140106804671776 -> 140106804671680
	140107071076720 [label="detail_enc.encoder.norm_layers_1.4.gamma
 (192)" fillcolor=lightblue]
	140107071076720 -> 140106804671776
	140106804671776 [label=AccumulateGrad]
	140106804671536 -> 140106804671680
	140107071076880 [label="detail_enc.encoder.norm_layers_1.4.beta
 (192)" fillcolor=lightblue]
	140107071076880 -> 140106804671536
	140106804671536 [label=AccumulateGrad]
	140106804671488 -> 140106804671392
	140106804671488 -> 140106832802048 [dir=none]
	140106832802048 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804671488 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804671824 -> 140106804671488
	140106804671824 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804671824 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804671968 -> 140106804671824
	140106804671968 -> 140106861007936 [dir=none]
	140106861007936 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106804671968 -> 140107071077760 [dir=none]
	140107071077760 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106804671968 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804672256 -> 140106804671968
	140106804672256 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106835496144 -> 140106804672256
	140106835496144 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106835496144 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804673168 -> 140106835496144
	140106804673168 -> 140106832802368 [dir=none]
	140106832802368 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106804673168 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827731904 -> 140106804673168
	140106827731904 -> 140106832802208 [dir=none]
	140106832802208 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106827731904 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106827732864 -> 140106827731904
	140106827732864 -> 140106861018016 [dir=none]
	140106861018016 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106827732864 -> 140107071077200 [dir=none]
	140107071077200 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106827732864 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827733296 -> 140106827732864
	140106827733296 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106827733392 -> 140106827733296
	140106827733392 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106827733392 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804671440 -> 140106827733392
	140106827733056 -> 140106827732864
	140107071077200 [label="detail_enc.encoder.ffn_layers.4.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107071077200 -> 140106827733056
	140106827733056 [label=AccumulateGrad]
	140106827728496 -> 140106827732864
	140107071077440 [label="detail_enc.encoder.ffn_layers.4.conv_1.bias
 (768)" fillcolor=lightblue]
	140107071077440 -> 140106827728496
	140106827728496 [label=AccumulateGrad]
	140106804671872 -> 140106804671968
	140107071077760 [label="detail_enc.encoder.ffn_layers.4.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107071077760 -> 140106804671872
	140106804671872 [label=AccumulateGrad]
	140106804671584 -> 140106804671968
	140107071077840 [label="detail_enc.encoder.ffn_layers.4.conv_2.bias
 (192)" fillcolor=lightblue]
	140107071077840 -> 140106804671584
	140106804671584 [label=AccumulateGrad]
	140106804671152 -> 140106804670624
	140107071078240 [label="detail_enc.encoder.norm_layers_2.4.gamma
 (192)" fillcolor=lightblue]
	140107071078240 -> 140106804671152
	140106804671152 [label=AccumulateGrad]
	140106804670432 -> 140106804670624
	140107071078320 [label="detail_enc.encoder.norm_layers_2.4.beta
 (192)" fillcolor=lightblue]
	140107071078320 -> 140106804670432
	140106804670432 [label=AccumulateGrad]
	140106804670288 -> 140106804670144
	140106804670288 -> 140106832802448 [dir=none]
	140106832802448 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804670288 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804671296 -> 140106804670288
	140106804671296 -> 140106853831744 [dir=none]
	140106853831744 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106804671296 -> 140107065181504 [dir=none]
	140107065181504 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106804671296 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804671344 -> 140106804671296
	140106804671344 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106804672496 -> 140106804671344
	140106804672496 [label=CloneBackward0]
	140106835504784 -> 140106804672496
	140106835504784 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827732432 -> 140106835504784
	140106827732432 [label="AddBackward0
------------
alpha: 1"]
	140106827729888 -> 140106827732432
	140106827729888 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106827732144 -> 140106827729888
	140106827732144 -> 140106832801408 [dir=none]
	140106832801408 [label="mat2
 (14, 286, 96)" fillcolor=orange]
	140106827732144 -> 140106832800768 [dir=none]
	140106832800768 [label="self
 (14, 286, 286)" fillcolor=orange]
	140106827732144 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827733584 -> 140106827732144
	140106827733584 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106827733728 -> 140106827733584
	140106827733728 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 286)"]
	140106827733824 -> 140106827733728
	140106827733824 -> 140106832802608 [dir=none]
	140106832802608 [label="result1
 (7, 2, 286, 286)" fillcolor=orange]
	140106827733824 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827733920 -> 140106827733824
	140106827733920 -> 140106832801968 [dir=none]
	140106832801968 [label="result
 (7, 2, 286, 286)" fillcolor=orange]
	140106827733920 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106827734016 -> 140106827733920
	140106827734016 -> 140106858889760 [dir=none]
	140106858889760 [label="mask
 (7, 1, 286, 286)" fillcolor=orange]
	140106827734016 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106827734112 -> 140106827734016
	140106827734112 [label="AddBackward0
------------
alpha: 1"]
	140106827734208 -> 140106827734112
	140106827734208 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 286)"]
	140106827734352 -> 140106827734208
	140106827734352 -> 140106832803008 [dir=none]
	140106832803008 [label="mat2
 (14, 96, 286)" fillcolor=orange]
	140106827734352 -> 140106832802928 [dir=none]
	140106832802928 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106827734352 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827734448 -> 140106827734352
	140106827734448 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827734592 -> 140106827734448
	140106827734592 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827734688 -> 140106827734592
	140106827734688 -> 140106832802848 [dir=none]
	140106832802848 [label="other
 ()" fillcolor=orange]
	140106827734688 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827734784 -> 140106827734688
	140106827734784 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827734880 -> 140106827734784
	140106827734880 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827734976 -> 140106827734880
	140106827734976 -> 140106866476032 [dir=none]
	140106866476032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827734976 -> 140107065180384 [dir=none]
	140107065180384 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827734976 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804670240 -> 140106827734976
	140106827735072 -> 140106827734976
	140107065180384 [label="detail_enc.encoder.attn_layers.5.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107065180384 -> 140106827735072
	140106827735072 [label=AccumulateGrad]
	140106827735024 -> 140106827734976
	140107065180464 [label="detail_enc.encoder.attn_layers.5.conv_q.bias
 (192)" fillcolor=lightblue]
	140107065180464 -> 140106827735024
	140106827735024 [label=AccumulateGrad]
	140106827734400 -> 140106827734352
	140106827734400 [label="ViewBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106827734736 -> 140106827734400
	140106827734736 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 286)"]
	140106827734928 -> 140106827734736
	140106827734928 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106827735168 -> 140106827734928
	140106827735168 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827735216 -> 140106827735168
	140106827735216 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827735360 -> 140106827735216
	140106827735360 -> 140106866476032 [dir=none]
	140106866476032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827735360 -> 140107065180544 [dir=none]
	140107065180544 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827735360 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804670240 -> 140106827735360
	140106827735456 -> 140106827735360
	140107065180544 [label="detail_enc.encoder.attn_layers.5.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107065180544 -> 140106827735456
	140106827735456 [label=AccumulateGrad]
	140106827735408 -> 140106827735360
	140107065180784 [label="detail_enc.encoder.attn_layers.5.conv_k.bias
 (192)" fillcolor=lightblue]
	140107065180784 -> 140106827735408
	140106827735408 [label=AccumulateGrad]
	140106827734160 -> 140106827734112
	140106827734160 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 571)
start         :                 285
step          :                   1"]
	140106827734640 -> 140106827734160
	140106827734640 [label="SliceBackward0
--------------------------------
dim           :                2
end           :              286
self_sym_sizes: (7, 2, 287, 571)
start         :                0
step          :                1"]
	140106827735120 -> 140106827734640
	140106827735120 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106827735312 -> 140106827735120
	140106827735312 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 287, 571)
start         :                   0
step          :                   1"]
	140106827734544 -> 140106827735312
	140106827734544 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163877)"]
	140106827735552 -> 140106827734544
	140106827735552 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 285, 0, 0, 0, 0)"]
	140106827735648 -> 140106827735552
	140106827735648 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 572)"]
	140106827735744 -> 140106827735648
	140106827735744 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106827735840 -> 140106827735744
	140106827735840 [label="UnsafeViewBackward0
------------------------------
self_sym_sizes: (14, 286, 571)"]
	140106827735936 -> 140106827735840
	140106827735936 -> 140106832803168 [dir=none]
	140106832803168 [label="mat2
 (14, 96, 571)" fillcolor=orange]
	140106827735936 -> 140106832802528 [dir=none]
	140106832802528 [label="self
 (14, 286, 96)" fillcolor=orange]
	140106827735936 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827736272 -> 140106827735936
	140106827736272 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827736416 -> 140106827736272
	140106827736416 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827736512 -> 140106827736416
	140106827736512 -> 140106832802288 [dir=none]
	140106832802288 [label="other
 ()" fillcolor=orange]
	140106827736512 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827734784 -> 140106827736512
	140106827736224 -> 140106827735936
	140106827736224 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 96, 571)"]
	140106827736608 -> 140106827736224
	140106827736608 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 96, 571)"]
	140106827736320 -> 140106827736608
	140106827736320 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106827736752 -> 140106827736320
	140106827736752 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106827736800 -> 140106827736752
	140106827736800 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106827736944 -> 140106827736800
	140106827736944 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106827737040 -> 140106827736944
	140107071075600 [label="detail_enc.encoder.attn_layers.5.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107071075600 -> 140106827737040
	140106827737040 [label=AccumulateGrad]
	140106827733536 -> 140106827732144
	140106827733536 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827733872 -> 140106827733536
	140106827733872 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (7, 2, 286, 96)"]
	140106827734064 -> 140106827733872
	140106827734064 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106827734256 -> 140106827734064
	140106827734256 [label="ViewBackward0
-----------------------------
self_sym_sizes: (7, 192, 286)"]
	140106827734496 -> 140106827734256
	140106827734496 -> 140106866476032 [dir=none]
	140106866476032 [label="input
 (7, 192, 286)" fillcolor=orange]
	140106827734496 -> 140107065181264 [dir=none]
	140107065181264 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827734496 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804670240 -> 140106827734496
	140106827735504 -> 140106827734496
	140107065181264 [label="detail_enc.encoder.attn_layers.5.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107065181264 -> 140106827735504
	140106827735504 [label=AccumulateGrad]
	140106827735264 -> 140106827734496
	140107065181344 [label="detail_enc.encoder.attn_layers.5.conv_v.bias
 (192)" fillcolor=lightblue]
	140107065181344 -> 140106827735264
	140106827735264 [label=AccumulateGrad]
	140106827733344 -> 140106827732432
	140106827733344 [label="UnsafeViewBackward0
-----------------------------
self_sym_sizes: (14, 286, 96)"]
	140106827733776 -> 140106827733344
	140106827733776 -> 140106832803248 [dir=none]
	140106832803248 [label="mat2
 (14, 571, 96)" fillcolor=orange]
	140106827733776 -> 140106832803648 [dir=none]
	140106832803648 [label="self
 (14, 286, 571)" fillcolor=orange]
	140106827733776 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106827733632 -> 140106827733776
	140106827733632 [label="ReshapeAliasBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827735600 -> 140106827733632
	140106827735600 [label="ExpandBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827735792 -> 140106827735600
	140106827735792 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   1
step          :                   1"]
	140106827734304 -> 140106827735792
	140106827734304 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827736560 -> 140106827734304
	140106827736560 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827736656 -> 140106827736560
	140106827736656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:    (7, 2, 286, 572)
start         :                   0
step          :                   1"]
	140106827736848 -> 140106827736656
	140106827736848 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 163592)"]
	140106827737136 -> 140106827736848
	140106827737136 [label="ConstantPadNdBackward0
-------------------------
pad: (286, 0, 0, 0, 0, 0)"]
	140106827736368 -> 140106827737136
	140106827736368 [label="ViewBackward0
--------------------------------
self_sym_sizes: (7, 2, 286, 571)"]
	140106827737232 -> 140106827736368
	140106827737232 [label="ConstantPadNdBackward0
-------------------------------
pad: (0, 285, 0, 0, 0, 0, 0, 0)"]
	140106827733824 -> 140106827737232
	140106827733968 -> 140106827733776
	140106827733968 [label="ReshapeAliasBackward0
-------------------------------
self_sym_sizes: (7, 2, 571, 96)"]
	140106827735888 -> 140106827733968
	140106827735888 [label="ExpandBackward0
-------------------------------
self_sym_sizes: (1, 1, 571, 96)"]
	140106827736704 -> 140106827735888
	140106827736704 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106827736992 -> 140106827736704
	140106827736992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:        (1, 571, 96)
start         :                   0
step          :                   1"]
	140106827737184 -> 140106827736992
	140106827737184 [label="ConstantPadNdBackward0
---------------------------
pad: (0, 0, 281, 281, 0, 0)"]
	140106827737280 -> 140106827737184
	140107065181744 [label="detail_enc.encoder.attn_layers.5.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107065181744 -> 140106827737280
	140106827737280 [label=AccumulateGrad]
	140106804670912 -> 140106804671296
	140107065181504 [label="detail_enc.encoder.attn_layers.5.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107065181504 -> 140106804670912
	140106804670912 [label=AccumulateGrad]
	140106804670576 -> 140106804671296
	140107065181664 [label="detail_enc.encoder.attn_layers.5.conv_o.bias
 (192)" fillcolor=lightblue]
	140107065181664 -> 140106804670576
	140106804670576 [label=AccumulateGrad]
	140106804669616 -> 140106804669472
	140107065181904 [label="detail_enc.encoder.norm_layers_1.5.gamma
 (192)" fillcolor=lightblue]
	140107065181904 -> 140106804669616
	140106804669616 [label=AccumulateGrad]
	140106804663616 -> 140106804669472
	140107065182064 [label="detail_enc.encoder.norm_layers_1.5.beta
 (192)" fillcolor=lightblue]
	140107065182064 -> 140106804663616
	140106804663616 [label=AccumulateGrad]
	140106804665248 -> 140106832144816
	140106804665248 -> 140106832803088 [dir=none]
	140106832803088 [label="result1
 (7, 192, 286)" fillcolor=orange]
	140106804665248 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106804669712 -> 140106804665248
	140106804669712 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804669712 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804669952 -> 140106804669712
	140106804669952 -> 140106854222000 [dir=none]
	140106854222000 [label="input
 (7, 768, 288)" fillcolor=orange]
	140106804669952 -> 140107065182384 [dir=none]
	140107065182384 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106804669952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804670672 -> 140106804669952
	140106804670672 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106804671632 -> 140106804670672
	140106804671632 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106804671632 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827733488 -> 140106804671632
	140106827733488 -> 140106832803808 [dir=none]
	140106832803808 [label="result1
 (7, 768, 286)" fillcolor=orange]
	140106827733488 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827735696 -> 140106827733488
	140106827735696 -> 140106832803728 [dir=none]
	140106832803728 [label="result
 (7, 768, 286)" fillcolor=orange]
	140106827735696 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106827736896 -> 140106827735696
	140106827736896 -> 140106854230640 [dir=none]
	140106854230640 [label="input
 (7, 192, 288)" fillcolor=orange]
	140106827736896 -> 140107065182224 [dir=none]
	140107065182224 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106827736896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827737328 -> 140106827736896
	140106827737328 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106827737424 -> 140106827737328
	140106827737424 -> 140107019754432 [dir=none]
	140107019754432 [label="other
 (7, 1, 286)" fillcolor=orange]
	140106827737424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106832139200 -> 140106827737424
	140106827737088 -> 140106827736896
	140107065182224 [label="detail_enc.encoder.ffn_layers.5.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107065182224 -> 140106827737088
	140106827737088 [label=AccumulateGrad]
	140106827729648 -> 140106827736896
	140107065182304 [label="detail_enc.encoder.ffn_layers.5.conv_1.bias
 (768)" fillcolor=lightblue]
	140107065182304 -> 140106827729648
	140106827729648 [label=AccumulateGrad]
	140106804669808 -> 140106804669952
	140107065182384 [label="detail_enc.encoder.ffn_layers.5.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107065182384 -> 140106804669808
	140106804669808 [label=AccumulateGrad]
	140106804665392 -> 140106804669952
	140107065183424 [label="detail_enc.encoder.ffn_layers.5.conv_2.bias
 (192)" fillcolor=lightblue]
	140107065183424 -> 140106804665392
	140106804665392 [label=AccumulateGrad]
	140106807863392 -> 140106809804016
	140107065183504 [label="detail_enc.encoder.norm_layers_2.5.gamma
 (192)" fillcolor=lightblue]
	140107065183504 -> 140106807863392
	140106807863392 [label=AccumulateGrad]
	140106807860224 -> 140106809804016
	140107065183744 [label="detail_enc.encoder.norm_layers_2.5.beta
 (192)" fillcolor=lightblue]
	140107065183744 -> 140106807860224
	140106807860224 [label=AccumulateGrad]
	140106804668992 -> 140106804668944
	140107065183904 [label="detail_enc.out_proj.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107065183904 -> 140106804668992
	140106804668992 [label=AccumulateGrad]
	140106804667696 -> 140106804668944
	140107065184064 [label="detail_enc.out_proj.bias
 (192)" fillcolor=lightblue]
	140107065184064 -> 140106804667696
	140106804667696 [label=AccumulateGrad]
	140106869104624 -> 140106869108608
	140106869104624 -> 140107046558016 [dir=none]
	140107046558016 [label="other
 ()" fillcolor=orange]
	140106869104624 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106869107648 -> 140106869104624
	140106869107648 [label="SumBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106809804064 -> 140106869107648
	140106809804064 -> 140107099112928 [dir=none]
	140107099112928 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106809804064 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106807860272 -> 140106809804064
	140106807860272 -> 140107046552976 [dir=none]
	140107046552976 [label="self
 (7, 192, 28)" fillcolor=orange]
	140106807860272 [label="PowBackward0
------------------------
exponent:              2
self    : [saved tensor]"]
	140106832142032 -> 140106807860272
	140106832142032 [label="SubBackward0
------------
alpha: 1"]
	140106804670768 -> 140106832142032
	140106832137760 -> 140106832142032
	140106832137760 -> 140107031831040 [dir=none]
	140107031831040 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106832137760 -> 140107003791056 [dir=none]
	140107003791056 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106832137760 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106804668848 -> 140106832137760
	140106804668848 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106804668848 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106804669904 -> 140106804668848
	140106804669904 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106804671920 -> 140106804669904
	140106804671920 -> 140107003790976 [dir=none]
	140107003790976 [label="bias
 (192)" fillcolor=orange]
	140106804671920 -> 140107031834080 [dir=none]
	140107031834080 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106804671920 -> 140106832803968 [dir=none]
	140106832803968 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106804671920 -> 140106832803408 [dir=none]
	140106832803408 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106804671920 -> 140107003790656 [dir=none]
	140107003790656 [label="weight
 (192)" fillcolor=orange]
	140106804671920 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827733440 -> 140106804671920
	140106827733440 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827736176 -> 140106827733440
	140106827736176 [label="AddBackward0
------------
alpha: 1"]
	140106827737568 -> 140106827736176
	140106827737568 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827737808 -> 140106827737568
	140106827737808 -> 140107011980592 [dir=none]
	140107011980592 [label="bias
 (192)" fillcolor=orange]
	140106827737808 -> 140107031825600 [dir=none]
	140107031825600 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827737808 -> 140106832804048 [dir=none]
	140106832804048 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827737808 -> 140106832803328 [dir=none]
	140106832803328 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827737808 -> 140107011980112 [dir=none]
	140107011980112 [label="weight
 (192)" fillcolor=orange]
	140106827737808 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827737856 -> 140106827737808
	140106827737856 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827738144 -> 140106827737856
	140106827738144 [label="AddBackward0
------------
alpha: 1"]
	140106827738192 -> 140106827738144
	140106827738192 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827738432 -> 140106827738192
	140106827738432 -> 140107011977392 [dir=none]
	140107011977392 [label="bias
 (192)" fillcolor=orange]
	140106827738432 -> 140107040387088 [dir=none]
	140107040387088 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827738432 -> 140106832803568 [dir=none]
	140106832803568 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827738432 -> 140106832804208 [dir=none]
	140106832804208 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827738432 -> 140107011977312 [dir=none]
	140107011977312 [label="weight
 (192)" fillcolor=orange]
	140106827738432 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827738480 -> 140106827738432
	140106827738480 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827738768 -> 140106827738480
	140106827738768 [label="AddBackward0
------------
alpha: 1"]
	140106827738816 -> 140106827738768
	140106827738816 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827739056 -> 140106827738816
	140106827739056 -> 140107011976192 [dir=none]
	140107011976192 [label="bias
 (192)" fillcolor=orange]
	140106827739056 -> 140107040389088 [dir=none]
	140107040389088 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827739056 -> 140106832804128 [dir=none]
	140106832804128 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827739056 -> 140106832804368 [dir=none]
	140106832804368 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827739056 -> 140107011974512 [dir=none]
	140107011974512 [label="weight
 (192)" fillcolor=orange]
	140106827739056 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827739104 -> 140106827739056
	140106827739104 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827739392 -> 140106827739104
	140106827739392 [label="AddBackward0
------------
alpha: 1"]
	140106827739440 -> 140106827739392
	140106827739440 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827739680 -> 140106827739440
	140106827739680 -> 140107011972512 [dir=none]
	140107011972512 [label="bias
 (192)" fillcolor=orange]
	140106827739680 -> 140107040381488 [dir=none]
	140107040381488 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827739680 -> 140106832804288 [dir=none]
	140106832804288 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827739680 -> 140106832804528 [dir=none]
	140106832804528 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827739680 -> 140107011972352 [dir=none]
	140107011972352 [label="weight
 (192)" fillcolor=orange]
	140106827739680 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827739728 -> 140106827739680
	140106827739728 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827740016 -> 140106827739728
	140106827740016 [label="AddBackward0
------------
alpha: 1"]
	140106827740064 -> 140106827740016
	140106827740064 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827740304 -> 140106827740064
	140106827740304 -> 140107011971072 [dir=none]
	140107011971072 [label="bias
 (192)" fillcolor=orange]
	140106827740304 -> 140107040385968 [dir=none]
	140107040385968 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827740304 -> 140106832804448 [dir=none]
	140106832804448 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827740304 -> 140106832804688 [dir=none]
	140106832804688 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827740304 -> 140107011970912 [dir=none]
	140107011970912 [label="weight
 (192)" fillcolor=orange]
	140106827740304 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827740352 -> 140106827740304
	140106827740352 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827740640 -> 140106827740352
	140106827740640 [label="AddBackward0
------------
alpha: 1"]
	140106827740688 -> 140106827740640
	140106827740688 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827740928 -> 140106827740688
	140106827740928 -> 140107011968112 [dir=none]
	140107011968112 [label="bias
 (192)" fillcolor=orange]
	140106827740928 -> 140107040376848 [dir=none]
	140107040376848 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827740928 -> 140106832804608 [dir=none]
	140106832804608 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827740928 -> 140106832804848 [dir=none]
	140106832804848 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827740928 -> 140107011968032 [dir=none]
	140107011968032 [label="weight
 (192)" fillcolor=orange]
	140106827740928 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827740976 -> 140106827740928
	140106827740976 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827741264 -> 140106827740976
	140106827741264 [label="AddBackward0
------------
alpha: 1"]
	140106827741312 -> 140106827741264
	140106827741312 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827741552 -> 140106827741312
	140106827741552 -> 140107011966592 [dir=none]
	140107011966592 [label="bias
 (192)" fillcolor=orange]
	140106827741552 -> 140107040380448 [dir=none]
	140107040380448 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827741552 -> 140106832802688 [dir=none]
	140106832802688 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827741552 -> 140106832805008 [dir=none]
	140106832805008 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827741552 -> 140107011966432 [dir=none]
	140107011966432 [label="weight
 (192)" fillcolor=orange]
	140106827741552 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827741600 -> 140106827741552
	140106827741600 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827741888 -> 140106827741600
	140106827741888 [label="AddBackward0
------------
alpha: 1"]
	140106827741936 -> 140106827741888
	140106827741936 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827742176 -> 140106827741936
	140106827742176 -> 140107019762032 [dir=none]
	140107019762032 [label="bias
 (192)" fillcolor=orange]
	140106827742176 -> 140107040388928 [dir=none]
	140107040388928 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827742176 -> 140106832804768 [dir=none]
	140106832804768 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827742176 -> 140106832805248 [dir=none]
	140106832805248 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827742176 -> 140107019761632 [dir=none]
	140107019761632 [label="weight
 (192)" fillcolor=orange]
	140106827742176 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827742224 -> 140106827742176
	140106827742224 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827742512 -> 140106827742224
	140106827742512 [label="AddBackward0
------------
alpha: 1"]
	140106827742560 -> 140106827742512
	140106827742560 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827742800 -> 140106827742560
	140106827742800 -> 140107019759792 [dir=none]
	140107019759792 [label="bias
 (192)" fillcolor=orange]
	140106827742800 -> 140107040376688 [dir=none]
	140107040376688 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106827742800 -> 140106832805168 [dir=none]
	140106832805168 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106827742800 -> 140106832805408 [dir=none]
	140106832805408 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106827742800 -> 140107019759552 [dir=none]
	140107019759552 [label="weight
 (192)" fillcolor=orange]
	140106827742800 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106827742848 -> 140106827742800
	140106827742848 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106827743136 -> 140106827742848
	140106827743136 [label="AddBackward0
------------
alpha: 1"]
	140106827743184 -> 140106827743136
	140106827743184 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106825597184 -> 140106827743184
	140106825597184 -> 140107019757872 [dir=none]
	140107019757872 [label="bias
 (192)" fillcolor=orange]
	140106825597184 -> 140107046559616 [dir=none]
	140107046559616 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106825597184 -> 140106832805328 [dir=none]
	140106832805328 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106825597184 -> 140106832805568 [dir=none]
	140106832805568 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106825597184 -> 140107019757792 [dir=none]
	140107019757792 [label="weight
 (192)" fillcolor=orange]
	140106825597184 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106825597232 -> 140106825597184
	140106825597232 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106825597520 -> 140106825597232
	140106825597520 [label="AddBackward0
------------
alpha: 1"]
	140106825597568 -> 140106825597520
	140106825597568 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106825597808 -> 140106825597568
	140106825597808 -> 140107019756512 [dir=none]
	140107019756512 [label="bias
 (192)" fillcolor=orange]
	140106825597808 -> 140107046561376 [dir=none]
	140107046561376 [label="input
 (7, 28, 192)" fillcolor=orange]
	140106825597808 -> 140106832805488 [dir=none]
	140106832805488 [label="result1
 (7, 28, 1)" fillcolor=orange]
	140106825597808 -> 140106832805728 [dir=none]
	140106832805728 [label="result2
 (7, 28, 1)" fillcolor=orange]
	140106825597808 -> 140107019756352 [dir=none]
	140107019756352 [label="weight
 (192)" fillcolor=orange]
	140106825597808 [label="NativeLayerNormBackward0
--------------------------------
bias            : [saved tensor]
input           : [saved tensor]
normalized_shape:         (192,)
result1         : [saved tensor]
result2         : [saved tensor]
weight          : [saved tensor]"]
	140106825597856 -> 140106825597808
	140106825597856 [label="TransposeBackward0
--------------------------
dim0:                    1
dim1: 18446744073709551615"]
	140106825598144 -> 140106825597856
	140106825598144 [label="AddBackward0
------------
alpha: 1"]
	140106825598192 -> 140106825598144
	140106825598192 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825598192 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825598624 -> 140106825598192
	140106825598624 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825598624 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825598720 -> 140106825598624
	140106825598720 [label="AddBackward0
------------
alpha: 1"]
	140106825598816 -> 140106825598720
	140106825598816 [label="UnsqueezeBackward0
-------------------------
dim: 18446744073709551615"]
	140106825598960 -> 140106825598816
	140106825598960 -> 140107058265072 [dir=none]
	140107058265072 [label="mat1
 (7, 512)" fillcolor=orange]
	140106825598960 -> 140106832805808 [dir=none]
	140106832805808 [label="mat2
 (512, 192)" fillcolor=orange]
	140106825598960 [label="AddmmBackward0
--------------------------------
alpha           :              1
beta            :              1
mat1            : [saved tensor]
mat1_sym_sizes  :       (7, 512)
mat1_sym_strides:       (512, 1)
mat2            : [saved tensor]
mat2_sym_sizes  :     (512, 192)
mat2_sym_strides:       (1, 512)"]
	140106825599008 -> 140106825598960
	140107003791856 [label="dur_detail_enc.ge_proj.bias
 (192)" fillcolor=lightblue]
	140107003791856 -> 140106825599008
	140106825599008 [label=AccumulateGrad]
	140106825599056 -> 140106825598960
	140106825599056 [label="SqueezeBackward1
------------------------------------
dim           : 18446744073709551615
self_sym_sizes:          (7, 512, 1)"]
	140107604685488 -> 140106825599056
	140106825598288 -> 140106825598960
	140106825598288 [label=TBackward0]
	140106825599248 -> 140106825598288
	140107003791616 [label="dur_detail_enc.ge_proj.weight
 (192, 512)" fillcolor=lightblue]
	140107003791616 -> 140106825599248
	140106825599248 [label=AccumulateGrad]
	140106825598240 -> 140106825598144
	140106825598240 -> 140106832805648 [dir=none]
	140106832805648 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106825598240 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825598768 -> 140106825598240
	140106825598768 -> 140107046561616 [dir=none]
	140107046561616 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825598768 -> 140107019755792 [dir=none]
	140107019755792 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825598768 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825598912 -> 140106825598768
	140106825598912 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825599296 -> 140106825598912
	140106825599296 [label=CloneBackward0]
	140106825599392 -> 140106825599296
	140106825599392 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825599488 -> 140106825599392
	140106825599488 [label="AddBackward0
------------
alpha: 1"]
	140106825599584 -> 140106825599488
	140106825599584 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825599728 -> 140106825599584
	140106825599728 -> 140106821861760 [dir=none]
	140106821861760 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106825599728 -> 140106821861520 [dir=none]
	140106821861520 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106825599728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825599824 -> 140106825599728
	140106825599824 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825599968 -> 140106825599824
	140106825599968 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825600064 -> 140106825599968
	140106825600064 -> 140106821861440 [dir=none]
	140106821861440 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106825600064 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825600160 -> 140106825600064
	140106825600160 -> 140106821861920 [dir=none]
	140106821861920 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106825600160 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106825600256 -> 140106825600160
	140106825600256 -> 140107046559296 [dir=none]
	140107046559296 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106825600256 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106825600352 -> 140106825600256
	140106825600352 [label="AddBackward0
------------
alpha: 1"]
	140106825600448 -> 140106825600352
	140106825600448 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106825600592 -> 140106825600448
	140106825600592 -> 140106821861840 [dir=none]
	140106821861840 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106825600592 -> 140106821862000 [dir=none]
	140106821862000 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825600592 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825600688 -> 140106825600592
	140106825600688 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825600832 -> 140106825600688
	140106825600832 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825600928 -> 140106825600832
	140106825600928 -> 140106821862080 [dir=none]
	140106821862080 [label="other
 ()" fillcolor=orange]
	140106825600928 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825601024 -> 140106825600928
	140106825601024 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825601120 -> 140106825601024
	140106825601120 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825601216 -> 140106825601120
	140106825601216 -> 140107046558416 [dir=none]
	140107046558416 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825601216 -> 140107019754032 [dir=none]
	140107019754032 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825601216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825598192 -> 140106825601216
	140106825601312 -> 140106825601216
	140107019754032 [label="dur_detail_enc.encoder.attn_layers.0.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019754032 -> 140106825601312
	140106825601312 [label=AccumulateGrad]
	140106825601264 -> 140106825601216
	140107019754192 [label="dur_detail_enc.encoder.attn_layers.0.conv_q.bias
 (192)" fillcolor=lightblue]
	140107019754192 -> 140106825601264
	140106825601264 [label=AccumulateGrad]
	140106825600640 -> 140106825600592
	140106825600640 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825600976 -> 140106825600640
	140106825600976 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825601168 -> 140106825600976
	140106825601168 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825601408 -> 140106825601168
	140106825601408 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825601456 -> 140106825601408
	140106825601456 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825601600 -> 140106825601456
	140106825601600 -> 140107046558416 [dir=none]
	140107046558416 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825601600 -> 140107019754912 [dir=none]
	140107019754912 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825601600 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825598192 -> 140106825601600
	140106825601696 -> 140106825601600
	140107019754912 [label="dur_detail_enc.encoder.attn_layers.0.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019754912 -> 140106825601696
	140106825601696 [label=AccumulateGrad]
	140106825601648 -> 140106825601600
	140107019755312 [label="dur_detail_enc.encoder.attn_layers.0.conv_k.bias
 (192)" fillcolor=lightblue]
	140107019755312 -> 140106825601648
	140106825601648 [label=AccumulateGrad]
	140106825600400 -> 140106825600352
	140106825600400 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106825600880 -> 140106825600400
	140106825600880 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106825601360 -> 140106825600880
	140106825601360 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825601552 -> 140106825601360
	140106825601552 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825600784 -> 140106825601552
	140106825600784 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106825601792 -> 140106825600784
	140106825601792 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106825601888 -> 140106825601792
	140106825601888 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106825601984 -> 140106825601888
	140106825601984 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106825602080 -> 140106825601984
	140106825602080 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106825602176 -> 140106825602080
	140106825602176 -> 140106821862400 [dir=none]
	140106821862400 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106825602176 -> 140106821861600 [dir=none]
	140106821861600 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825602176 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825602272 -> 140106825602176
	140106825602272 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825602416 -> 140106825602272
	140106825602416 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825602512 -> 140106825602416
	140106825602512 -> 140106821862240 [dir=none]
	140106821862240 [label="other
 ()" fillcolor=orange]
	140106825602512 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825601024 -> 140106825602512
	140106825602224 -> 140106825602176
	140106825602224 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106825602608 -> 140106825602224
	140106825602608 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106825602320 -> 140106825602608
	140106825602320 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825602752 -> 140106825602320
	140106825602752 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106825602800 -> 140106825602752
	140106825602800 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106825602944 -> 140106825602800
	140106825602944 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106825603040 -> 140106825602944
	140107019754112 [label="dur_detail_enc.encoder.attn_layers.0.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107019754112 -> 140106825603040
	140106825603040 [label=AccumulateGrad]
	140106825599776 -> 140106825599728
	140106825599776 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825600112 -> 140106825599776
	140106825600112 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825600304 -> 140106825600112
	140106825600304 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825600496 -> 140106825600304
	140106825600496 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825600736 -> 140106825600496
	140106825600736 -> 140107046558416 [dir=none]
	140107046558416 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825600736 -> 140107019755552 [dir=none]
	140107019755552 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825600736 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825598192 -> 140106825600736
	140106825601744 -> 140106825600736
	140107019755552 [label="dur_detail_enc.encoder.attn_layers.0.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019755552 -> 140106825601744
	140106825601744 [label=AccumulateGrad]
	140106825601504 -> 140106825600736
	140107019755712 [label="dur_detail_enc.encoder.attn_layers.0.conv_v.bias
 (192)" fillcolor=lightblue]
	140107019755712 -> 140106825601504
	140106825601504 [label=AccumulateGrad]
	140106825599536 -> 140106825599488
	140106825599536 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825600016 -> 140106825599536
	140106825600016 -> 140106832805088 [dir=none]
	140106832805088 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106825600016 -> 140106821862320 [dir=none]
	140106821862320 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106825600016 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825599872 -> 140106825600016
	140106825599872 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825601840 -> 140106825599872
	140106825601840 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825602032 -> 140106825601840
	140106825602032 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106825598528 -> 140106825602032
	140106825598528 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825602560 -> 140106825598528
	140106825602560 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825600544 -> 140106825602560
	140106825600544 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825602848 -> 140106825600544
	140106825602848 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106825603136 -> 140106825602848
	140106825603136 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106825602368 -> 140106825603136
	140106825602368 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825603232 -> 140106825602368
	140106825603232 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106825600064 -> 140106825603232
	140106825600208 -> 140106825600016
	140106825600208 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106825602128 -> 140106825600208
	140106825602128 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106825602704 -> 140106825602128
	140106825602704 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106825602992 -> 140106825602704
	140106825602992 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106825603184 -> 140106825602992
	140106825603184 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106825603280 -> 140106825603184
	140107019756192 [label="dur_detail_enc.encoder.attn_layers.0.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107019756192 -> 140106825603280
	140106825603280 [label=AccumulateGrad]
	140106825598864 -> 140106825598768
	140107019755792 [label="dur_detail_enc.encoder.attn_layers.0.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019755792 -> 140106825598864
	140106825598864 [label=AccumulateGrad]
	140106825598576 -> 140106825598768
	140107019756032 [label="dur_detail_enc.encoder.attn_layers.0.conv_o.bias
 (192)" fillcolor=lightblue]
	140107019756032 -> 140106825598576
	140106825598576 [label=AccumulateGrad]
	140106825597904 -> 140106825597808
	140107019756352 [label="dur_detail_enc.encoder.norm_layers_1.0.gamma
 (192)" fillcolor=lightblue]
	140107019756352 -> 140106825597904
	140106825597904 [label=AccumulateGrad]
	140106825597664 -> 140106825597808
	140107019756512 [label="dur_detail_enc.encoder.norm_layers_1.0.beta
 (192)" fillcolor=lightblue]
	140107019756512 -> 140106825597664
	140106825597664 [label=AccumulateGrad]
	140106825597616 -> 140106825597520
	140106825597616 -> 140106821861680 [dir=none]
	140106821861680 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106825597616 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825597952 -> 140106825597616
	140106825597952 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825597952 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825598096 -> 140106825597952
	140106825598096 -> 140107046559696 [dir=none]
	140107046559696 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106825598096 -> 140107019757552 [dir=none]
	140107019757552 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106825598096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825598672 -> 140106825598096
	140106825598672 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106825599440 -> 140106825598672
	140106825599440 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825599440 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825599632 -> 140106825599440
	140106825599632 -> 140106821862560 [dir=none]
	140106821862560 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106825599632 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825601936 -> 140106825599632
	140106825601936 -> 140106821862480 [dir=none]
	140106821862480 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106825601936 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106825602896 -> 140106825601936
	140106825602896 -> 140107046560176 [dir=none]
	140107046560176 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106825602896 -> 140107019757232 [dir=none]
	140107019757232 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106825602896 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825603328 -> 140106825602896
	140106825603328 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106825603424 -> 140106825603328
	140106825603424 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825603424 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825597568 -> 140106825603424
	140106825603088 -> 140106825602896
	140107019757232 [label="dur_detail_enc.encoder.ffn_layers.0.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107019757232 -> 140106825603088
	140106825603088 [label=AccumulateGrad]
	140106825599344 -> 140106825602896
	140107019757472 [label="dur_detail_enc.encoder.ffn_layers.0.conv_1.bias
 (768)" fillcolor=lightblue]
	140107019757472 -> 140106825599344
	140106825599344 [label=AccumulateGrad]
	140106825598000 -> 140106825598096
	140107019757552 [label="dur_detail_enc.encoder.ffn_layers.0.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107019757552 -> 140106825598000
	140106825598000 [label=AccumulateGrad]
	140106825597712 -> 140106825598096
	140107019757632 [label="dur_detail_enc.encoder.ffn_layers.0.conv_2.bias
 (192)" fillcolor=lightblue]
	140107019757632 -> 140106825597712
	140106825597712 [label=AccumulateGrad]
	140106825597280 -> 140106825597184
	140107019757792 [label="dur_detail_enc.encoder.norm_layers_2.0.gamma
 (192)" fillcolor=lightblue]
	140107019757792 -> 140106825597280
	140106825597280 [label=AccumulateGrad]
	140106825597040 -> 140106825597184
	140107019757872 [label="dur_detail_enc.encoder.norm_layers_2.0.beta
 (192)" fillcolor=lightblue]
	140107019757872 -> 140106825597040
	140106825597040 [label=AccumulateGrad]
	140106827742992 -> 140106827743136
	140106827742992 -> 140106821862880 [dir=none]
	140106821862880 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827742992 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825597328 -> 140106827742992
	140106825597328 -> 140107040380688 [dir=none]
	140107040380688 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825597328 -> 140107019758992 [dir=none]
	140107019758992 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825597328 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825597472 -> 140106825597328
	140106825597472 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825599104 -> 140106825597472
	140106825599104 [label=CloneBackward0]
	140106825599200 -> 140106825599104
	140106825599200 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825602464 -> 140106825599200
	140106825602464 [label="AddBackward0
------------
alpha: 1"]
	140106825602656 -> 140106825602464
	140106825602656 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825601072 -> 140106825602656
	140106825601072 -> 140106821862960 [dir=none]
	140106821862960 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106825601072 -> 140106821862720 [dir=none]
	140106821862720 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106825601072 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825603616 -> 140106825601072
	140106825603616 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825603760 -> 140106825603616
	140106825603760 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825603856 -> 140106825603760
	140106825603856 -> 140106821863120 [dir=none]
	140106821863120 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106825603856 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825603952 -> 140106825603856
	140106825603952 -> 140106821863200 [dir=none]
	140106821863200 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106825603952 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106825604048 -> 140106825603952
	140106825604048 -> 140107040391088 [dir=none]
	140107040391088 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106825604048 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106825604144 -> 140106825604048
	140106825604144 [label="AddBackward0
------------
alpha: 1"]
	140106825604240 -> 140106825604144
	140106825604240 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106825604384 -> 140106825604240
	140106825604384 -> 140106821862640 [dir=none]
	140106821862640 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106825604384 -> 140106821863280 [dir=none]
	140106821863280 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825604384 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825604480 -> 140106825604384
	140106825604480 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825604624 -> 140106825604480
	140106825604624 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825604720 -> 140106825604624
	140106825604720 -> 140106821863360 [dir=none]
	140106821863360 [label="other
 ()" fillcolor=orange]
	140106825604720 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825604816 -> 140106825604720
	140106825604816 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825604912 -> 140106825604816
	140106825604912 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825605008 -> 140106825604912
	140106825605008 -> 140107046565776 [dir=none]
	140107046565776 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825605008 -> 140107019757952 [dir=none]
	140107019757952 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825605008 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827743184 -> 140106825605008
	140106825605104 -> 140106825605008
	140107019757952 [label="dur_detail_enc.encoder.attn_layers.1.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019757952 -> 140106825605104
	140106825605104 [label=AccumulateGrad]
	140106825605056 -> 140106825605008
	140107019758112 [label="dur_detail_enc.encoder.attn_layers.1.conv_q.bias
 (192)" fillcolor=lightblue]
	140107019758112 -> 140106825605056
	140106825605056 [label=AccumulateGrad]
	140106825604432 -> 140106825604384
	140106825604432 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825604768 -> 140106825604432
	140106825604768 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825604960 -> 140106825604768
	140106825604960 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825605200 -> 140106825604960
	140106825605200 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825605248 -> 140106825605200
	140106825605248 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825605392 -> 140106825605248
	140106825605392 -> 140107046565776 [dir=none]
	140107046565776 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825605392 -> 140107019758672 [dir=none]
	140107019758672 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825605392 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827743184 -> 140106825605392
	140106825605488 -> 140106825605392
	140107019758672 [label="dur_detail_enc.encoder.attn_layers.1.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019758672 -> 140106825605488
	140106825605488 [label=AccumulateGrad]
	140106825605440 -> 140106825605392
	140107019758512 [label="dur_detail_enc.encoder.attn_layers.1.conv_k.bias
 (192)" fillcolor=lightblue]
	140107019758512 -> 140106825605440
	140106825605440 [label=AccumulateGrad]
	140106825604192 -> 140106825604144
	140106825604192 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106825604672 -> 140106825604192
	140106825604672 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106825605152 -> 140106825604672
	140106825605152 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825605344 -> 140106825605152
	140106825605344 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825604576 -> 140106825605344
	140106825604576 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106825605584 -> 140106825604576
	140106825605584 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106825605680 -> 140106825605584
	140106825605680 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106825605776 -> 140106825605680
	140106825605776 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106825605872 -> 140106825605776
	140106825605872 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106825606208 -> 140106825605872
	140106825606208 -> 140106821863920 [dir=none]
	140106821863920 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106825606208 -> 140106821862800 [dir=none]
	140106821862800 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825606208 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825606304 -> 140106825606208
	140106825606304 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825606448 -> 140106825606304
	140106825606448 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825606544 -> 140106825606448
	140106825606544 -> 140106821864000 [dir=none]
	140106821864000 [label="other
 ()" fillcolor=orange]
	140106825606544 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825604816 -> 140106825606544
	140106825606256 -> 140106825606208
	140106825606256 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106825606640 -> 140106825606256
	140106825606640 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106825606352 -> 140106825606640
	140106825606352 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825606784 -> 140106825606352
	140106825606784 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106825606832 -> 140106825606784
	140106825606832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106825606976 -> 140106825606832
	140106825606976 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106825607072 -> 140106825606976
	140107019756272 [label="dur_detail_enc.encoder.attn_layers.1.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107019756272 -> 140106825607072
	140106825607072 [label=AccumulateGrad]
	140106825603568 -> 140106825601072
	140106825603568 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825603904 -> 140106825603568
	140106825603904 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825604096 -> 140106825603904
	140106825604096 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825604288 -> 140106825604096
	140106825604288 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825604528 -> 140106825604288
	140106825604528 -> 140107046565776 [dir=none]
	140107046565776 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825604528 -> 140107019758352 [dir=none]
	140107019758352 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825604528 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827743184 -> 140106825604528
	140106825605536 -> 140106825604528
	140107019758352 [label="dur_detail_enc.encoder.attn_layers.1.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019758352 -> 140106825605536
	140106825605536 [label=AccumulateGrad]
	140106825605296 -> 140106825604528
	140107019758752 [label="dur_detail_enc.encoder.attn_layers.1.conv_v.bias
 (192)" fillcolor=lightblue]
	140107019758752 -> 140106825605296
	140106825605296 [label=AccumulateGrad]
	140106825603376 -> 140106825602464
	140106825603376 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825603808 -> 140106825603376
	140106825603808 -> 140106821863760 [dir=none]
	140106821863760 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106825603808 -> 140106821864320 [dir=none]
	140106821864320 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106825603808 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825603664 -> 140106825603808
	140106825603664 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825605632 -> 140106825603664
	140106825605632 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825605824 -> 140106825605632
	140106825605824 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106825604336 -> 140106825605824
	140106825604336 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825606592 -> 140106825604336
	140106825606592 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825606160 -> 140106825606592
	140106825606160 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825606880 -> 140106825606160
	140106825606880 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106825607168 -> 140106825606880
	140106825607168 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106825606400 -> 140106825607168
	140106825606400 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825607264 -> 140106825606400
	140106825607264 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106825603856 -> 140106825607264
	140106825604000 -> 140106825603808
	140106825604000 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106825605920 -> 140106825604000
	140106825605920 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106825606736 -> 140106825605920
	140106825606736 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106825607024 -> 140106825606736
	140106825607024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106825607216 -> 140106825607024
	140106825607216 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106825607312 -> 140106825607216
	140107019759392 [label="dur_detail_enc.encoder.attn_layers.1.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107019759392 -> 140106825607312
	140106825607312 [label=AccumulateGrad]
	140106825597424 -> 140106825597328
	140107019758992 [label="dur_detail_enc.encoder.attn_layers.1.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019758992 -> 140106825597424
	140106825597424 [label=AccumulateGrad]
	140106825597088 -> 140106825597328
	140107019759312 [label="dur_detail_enc.encoder.attn_layers.1.conv_o.bias
 (192)" fillcolor=lightblue]
	140107019759312 -> 140106825597088
	140106825597088 [label=AccumulateGrad]
	140106827742896 -> 140106827742800
	140107019759552 [label="dur_detail_enc.encoder.norm_layers_1.1.gamma
 (192)" fillcolor=lightblue]
	140107019759552 -> 140106827742896
	140106827742896 [label=AccumulateGrad]
	140106827742656 -> 140106827742800
	140107019759792 [label="dur_detail_enc.encoder.norm_layers_1.1.beta
 (192)" fillcolor=lightblue]
	140107019759792 -> 140106827742656
	140106827742656 [label=AccumulateGrad]
	140106827742608 -> 140106827742512
	140106827742608 -> 140106821864240 [dir=none]
	140106821864240 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827742608 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827742944 -> 140106827742608
	140106827742944 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827742944 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827743088 -> 140106827742944
	140106827743088 -> 140107040379408 [dir=none]
	140107040379408 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106827743088 -> 140107019760352 [dir=none]
	140107019760352 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106827743088 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827742704 -> 140106827743088
	140106827742704 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106825599680 -> 140106827742704
	140106825599680 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825599680 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825603520 -> 140106825599680
	140106825603520 -> 140106821864400 [dir=none]
	140106821864400 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106825603520 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825605728 -> 140106825603520
	140106825605728 -> 140106821863600 [dir=none]
	140106821863600 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106825605728 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106825606928 -> 140106825605728
	140106825606928 -> 140107040384208 [dir=none]
	140107040384208 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106825606928 -> 140107019760032 [dir=none]
	140107019760032 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106825606928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825607360 -> 140106825606928
	140106825607360 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106825607456 -> 140106825607360
	140106825607456 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825607456 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827742560 -> 140106825607456
	140106825607120 -> 140106825606928
	140107019760032 [label="dur_detail_enc.encoder.ffn_layers.1.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107019760032 -> 140106825607120
	140106825607120 [label=AccumulateGrad]
	140106825598048 -> 140106825606928
	140107019760192 [label="dur_detail_enc.encoder.ffn_layers.1.conv_1.bias
 (768)" fillcolor=lightblue]
	140107019760192 -> 140106825598048
	140106825598048 [label=AccumulateGrad]
	140106825597136 -> 140106827743088
	140107019760352 [label="dur_detail_enc.encoder.ffn_layers.1.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107019760352 -> 140106825597136
	140106825597136 [label=AccumulateGrad]
	140106825596992 -> 140106827743088
	140107019760992 [label="dur_detail_enc.encoder.ffn_layers.1.conv_2.bias
 (192)" fillcolor=lightblue]
	140107019760992 -> 140106825596992
	140106825596992 [label=AccumulateGrad]
	140106827742272 -> 140106827742176
	140107019761632 [label="dur_detail_enc.encoder.norm_layers_2.1.gamma
 (192)" fillcolor=lightblue]
	140107019761632 -> 140106827742272
	140106827742272 [label=AccumulateGrad]
	140106827742032 -> 140106827742176
	140107019762032 [label="dur_detail_enc.encoder.norm_layers_2.1.beta
 (192)" fillcolor=lightblue]
	140107019762032 -> 140106827742032
	140106827742032 [label=AccumulateGrad]
	140106827741984 -> 140106827741888
	140106827741984 -> 140106821864480 [dir=none]
	140106821864480 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827741984 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827742320 -> 140106827741984
	140106827742320 -> 140107040379328 [dir=none]
	140107040379328 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106827742320 -> 140107011965952 [dir=none]
	140107011965952 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827742320 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827742464 -> 140106827742320
	140106827742464 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106827743040 -> 140106827742464
	140106827743040 [label=CloneBackward0]
	140106825597760 -> 140106827743040
	140106825597760 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825606496 -> 140106825597760
	140106825606496 [label="AddBackward0
------------
alpha: 1"]
	140106825603712 -> 140106825606496
	140106825603712 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825604864 -> 140106825603712
	140106825604864 -> 140106821864160 [dir=none]
	140106821864160 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106825604864 -> 140106821863520 [dir=none]
	140106821863520 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106825604864 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825607648 -> 140106825604864
	140106825607648 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825607792 -> 140106825607648
	140106825607792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825607888 -> 140106825607792
	140106825607888 -> 140106821864560 [dir=none]
	140106821864560 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106825607888 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825607984 -> 140106825607888
	140106825607984 -> 140106821864720 [dir=none]
	140106821864720 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106825607984 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106825608080 -> 140106825607984
	140106825608080 -> 140107058265392 [dir=none]
	140107058265392 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106825608080 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106825608176 -> 140106825608080
	140106825608176 [label="AddBackward0
------------
alpha: 1"]
	140106825608272 -> 140106825608176
	140106825608272 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106825608416 -> 140106825608272
	140106825608416 -> 140106821864640 [dir=none]
	140106821864640 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106825608416 -> 140106821864800 [dir=none]
	140106821864800 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825608416 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825608512 -> 140106825608416
	140106825608512 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825608656 -> 140106825608512
	140106825608656 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825608752 -> 140106825608656
	140106825608752 -> 140106821864880 [dir=none]
	140106821864880 [label="other
 ()" fillcolor=orange]
	140106825608752 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825608848 -> 140106825608752
	140106825608848 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825608944 -> 140106825608848
	140106825608944 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825609040 -> 140106825608944
	140106825609040 -> 140107040390688 [dir=none]
	140107040390688 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825609040 -> 140107019762592 [dir=none]
	140107019762592 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825609040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827741936 -> 140106825609040
	140106825609136 -> 140106825609040
	140107019762592 [label="dur_detail_enc.encoder.attn_layers.2.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019762592 -> 140106825609136
	140106825609136 [label=AccumulateGrad]
	140106825609088 -> 140106825609040
	140107019762672 [label="dur_detail_enc.encoder.attn_layers.2.conv_q.bias
 (192)" fillcolor=lightblue]
	140107019762672 -> 140106825609088
	140106825609088 [label=AccumulateGrad]
	140106825608464 -> 140106825608416
	140106825608464 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825608800 -> 140106825608464
	140106825608800 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825608992 -> 140106825608800
	140106825608992 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825609232 -> 140106825608992
	140106825609232 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825609280 -> 140106825609232
	140106825609280 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825609424 -> 140106825609280
	140106825609424 -> 140107040390688 [dir=none]
	140107040390688 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825609424 -> 140107019763312 [dir=none]
	140107019763312 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825609424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827741936 -> 140106825609424
	140106825609520 -> 140106825609424
	140107019763312 [label="dur_detail_enc.encoder.attn_layers.2.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107019763312 -> 140106825609520
	140106825609520 [label=AccumulateGrad]
	140106825609472 -> 140106825609424
	140107019763632 [label="dur_detail_enc.encoder.attn_layers.2.conv_k.bias
 (192)" fillcolor=lightblue]
	140107019763632 -> 140106825609472
	140106825609472 [label=AccumulateGrad]
	140106825608224 -> 140106825608176
	140106825608224 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106825608704 -> 140106825608224
	140106825608704 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106825609184 -> 140106825608704
	140106825609184 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825609376 -> 140106825609184
	140106825609376 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825608608 -> 140106825609376
	140106825608608 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106825609616 -> 140106825608608
	140106825609616 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106825609712 -> 140106825609616
	140106825609712 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106825609808 -> 140106825609712
	140106825609808 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106825609904 -> 140106825609808
	140106825609904 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106825606688 -> 140106825609904
	140106825606688 -> 140106821865040 [dir=none]
	140106821865040 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106825606688 -> 140106821865280 [dir=none]
	140106821865280 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825606688 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825610096 -> 140106825606688
	140106825610096 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825610240 -> 140106825610096
	140106825610240 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825610336 -> 140106825610240
	140106825610336 -> 140106821862160 [dir=none]
	140106821862160 [label="other
 ()" fillcolor=orange]
	140106825610336 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825608848 -> 140106825610336
	140106825610048 -> 140106825606688
	140106825610048 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106825610432 -> 140106825610048
	140106825610432 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106825610144 -> 140106825610432
	140106825610144 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825610576 -> 140106825610144
	140106825610576 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106825610624 -> 140106825610576
	140106825610624 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106825610768 -> 140106825610624
	140106825610768 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106825610864 -> 140106825610768
	140107019759472 [label="dur_detail_enc.encoder.attn_layers.2.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107019759472 -> 140106825610864
	140106825610864 [label=AccumulateGrad]
	140106825607600 -> 140106825604864
	140106825607600 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825607936 -> 140106825607600
	140106825607936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825608128 -> 140106825607936
	140106825608128 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825608320 -> 140106825608128
	140106825608320 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825608560 -> 140106825608320
	140106825608560 -> 140107040390688 [dir=none]
	140107040390688 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825608560 -> 140107011965712 [dir=none]
	140107011965712 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825608560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827741936 -> 140106825608560
	140106825609568 -> 140106825608560
	140107011965712 [label="dur_detail_enc.encoder.attn_layers.2.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011965712 -> 140106825609568
	140106825609568 [label=AccumulateGrad]
	140106825609328 -> 140106825608560
	140107011965792 [label="dur_detail_enc.encoder.attn_layers.2.conv_v.bias
 (192)" fillcolor=lightblue]
	140107011965792 -> 140106825609328
	140106825609328 [label=AccumulateGrad]
	140106825607408 -> 140106825606496
	140106825607408 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825607840 -> 140106825607408
	140106825607840 -> 140106821865200 [dir=none]
	140106821865200 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106825607840 -> 140106821865600 [dir=none]
	140106821865600 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106825607840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825607696 -> 140106825607840
	140106825607696 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825609664 -> 140106825607696
	140106825609664 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825609856 -> 140106825609664
	140106825609856 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106825608368 -> 140106825609856
	140106825608368 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825610384 -> 140106825608368
	140106825610384 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825610480 -> 140106825610384
	140106825610480 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106825610672 -> 140106825610480
	140106825610672 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106825610960 -> 140106825610672
	140106825610960 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106825610192 -> 140106825610960
	140106825610192 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825611056 -> 140106825610192
	140106825611056 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106825607888 -> 140106825611056
	140106825608032 -> 140106825607840
	140106825608032 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106825609952 -> 140106825608032
	140106825609952 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106825610528 -> 140106825609952
	140106825610528 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106825610816 -> 140106825610528
	140106825610816 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106825611008 -> 140106825610816
	140106825611008 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106825611104 -> 140106825611008
	140107011966272 [label="dur_detail_enc.encoder.attn_layers.2.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107011966272 -> 140106825611104
	140106825611104 [label=AccumulateGrad]
	140106827742416 -> 140106827742320
	140107011965952 [label="dur_detail_enc.encoder.attn_layers.2.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011965952 -> 140106827742416
	140106827742416 [label=AccumulateGrad]
	140106827742080 -> 140106827742320
	140107011966192 [label="dur_detail_enc.encoder.attn_layers.2.conv_o.bias
 (192)" fillcolor=lightblue]
	140107011966192 -> 140106827742080
	140106827742080 [label=AccumulateGrad]
	140106827741648 -> 140106827741552
	140107011966432 [label="dur_detail_enc.encoder.norm_layers_1.2.gamma
 (192)" fillcolor=lightblue]
	140107011966432 -> 140106827741648
	140106827741648 [label=AccumulateGrad]
	140106827741408 -> 140106827741552
	140107011966592 [label="dur_detail_enc.encoder.norm_layers_1.2.beta
 (192)" fillcolor=lightblue]
	140107011966592 -> 140106827741408
	140106827741408 [label=AccumulateGrad]
	140106827741360 -> 140106827741264
	140106827741360 -> 140106821865440 [dir=none]
	140106821865440 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827741360 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827741696 -> 140106827741360
	140106827741696 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827741696 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827741840 -> 140106827741696
	140106827741840 -> 140107040379888 [dir=none]
	140107040379888 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106827741840 -> 140107011967552 [dir=none]
	140107011967552 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106827741840 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827742128 -> 140106827741840
	140106827742128 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106827742752 -> 140106827742128
	140106827742752 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827742752 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825607552 -> 140106827742752
	140106825607552 -> 140106821865840 [dir=none]
	140106821865840 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106825607552 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825609760 -> 140106825607552
	140106825609760 -> 140106821865760 [dir=none]
	140106821865760 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106825609760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106825610000 -> 140106825609760
	140106825610000 -> 140107040378688 [dir=none]
	140107040378688 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106825610000 -> 140107011966912 [dir=none]
	140107011966912 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106825610000 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106825611152 -> 140106825610000
	140106825611152 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106825611248 -> 140106825611152
	140106825611248 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825611248 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827741312 -> 140106825611248
	140106825610912 -> 140106825610000
	140107011966912 [label="dur_detail_enc.encoder.ffn_layers.2.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107011966912 -> 140106825610912
	140106825610912 [label=AccumulateGrad]
	140106825599152 -> 140106825610000
	140107011967232 [label="dur_detail_enc.encoder.ffn_layers.2.conv_1.bias
 (768)" fillcolor=lightblue]
	140107011967232 -> 140106825599152
	140106825599152 [label=AccumulateGrad]
	140106827741744 -> 140106827741840
	140107011967552 [label="dur_detail_enc.encoder.ffn_layers.2.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107011967552 -> 140106827741744
	140106827741744 [label=AccumulateGrad]
	140106827741456 -> 140106827741840
	140107011967792 [label="dur_detail_enc.encoder.ffn_layers.2.conv_2.bias
 (192)" fillcolor=lightblue]
	140107011967792 -> 140106827741456
	140106827741456 [label=AccumulateGrad]
	140106827741024 -> 140106827740928
	140107011968032 [label="dur_detail_enc.encoder.norm_layers_2.2.gamma
 (192)" fillcolor=lightblue]
	140107011968032 -> 140106827741024
	140106827741024 [label=AccumulateGrad]
	140106827740784 -> 140106827740928
	140107011968112 [label="dur_detail_enc.encoder.norm_layers_2.2.beta
 (192)" fillcolor=lightblue]
	140107011968112 -> 140106827740784
	140106827740784 [label=AccumulateGrad]
	140106827740736 -> 140106827740640
	140106827740736 -> 140106821865920 [dir=none]
	140106821865920 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827740736 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827737952 -> 140106827740736
	140106827737952 -> 140107040384928 [dir=none]
	140107040384928 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106827737952 -> 140107011970432 [dir=none]
	140107011970432 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827737952 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827741216 -> 140106827737952
	140106827741216 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106827742368 -> 140106827741216
	140106827742368 [label=CloneBackward0]
	140106827741504 -> 140106827742368
	140106827741504 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825610288 -> 140106827741504
	140106825610288 [label="AddBackward0
------------
alpha: 1"]
	140106825607744 -> 140106825610288
	140106825607744 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825608896 -> 140106825607744
	140106825608896 -> 140106821863440 [dir=none]
	140106821863440 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106825608896 -> 140106821865360 [dir=none]
	140106821865360 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106825608896 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825611440 -> 140106825608896
	140106825611440 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825611584 -> 140106825611440
	140106825611584 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106825611680 -> 140106825611584
	140106825611680 -> 140106821866000 [dir=none]
	140106821866000 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106825611680 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825611776 -> 140106825611680
	140106825611776 -> 140106821866160 [dir=none]
	140106821866160 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106825611776 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106825611872 -> 140106825611776
	140106825611872 -> 140107040380768 [dir=none]
	140107040380768 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106825611872 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106825611968 -> 140106825611872
	140106825611968 [label="AddBackward0
------------
alpha: 1"]
	140106825612064 -> 140106825611968
	140106825612064 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106825612208 -> 140106825612064
	140106825612208 -> 140106821866080 [dir=none]
	140106821866080 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106825612208 -> 140106821866240 [dir=none]
	140106821866240 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106825612208 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825612304 -> 140106825612208
	140106825612304 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825612448 -> 140106825612304
	140106825612448 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825612544 -> 140106825612448
	140106825612544 -> 140106821866320 [dir=none]
	140106821866320 [label="other
 ()" fillcolor=orange]
	140106825612544 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825612640 -> 140106825612544
	140106825612640 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825612736 -> 140106825612640
	140106825612736 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825612832 -> 140106825612736
	140106825612832 -> 140107040376048 [dir=none]
	140107040376048 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825612832 -> 140107011968272 [dir=none]
	140107011968272 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825612832 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827740688 -> 140106825612832
	140106825612928 -> 140106825612832
	140107011968272 [label="dur_detail_enc.encoder.attn_layers.3.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011968272 -> 140106825612928
	140106825612928 [label=AccumulateGrad]
	140106825612880 -> 140106825612832
	140107011968352 [label="dur_detail_enc.encoder.attn_layers.3.conv_q.bias
 (192)" fillcolor=lightblue]
	140107011968352 -> 140106825612880
	140106825612880 [label=AccumulateGrad]
	140106825612256 -> 140106825612208
	140106825612256 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825612592 -> 140106825612256
	140106825612592 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106825612784 -> 140106825612592
	140106825612784 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106825613024 -> 140106825612784
	140106825613024 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825613072 -> 140106825613024
	140106825613072 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825613216 -> 140106825613072
	140106825613216 -> 140107040376048 [dir=none]
	140107040376048 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825613216 -> 140107011968752 [dir=none]
	140107011968752 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825613216 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827740688 -> 140106825613216
	140106825613264 -> 140106825613216
	140107011968752 [label="dur_detail_enc.encoder.attn_layers.3.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011968752 -> 140106825613264
	140106825613264 [label=AccumulateGrad]
	140106825612400 -> 140106825613216
	140107011968992 [label="dur_detail_enc.encoder.attn_layers.3.conv_k.bias
 (192)" fillcolor=lightblue]
	140107011968992 -> 140106825612400
	140106825612400 [label=AccumulateGrad]
	140106825612016 -> 140106825611968
	140106825612016 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106825612496 -> 140106825612016
	140106825612496 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106825612976 -> 140106825612496
	140106825612976 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825613168 -> 140106825612976
	140106825613168 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106825613120 -> 140106825613168
	140106825613120 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106816241824 -> 140106825613120
	140106816241824 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106816241920 -> 140106816241824
	140106816241920 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106816242016 -> 140106816241920
	140106816242016 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106825606064 -> 140106816242016
	140106825606064 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106816242160 -> 140106825606064
	140106816242160 -> 140106821865120 [dir=none]
	140106821865120 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106816242160 -> 140106821866480 [dir=none]
	140106821866480 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106816242160 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816242256 -> 140106816242160
	140106816242256 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816242448 -> 140106816242256
	140106816242448 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816242544 -> 140106816242448
	140106816242544 -> 140106821865520 [dir=none]
	140106821865520 [label="other
 ()" fillcolor=orange]
	140106816242544 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825612640 -> 140106816242544
	140106816242208 -> 140106816242160
	140106816242208 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106816242640 -> 140106816242208
	140106816242640 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106816242352 -> 140106816242640
	140106816242352 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106816242784 -> 140106816242352
	140106816242784 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106816242832 -> 140106816242784
	140106816242832 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106816242976 -> 140106816242832
	140106816242976 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106816243072 -> 140106816242976
	140107011966352 [label="dur_detail_enc.encoder.attn_layers.3.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107011966352 -> 140106816243072
	140106816243072 [label=AccumulateGrad]
	140106825611392 -> 140106825608896
	140106825611392 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825611728 -> 140106825611392
	140106825611728 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106825611920 -> 140106825611728
	140106825611920 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825612112 -> 140106825611920
	140106825612112 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106825612352 -> 140106825612112
	140106825612352 -> 140107040376048 [dir=none]
	140107040376048 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106825612352 -> 140107011970192 [dir=none]
	140107011970192 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106825612352 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827740688 -> 140106825612352
	140106825612160 -> 140106825612352
	140107011970192 [label="dur_detail_enc.encoder.attn_layers.3.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011970192 -> 140106825612160
	140106825612160 [label=AccumulateGrad]
	140106825611536 -> 140106825612352
	140107011970352 [label="dur_detail_enc.encoder.attn_layers.3.conv_v.bias
 (192)" fillcolor=lightblue]
	140107011970352 -> 140106825611536
	140106825611536 [label=AccumulateGrad]
	140106825611200 -> 140106825610288
	140106825611200 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106825611632 -> 140106825611200
	140106825611632 -> 140106821866720 [dir=none]
	140106821866720 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106825611632 -> 140106821866960 [dir=none]
	140106821866960 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106825611632 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106825611488 -> 140106825611632
	140106825611488 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106825612688 -> 140106825611488
	140106825612688 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816242112 -> 140106825612688
	140106816242112 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106816242064 -> 140106816242112
	140106816242064 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816242592 -> 140106816242064
	140106816242592 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816242688 -> 140106816242592
	140106816242688 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816242880 -> 140106816242688
	140106816242880 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106816243168 -> 140106816242880
	140106816243168 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106816242400 -> 140106816243168
	140106816242400 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816243264 -> 140106816242400
	140106816243264 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106825611680 -> 140106816243264
	140106825611824 -> 140106825611632
	140106825611824 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106816241728 -> 140106825611824
	140106816241728 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106816242736 -> 140106816241728
	140106816242736 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106816243024 -> 140106816242736
	140106816243024 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106816243216 -> 140106816243024
	140106816243216 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106816243312 -> 140106816243216
	140107011970592 [label="dur_detail_enc.encoder.attn_layers.3.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107011970592 -> 140106816243312
	140106816243312 [label=AccumulateGrad]
	140106827741168 -> 140106827737952
	140107011970432 [label="dur_detail_enc.encoder.attn_layers.3.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011970432 -> 140106827741168
	140106827741168 [label=AccumulateGrad]
	140106827740832 -> 140106827737952
	140107011970512 [label="dur_detail_enc.encoder.attn_layers.3.conv_o.bias
 (192)" fillcolor=lightblue]
	140107011970512 -> 140106827740832
	140106827740832 [label=AccumulateGrad]
	140106827740400 -> 140106827740304
	140107011970912 [label="dur_detail_enc.encoder.norm_layers_1.3.gamma
 (192)" fillcolor=lightblue]
	140107011970912 -> 140106827740400
	140106827740400 [label=AccumulateGrad]
	140106827740160 -> 140106827740304
	140107011971072 [label="dur_detail_enc.encoder.norm_layers_1.3.beta
 (192)" fillcolor=lightblue]
	140107011971072 -> 140106827740160
	140106827740160 [label=AccumulateGrad]
	140106827740112 -> 140106827740016
	140106827740112 -> 140106821866400 [dir=none]
	140106821866400 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827740112 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827740448 -> 140106827740112
	140106827740448 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827740448 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827740592 -> 140106827740448
	140106827740592 -> 140107040385248 [dir=none]
	140107040385248 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106827740592 -> 140107011971632 [dir=none]
	140107011971632 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106827740592 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827740880 -> 140106827740592
	140106827740880 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106827741792 -> 140106827740880
	140106827741792 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827741792 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106825611344 -> 140106827741792
	140106825611344 -> 140106821867280 [dir=none]
	140106821867280 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106825611344 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106825611296 -> 140106825611344
	140106825611296 -> 140106821867200 [dir=none]
	140106821867200 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106825611296 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106816242928 -> 140106825611296
	140106816242928 -> 140107040384608 [dir=none]
	140107040384608 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106816242928 -> 140107011971312 [dir=none]
	140107011971312 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106816242928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106816243360 -> 140106816242928
	140106816243360 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106816243456 -> 140106816243360
	140106816243456 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106816243456 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827740064 -> 140106816243456
	140106816243120 -> 140106816242928
	140107011971312 [label="dur_detail_enc.encoder.ffn_layers.3.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107011971312 -> 140106816243120
	140106816243120 [label=AccumulateGrad]
	140106816241968 -> 140106816242928
	140107011971472 [label="dur_detail_enc.encoder.ffn_layers.3.conv_1.bias
 (768)" fillcolor=lightblue]
	140107011971472 -> 140106816241968
	140106816241968 [label=AccumulateGrad]
	140106827740496 -> 140106827740592
	140107011971632 [label="dur_detail_enc.encoder.ffn_layers.3.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107011971632 -> 140106827740496
	140106827740496 [label=AccumulateGrad]
	140106827740208 -> 140106827740592
	140107011972112 [label="dur_detail_enc.encoder.ffn_layers.3.conv_2.bias
 (192)" fillcolor=lightblue]
	140107011972112 -> 140106827740208
	140106827740208 [label=AccumulateGrad]
	140106827739776 -> 140106827739680
	140107011972352 [label="dur_detail_enc.encoder.norm_layers_2.3.gamma
 (192)" fillcolor=lightblue]
	140107011972352 -> 140106827739776
	140106827739776 [label=AccumulateGrad]
	140106827739536 -> 140106827739680
	140107011972512 [label="dur_detail_enc.encoder.norm_layers_2.3.beta
 (192)" fillcolor=lightblue]
	140107011972512 -> 140106827739536
	140106827739536 [label=AccumulateGrad]
	140106827739488 -> 140106827739392
	140106827739488 -> 140106821867360 [dir=none]
	140106821867360 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827739488 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827739824 -> 140106827739488
	140106827739824 -> 140107031837360 [dir=none]
	140107031837360 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106827739824 -> 140107011973952 [dir=none]
	140107011973952 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827739824 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827739968 -> 140106827739824
	140106827739968 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106827741120 -> 140106827739968
	140106827741120 [label=CloneBackward0]
	140106827740256 -> 140106827741120
	140106827740256 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106825597376 -> 140106827740256
	140106825597376 [label="AddBackward0
------------
alpha: 1"]
	140106816241872 -> 140106825597376
	140106816241872 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106816241776 -> 140106816241872
	140106816241776 -> 140106821866640 [dir=none]
	140106821866640 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106816241776 -> 140106821867040 [dir=none]
	140106821867040 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106816241776 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816243648 -> 140106816241776
	140106816243648 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106816243792 -> 140106816243648
	140106816243792 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106816243888 -> 140106816243792
	140106816243888 -> 140106821867440 [dir=none]
	140106821867440 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106816243888 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106816243984 -> 140106816243888
	140106816243984 -> 140106821867600 [dir=none]
	140106821867600 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106816243984 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106816244080 -> 140106816243984
	140106816244080 -> 140107040386368 [dir=none]
	140107040386368 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106816244080 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106816244176 -> 140106816244080
	140106816244176 [label="AddBackward0
------------
alpha: 1"]
	140106816244272 -> 140106816244176
	140106816244272 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106816244416 -> 140106816244272
	140106816244416 -> 140106821867520 [dir=none]
	140106821867520 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106816244416 -> 140106821867680 [dir=none]
	140106821867680 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106816244416 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816244512 -> 140106816244416
	140106816244512 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816244656 -> 140106816244512
	140106816244656 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816244752 -> 140106816244656
	140106816244752 -> 140106821867760 [dir=none]
	140106821867760 [label="other
 ()" fillcolor=orange]
	140106816244752 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106816244848 -> 140106816244752
	140106816244848 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816244944 -> 140106816244848
	140106816244944 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106816245040 -> 140106816244944
	140106816245040 -> 140107040381248 [dir=none]
	140107040381248 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106816245040 -> 140107011972592 [dir=none]
	140107011972592 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106816245040 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827739440 -> 140106816245040
	140106816245136 -> 140106816245040
	140107011972592 [label="dur_detail_enc.encoder.attn_layers.4.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011972592 -> 140106816245136
	140106816245136 [label=AccumulateGrad]
	140106816245088 -> 140106816245040
	140107011972752 [label="dur_detail_enc.encoder.attn_layers.4.conv_q.bias
 (192)" fillcolor=lightblue]
	140107011972752 -> 140106816245088
	140106816245088 [label=AccumulateGrad]
	140106816244464 -> 140106816244416
	140106816244464 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106816244800 -> 140106816244464
	140106816244800 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106816244992 -> 140106816244800
	140106816244992 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106816245232 -> 140106816244992
	140106816245232 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816245280 -> 140106816245232
	140106816245280 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106816245424 -> 140106816245280
	140106816245424 -> 140107040381248 [dir=none]
	140107040381248 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106816245424 -> 140107011972992 [dir=none]
	140107011972992 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106816245424 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827739440 -> 140106816245424
	140106816245520 -> 140106816245424
	140107011972992 [label="dur_detail_enc.encoder.attn_layers.4.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011972992 -> 140106816245520
	140106816245520 [label=AccumulateGrad]
	140106816245472 -> 140106816245424
	140107011973152 [label="dur_detail_enc.encoder.attn_layers.4.conv_k.bias
 (192)" fillcolor=lightblue]
	140107011973152 -> 140106816245472
	140106816245472 [label=AccumulateGrad]
	140106816244224 -> 140106816244176
	140106816244224 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106816244704 -> 140106816244224
	140106816244704 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106816245184 -> 140106816244704
	140106816245184 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106816245376 -> 140106816245184
	140106816245376 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106816244608 -> 140106816245376
	140106816244608 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106816245616 -> 140106816244608
	140106816245616 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106816245712 -> 140106816245616
	140106816245712 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106816245808 -> 140106816245712
	140106816245808 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106816245904 -> 140106816245808
	140106816245904 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106816246000 -> 140106816245904
	140106816246000 -> 140106821868080 [dir=none]
	140106821868080 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106816246000 -> 140106821867920 [dir=none]
	140106821867920 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106816246000 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816246096 -> 140106816246000
	140106816246096 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816246336 -> 140106816246096
	140106816246336 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816246432 -> 140106816246336
	140106816246432 -> 140106821866800 [dir=none]
	140106821866800 [label="other
 ()" fillcolor=orange]
	140106816246432 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106816244848 -> 140106816246432
	140106816246048 -> 140106816246000
	140106816246048 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106816246528 -> 140106816246048
	140106816246528 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106816246240 -> 140106816246528
	140106816246240 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106816246672 -> 140106816246240
	140106816246672 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106816246720 -> 140106816246672
	140106816246720 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106816246864 -> 140106816246720
	140106816246864 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106816246960 -> 140106816246864
	140107011970672 [label="dur_detail_enc.encoder.attn_layers.4.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107011970672 -> 140106816246960
	140106816246960 [label=AccumulateGrad]
	140106816243600 -> 140106816241776
	140106816243600 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816243936 -> 140106816243600
	140106816243936 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816244128 -> 140106816243936
	140106816244128 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816244320 -> 140106816244128
	140106816244320 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106816244560 -> 140106816244320
	140106816244560 -> 140107040381248 [dir=none]
	140107040381248 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106816244560 -> 140107011973312 [dir=none]
	140107011973312 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106816244560 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827739440 -> 140106816244560
	140106816245568 -> 140106816244560
	140107011973312 [label="dur_detail_enc.encoder.attn_layers.4.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011973312 -> 140106816245568
	140106816245568 [label=AccumulateGrad]
	140106816245328 -> 140106816244560
	140107011973792 [label="dur_detail_enc.encoder.attn_layers.4.conv_v.bias
 (192)" fillcolor=lightblue]
	140107011973792 -> 140106816245328
	140106816245328 [label=AccumulateGrad]
	140106816243408 -> 140106825597376
	140106816243408 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106816243840 -> 140106816243408
	140106816243840 -> 140106821868160 [dir=none]
	140106821868160 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106816243840 -> 140106821868400 [dir=none]
	140106821868400 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106816243840 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816243696 -> 140106816243840
	140106816243696 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816245664 -> 140106816243696
	140106816245664 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816245856 -> 140106816245664
	140106816245856 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106816244368 -> 140106816245856
	140106816244368 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816246480 -> 140106816244368
	140106816246480 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816246576 -> 140106816246480
	140106816246576 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816246768 -> 140106816246576
	140106816246768 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106816247056 -> 140106816246768
	140106816247056 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106816246288 -> 140106816247056
	140106816246288 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816247152 -> 140106816246288
	140106816247152 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106816243888 -> 140106816247152
	140106816244032 -> 140106816243840
	140106816244032 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106816245952 -> 140106816244032
	140106816245952 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106816246624 -> 140106816245952
	140106816246624 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106816246912 -> 140106816246624
	140106816246912 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106816247104 -> 140106816246912
	140106816247104 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106816247200 -> 140106816247104
	140107011974352 [label="dur_detail_enc.encoder.attn_layers.4.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107011974352 -> 140106816247200
	140106816247200 [label=AccumulateGrad]
	140106827739920 -> 140106827739824
	140107011973952 [label="dur_detail_enc.encoder.attn_layers.4.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011973952 -> 140106827739920
	140106827739920 [label=AccumulateGrad]
	140106827739584 -> 140106827739824
	140107011974112 [label="dur_detail_enc.encoder.attn_layers.4.conv_o.bias
 (192)" fillcolor=lightblue]
	140107011974112 -> 140106827739584
	140106827739584 [label=AccumulateGrad]
	140106827739152 -> 140106827739056
	140107011974512 [label="dur_detail_enc.encoder.norm_layers_1.4.gamma
 (192)" fillcolor=lightblue]
	140107011974512 -> 140106827739152
	140106827739152 [label=AccumulateGrad]
	140106827738912 -> 140106827739056
	140107011976192 [label="dur_detail_enc.encoder.norm_layers_1.4.beta
 (192)" fillcolor=lightblue]
	140107011976192 -> 140106827738912
	140106827738912 [label=AccumulateGrad]
	140106827738864 -> 140106827738768
	140106827738864 -> 140106821865680 [dir=none]
	140106821865680 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827738864 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827739200 -> 140106827738864
	140106827739200 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827739200 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827739344 -> 140106827739200
	140106827739344 -> 140107040387248 [dir=none]
	140107040387248 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106827739344 -> 140107011977072 [dir=none]
	140107011977072 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106827739344 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827739632 -> 140106827739344
	140106827739632 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106825607504 -> 140106827739632
	140106825607504 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106825607504 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827740544 -> 140106825607504
	140106827740544 -> 140106821868640 [dir=none]
	140106821868640 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106827740544 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106816245760 -> 140106827740544
	140106816245760 -> 140106821868560 [dir=none]
	140106821868560 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106816245760 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106816246816 -> 140106816245760
	140106816246816 -> 140107040389968 [dir=none]
	140107040389968 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106816246816 -> 140107011976352 [dir=none]
	140107011976352 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106816246816 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106816247248 -> 140106816246816
	140106816247248 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106816247344 -> 140106816247248
	140106816247344 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106816247344 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827738816 -> 140106816247344
	140106816247008 -> 140106816246816
	140107011976352 [label="dur_detail_enc.encoder.ffn_layers.4.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107011976352 -> 140106816247008
	140106816247008 [label=AccumulateGrad]
	140106816242496 -> 140106816246816
	140107011976432 [label="dur_detail_enc.encoder.ffn_layers.4.conv_1.bias
 (768)" fillcolor=lightblue]
	140107011976432 -> 140106816242496
	140106816242496 [label=AccumulateGrad]
	140106827739248 -> 140106827739344
	140107011977072 [label="dur_detail_enc.encoder.ffn_layers.4.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107011977072 -> 140106827739248
	140106827739248 [label=AccumulateGrad]
	140106827738960 -> 140106827739344
	140107011977232 [label="dur_detail_enc.encoder.ffn_layers.4.conv_2.bias
 (192)" fillcolor=lightblue]
	140107011977232 -> 140106827738960
	140106827738960 [label=AccumulateGrad]
	140106827738528 -> 140106827738432
	140107011977312 [label="dur_detail_enc.encoder.norm_layers_2.4.gamma
 (192)" fillcolor=lightblue]
	140107011977312 -> 140106827738528
	140106827738528 [label=AccumulateGrad]
	140106827738288 -> 140106827738432
	140107011977392 [label="dur_detail_enc.encoder.norm_layers_2.4.beta
 (192)" fillcolor=lightblue]
	140107011977392 -> 140106827738288
	140106827738288 [label=AccumulateGrad]
	140106827738240 -> 140106827738144
	140106827738240 -> 140106821868720 [dir=none]
	140106821868720 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827738240 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827738576 -> 140106827738240
	140106827738576 -> 140107031828400 [dir=none]
	140107031828400 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106827738576 -> 140107011979312 [dir=none]
	140107011979312 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106827738576 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827738720 -> 140106827738576
	140106827738720 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106827739872 -> 140106827738720
	140106827739872 [label=CloneBackward0]
	140106825603472 -> 140106827739872
	140106825603472 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816246384 -> 140106825603472
	140106816246384 [label="AddBackward0
------------
alpha: 1"]
	140106816243744 -> 140106816246384
	140106816243744 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106816244896 -> 140106816243744
	140106816244896 -> 140106821867120 [dir=none]
	140106821867120 [label="mat2
 (14, 28, 96)" fillcolor=orange]
	140106816244896 -> 140106821868320 [dir=none]
	140106821868320 [label="self
 (14, 28, 28)" fillcolor=orange]
	140106816244896 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816247536 -> 140106816244896
	140106816247536 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106816247680 -> 140106816247536
	140106816247680 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 28)"]
	140106816247776 -> 140106816247680
	140106816247776 -> 140106821868800 [dir=none]
	140106821868800 [label="result1
 (7, 2, 28, 28)" fillcolor=orange]
	140106816247776 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106816247872 -> 140106816247776
	140106816247872 -> 140106821868960 [dir=none]
	140106821868960 [label="result
 (7, 2, 28, 28)" fillcolor=orange]
	140106816247872 [label="SoftmaxBackward0
----------------------------
dim   : 18446744073709551615
result:       [saved tensor]"]
	140106816247968 -> 140106816247872
	140106816247968 -> 140107031836720 [dir=none]
	140107031836720 [label="mask
 (7, 1, 28, 28)" fillcolor=orange]
	140106816247968 [label="MaskedFillBackward0
--------------------
mask: [saved tensor]"]
	140106816248064 -> 140106816247968
	140106816248064 [label="AddBackward0
------------
alpha: 1"]
	140106816248160 -> 140106816248064
	140106816248160 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 28)"]
	140106816248304 -> 140106816248160
	140106816248304 -> 140106821868880 [dir=none]
	140106821868880 [label="mat2
 (14, 96, 28)" fillcolor=orange]
	140106816248304 -> 140106821869040 [dir=none]
	140106821869040 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106816248304 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816248400 -> 140106816248304
	140106816248400 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816248544 -> 140106816248400
	140106816248544 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816248640 -> 140106816248544
	140106816248640 -> 140106821869120 [dir=none]
	140106821869120 [label="other
 ()" fillcolor=orange]
	140106816248640 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106816248736 -> 140106816248640
	140106816248736 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816248832 -> 140106816248736
	140106816248832 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106816248928 -> 140106816248832
	140106816248928 -> 140107031837680 [dir=none]
	140107031837680 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106816248928 -> 140107011977712 [dir=none]
	140107011977712 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106816248928 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827738192 -> 140106816248928
	140106816249024 -> 140106816248928
	140107011977712 [label="dur_detail_enc.encoder.attn_layers.5.conv_q.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011977712 -> 140106816249024
	140106816249024 [label=AccumulateGrad]
	140106816248976 -> 140106816248928
	140107011978192 [label="dur_detail_enc.encoder.attn_layers.5.conv_q.bias
 (192)" fillcolor=lightblue]
	140107011978192 -> 140106816248976
	140106816248976 [label=AccumulateGrad]
	140106816248352 -> 140106816248304
	140106816248352 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106816248688 -> 140106816248352
	140106816248688 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 28)"]
	140106816248880 -> 140106816248688
	140106816248880 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106816249120 -> 140106816248880
	140106816249120 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816249168 -> 140106816249120
	140106816249168 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106816249312 -> 140106816249168
	140106816249312 -> 140107031837680 [dir=none]
	140107031837680 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106816249312 -> 140107011978672 [dir=none]
	140107011978672 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106816249312 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827738192 -> 140106816249312
	140106816249408 -> 140106816249312
	140107011978672 [label="dur_detail_enc.encoder.attn_layers.5.conv_k.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011978672 -> 140106816249408
	140106816249408 [label=AccumulateGrad]
	140106816249360 -> 140106816249312
	140107011978752 [label="dur_detail_enc.encoder.attn_layers.5.conv_k.bias
 (192)" fillcolor=lightblue]
	140107011978752 -> 140106816249360
	140106816249360 [label=AccumulateGrad]
	140106816248112 -> 140106816248064
	140106816248112 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 55)
start         :                  27
step          :                   1"]
	140106816248592 -> 140106816248112
	140106816248592 [label="SliceBackward0
------------------------------
dim           :              2
end           :             28
self_sym_sizes: (7, 2, 29, 55)
start         :              0
step          :              1"]
	140106816249072 -> 140106816248592
	140106816249072 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106816249264 -> 140106816249072
	140106816249264 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 29, 55)
start         :                   0
step          :                   1"]
	140106816248496 -> 140106816249264
	140106816248496 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1595)"]
	140106816249504 -> 140106816248496
	140106816249504 [label="ConstantPadNdBackward0
------------------------
pad: (0, 27, 0, 0, 0, 0)"]
	140106816249600 -> 140106816249504
	140106816249600 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 56)"]
	140106816249696 -> 140106816249600
	140106816249696 [label="ConstantPadNdBackward0
-----------------------------
pad: (0, 1, 0, 0, 0, 0, 0, 0)"]
	140106816249792 -> 140106816249696
	140106816249792 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 55)"]
	140106816250032 -> 140106816249792
	140106816250032 -> 140106821869280 [dir=none]
	140106821869280 [label="mat2
 (14, 96, 55)" fillcolor=orange]
	140106816250032 -> 140106821868240 [dir=none]
	140106821868240 [label="self
 (14, 28, 96)" fillcolor=orange]
	140106816250032 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816250128 -> 140106816250032
	140106816250128 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816250272 -> 140106816250128
	140106816250272 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816250368 -> 140106816250272
	140106816250368 -> 140106821869200 [dir=none]
	140106821869200 [label="other
 ()" fillcolor=orange]
	140106816250368 [label="DivBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106816248736 -> 140106816250368
	140106816250080 -> 140106816250032
	140106816250080 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 96, 55)"]
	140106816250464 -> 140106816250080
	140106816250464 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 96, 55)"]
	140106816250176 -> 140106816250464
	140106816250176 [label="TransposeBackward0
--------------------------
dim0: 18446744073709551614
dim1: 18446744073709551615"]
	140106816250608 -> 140106816250176
	140106816250608 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106816250656 -> 140106816250608
	140106816250656 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106816250800 -> 140106816250656
	140106816250800 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106816250896 -> 140106816250800
	140107011974432 [label="dur_detail_enc.encoder.attn_layers.5.emb_rel_k
 (1, 9, 96)" fillcolor=lightblue]
	140107011974432 -> 140106816250896
	140106816250896 [label=AccumulateGrad]
	140106816247488 -> 140106816244896
	140106816247488 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816247824 -> 140106816247488
	140106816247824 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 96)"]
	140106816248016 -> 140106816247824
	140106816248016 [label="TransposeBackward0
------------------
dim0: 2
dim1: 3"]
	140106816248208 -> 140106816248016
	140106816248208 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 192, 28)"]
	140106816248448 -> 140106816248208
	140106816248448 -> 140107031837680 [dir=none]
	140107031837680 [label="input
 (7, 192, 28)" fillcolor=orange]
	140106816248448 -> 140107011978832 [dir=none]
	140107011978832 [label="weight
 (192, 192, 1)" fillcolor=orange]
	140106816248448 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827738192 -> 140106816248448
	140106816249456 -> 140106816248448
	140107011978832 [label="dur_detail_enc.encoder.attn_layers.5.conv_v.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011978832 -> 140106816249456
	140106816249456 [label=AccumulateGrad]
	140106816249216 -> 140106816248448
	140107011979072 [label="dur_detail_enc.encoder.attn_layers.5.conv_v.bias
 (192)" fillcolor=lightblue]
	140107011979072 -> 140106816249216
	140106816249216 [label=AccumulateGrad]
	140106816247296 -> 140106816246384
	140106816247296 [label="UnsafeViewBackward0
----------------------------
self_sym_sizes: (14, 28, 96)"]
	140106816247728 -> 140106816247296
	140106816247728 -> 140106821869440 [dir=none]
	140106821869440 [label="mat2
 (14, 55, 96)" fillcolor=orange]
	140106816247728 -> 140106821869760 [dir=none]
	140106821869760 [label="self
 (14, 28, 55)" fillcolor=orange]
	140106816247728 [label="BmmBackward0
--------------------
mat2: [saved tensor]
self: [saved tensor]"]
	140106816247584 -> 140106816247728
	140106816247584 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816249552 -> 140106816247584
	140106816249552 [label="ExpandBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816249744 -> 140106816249552
	140106816249744 [label="SliceBackward0
-----------------------------------
dim           :                   3
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   1
step          :                   1"]
	140106816248256 -> 140106816249744
	140106816248256 [label="SliceBackward0
-----------------------------------
dim           :                   2
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816250416 -> 140106816248256
	140106816250416 [label="SliceBackward0
-----------------------------------
dim           :                   1
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816250512 -> 140106816250416
	140106816250512 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:      (7, 2, 28, 56)
start         :                   0
step          :                   1"]
	140106816250704 -> 140106816250512
	140106816250704 [label="ViewBackward0
----------------------------
self_sym_sizes: (7, 2, 1568)"]
	140106816250992 -> 140106816250704
	140106816250992 [label="ConstantPadNdBackward0
------------------------
pad: (28, 0, 0, 0, 0, 0)"]
	140106816250224 -> 140106816250992
	140106816250224 [label="ViewBackward0
------------------------------
self_sym_sizes: (7, 2, 28, 55)"]
	140106816251088 -> 140106816250224
	140106816251088 [label="ConstantPadNdBackward0
------------------------------
pad: (0, 27, 0, 0, 0, 0, 0, 0)"]
	140106816247776 -> 140106816251088
	140106816247920 -> 140106816247728
	140106816247920 [label="ReshapeAliasBackward0
------------------------------
self_sym_sizes: (7, 2, 55, 96)"]
	140106816249840 -> 140106816247920
	140106816249840 [label="ExpandBackward0
------------------------------
self_sym_sizes: (1, 1, 55, 96)"]
	140106816250560 -> 140106816249840
	140106816250560 [label="UnsqueezeBackward0
------------------
dim: 0"]
	140106816250848 -> 140106816250560
	140106816250848 [label="SliceBackward0
-----------------------------------
dim           :                   0
end           : 9223372036854775807
self_sym_sizes:         (1, 55, 96)
start         :                   0
step          :                   1"]
	140106816251040 -> 140106816250848
	140106816251040 [label="ConstantPadNdBackward0
-------------------------
pad: (0, 0, 23, 23, 0, 0)"]
	140106816251136 -> 140106816251040
	140107011979712 [label="dur_detail_enc.encoder.attn_layers.5.emb_rel_v
 (1, 9, 96)" fillcolor=lightblue]
	140107011979712 -> 140106816251136
	140106816251136 [label=AccumulateGrad]
	140106827738672 -> 140106827738576
	140107011979312 [label="dur_detail_enc.encoder.attn_layers.5.conv_o.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107011979312 -> 140106827738672
	140106827738672 [label=AccumulateGrad]
	140106827738336 -> 140106827738576
	140107011979392 [label="dur_detail_enc.encoder.attn_layers.5.conv_o.bias
 (192)" fillcolor=lightblue]
	140107011979392 -> 140106827738336
	140106827738336 [label=AccumulateGrad]
	140106827737904 -> 140106827737808
	140107011980112 [label="dur_detail_enc.encoder.norm_layers_1.5.gamma
 (192)" fillcolor=lightblue]
	140107011980112 -> 140106827737904
	140106827737904 [label=AccumulateGrad]
	140106827737664 -> 140106827737808
	140107011980592 [label="dur_detail_enc.encoder.norm_layers_1.5.beta
 (192)" fillcolor=lightblue]
	140107011980592 -> 140106827737664
	140106827737664 [label=AccumulateGrad]
	140106827737616 -> 140106827736176
	140106827737616 -> 140106821869360 [dir=none]
	140106821869360 [label="result1
 (7, 192, 28)" fillcolor=orange]
	140106827737616 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106827734832 -> 140106827737616
	140106827734832 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827734832 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827738096 -> 140106827734832
	140106827738096 -> 140107031827760 [dir=none]
	140107031827760 [label="input
 (7, 768, 30)" fillcolor=orange]
	140106827738096 -> 140107003790496 [dir=none]
	140107003790496 [label="weight
 (192, 768, 3)" fillcolor=orange]
	140106827738096 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (192,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106827738384 -> 140106827738096
	140106827738384 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106827739008 -> 140106827738384
	140106827739008 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106827739008 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106816247440 -> 140106827739008
	140106816247440 -> 140106821870000 [dir=none]
	140106821870000 [label="result1
 (7, 768, 28)" fillcolor=orange]
	140106816247440 [label="NativeDropoutBackward0
-----------------------
p      :            0.1
result1: [saved tensor]"]
	140106816249648 -> 140106816247440
	140106816249648 -> 140106821869920 [dir=none]
	140106821869920 [label="result
 (7, 768, 28)" fillcolor=orange]
	140106816249648 [label="ReluBackward0
----------------------
result: [saved tensor]"]
	140106816250752 -> 140106816249648
	140106816250752 -> 140107031830480 [dir=none]
	140107031830480 [label="input
 (7, 192, 30)" fillcolor=orange]
	140106816250752 -> 140107011980752 [dir=none]
	140107011980752 [label="weight
 (768, 192, 3)" fillcolor=orange]
	140106816250752 [label="ConvolutionBackward0
----------------------------------
bias_sym_sizes_opt:         (768,)
dilation          :           (1,)
groups            :              1
input             : [saved tensor]
output_padding    :           (0,)
padding           :           (0,)
stride            :           (1,)
transposed        :          False
weight            : [saved tensor]"]
	140106816251184 -> 140106816250752
	140106816251184 [label="ConstantPadNdBackward0
-----------------------
pad: (1, 1, 0, 0, 0, 0)"]
	140106816251280 -> 140106816251184
	140106816251280 -> 140107046554256 [dir=none]
	140107046554256 [label="other
 (7, 1, 28)" fillcolor=orange]
	140106816251280 [label="MulBackward0
---------------------
other: [saved tensor]
self :           None"]
	140106827737568 -> 140106816251280
	140106816249984 -> 140106816250752
	140107011980752 [label="dur_detail_enc.encoder.ffn_layers.5.conv_1.weight
 (768, 192, 3)" fillcolor=lightblue]
	140107011980752 -> 140106816249984
	140106816249984 [label=AccumulateGrad]
	140106816243504 -> 140106816250752
	140107003790336 [label="dur_detail_enc.encoder.ffn_layers.5.conv_1.bias
 (768)" fillcolor=lightblue]
	140107003790336 -> 140106816243504
	140106816243504 [label=AccumulateGrad]
	140106827738000 -> 140106827738096
	140107003790496 [label="dur_detail_enc.encoder.ffn_layers.5.conv_2.weight
 (192, 768, 3)" fillcolor=lightblue]
	140107003790496 -> 140106827738000
	140106827738000 [label=AccumulateGrad]
	140106827737712 -> 140106827738096
	140107003790576 [label="dur_detail_enc.encoder.ffn_layers.5.conv_2.bias
 (192)" fillcolor=lightblue]
	140107003790576 -> 140106827737712
	140106827737712 [label=AccumulateGrad]
	140106827736464 -> 140106804671920
	140107003790656 [label="dur_detail_enc.encoder.norm_layers_2.5.gamma
 (192)" fillcolor=lightblue]
	140107003790656 -> 140106827736464
	140106827736464 [label=AccumulateGrad]
	140106827729696 -> 140106804671920
	140107003790976 [label="dur_detail_enc.encoder.norm_layers_2.5.beta
 (192)" fillcolor=lightblue]
	140107003790976 -> 140106827729696
	140106827729696 [label=AccumulateGrad]
	140106804667840 -> 140106832137760
	140107003791056 [label="dur_detail_enc.out_proj.weight
 (192, 192, 1)" fillcolor=lightblue]
	140107003791056 -> 140106804667840
	140106804667840 [label=AccumulateGrad]
	140106804668224 -> 140106832137760
	140107003791216 [label="dur_detail_enc.out_proj.bias
 (192)" fillcolor=lightblue]
	140107003791216 -> 140106804668224
	140106804668224 [label=AccumulateGrad]
	140106869108608 -> 140106745666896
}
